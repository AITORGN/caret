
R version 3.2.3 beta (2015-11-29 r69717) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "caret"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('caret')
Loading required package: lattice
Loading required package: ggplot2
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("BoxCoxTrans")
> ### * BoxCoxTrans
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: BoxCoxTrans.default
> ### Title: Box-Cox and Exponential Transformations
> ### Aliases: BoxCoxTrans.default BoxCoxTrans predict.BoxCoxTrans
> ###   expoTrans.default expoTrans predict.expoTrans
> ### Keywords: utilities
> 
> ### ** Examples
> 
> data(BloodBrain)
> 
> ratio <- exp(logBBB)
> bc <- BoxCoxTrans(ratio)
> bc
Box-Cox Transformation

208 data points used to estimate Lambda

Input data summary:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1165  0.6554  1.0200  1.2820  1.6990  5.1550 

Largest/Smallest: 44.3 
Sample Skewness: 1.46 

Estimated Lambda: 0.2 

> 
> predict(bc, ratio[1:5])
[1]  1.2055119 -0.3844183  0.2249118  0.1419784  0.7398778
> 
> ratio[5] <- NA
> bc2 <- BoxCoxTrans(ratio, bbbDescr$tpsa, na.rm = TRUE)
> bc2
Box-Cox Transformation

207 data points used to estimate Lambda

Input data summary:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1165  0.6538  1.0000  1.2780  1.6900  5.1550 

Largest/Smallest: 44.3 
Sample Skewness: 1.47 

Estimated Lambda: 0.2 

> 
> manly <- expoTrans(ratio)
> manly
Exponential Transformation

207 data points used to estimate Lambda

Input data summary:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
 0.1165  0.6538  1.0000  1.2780  1.6900  5.1550       1 

Largest/Smallest: 44.26 
Sample Skewness: 1.467 

Estimated Lambda: -0.5726 

> 
> 
> 
> 
> 
> cleanEx()
> nameEx("as.table.confusionMatrix")
> ### * as.table.confusionMatrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: as.table.confusionMatrix
> ### Title: Save Confusion Table Results
> ### Aliases: as.table.confusionMatrix as.matrix.confusionMatrix
> ### Keywords: utilities
> 
> ### ** Examples
> 
> ###################
> ## 2 class example
> 
> lvs <- c("normal", "abnormal")
> truth <- factor(rep(lvs, times = c(86, 258)),
+                 levels = rev(lvs))
> pred <- factor(
+                c(
+                  rep(lvs, times = c(54, 32)),
+                  rep(lvs, times = c(27, 231))),               
+                levels = rev(lvs))
> 
> xtab <- table(pred, truth)
> 
> results <- confusionMatrix(xtab)
> as.table(results)
          truth
pred       abnormal normal
  abnormal      231     32
  normal         27     54
> as.matrix(results)
         abnormal normal
abnormal      231     32
normal         27     54
> as.matrix(results, what = "overall")
                       [,1]
Accuracy       0.8284883721
Kappa          0.5335968379
AccuracyLower  0.7844134380
AccuracyUpper  0.8667985207
AccuracyNull   0.7500000000
AccuracyPValue 0.0003096983
McnemarPValue  0.6025370061
> as.matrix(results, what = "classes")
                          [,1]
Sensitivity          0.8953488
Specificity          0.6279070
Pos Pred Value       0.8783270
Neg Pred Value       0.6666667
Prevalence           0.7500000
Detection Rate       0.6715116
Detection Prevalence 0.7645349
Balanced Accuracy    0.7616279
>   
> ###################
> ## 3 class example
> 
> xtab <- confusionMatrix(iris$Species, sample(iris$Species))
> as.matrix(xtab)
           setosa versicolor virginica
setosa         17         22        11
versicolor     15         14        21
virginica      18         14        18
> 
> 
> 
> cleanEx()
> nameEx("avNNet")
> ### * avNNet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: avNNet.default
> ### Title: Neural Networks Using Model Averaging
> ### Aliases: avNNet.default predict.avNNet avNNet.formula avNNet
> ### Keywords: neural
> 
> ### ** Examples
> 
> data(BloodBrain)
> ## Not run: 
> ##D modelFit <- avNNet(bbbDescr, logBBB, size = 5, linout = TRUE, trace = FALSE)
> ##D modelFit
> ##D 
> ##D predict(modelFit, bbbDescr)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bag")
> ### * bag
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bag.default
> ### Title: A General Framework For Bagging
> ### Aliases: bag.default bag bagControl predict.bag ldaBag plsBag nbBag
> ###   ctreeBag svmBag nnetBag
> ### Keywords: models
> 
> ### ** Examples
> 
> ## A simple example of bagging conditional inference regression trees:
> data(BloodBrain)
> 
> ## treebag <- bag(bbbDescr, logBBB, B = 10,
> ##                bagControl = bagControl(fit = ctreeBag$fit,
> ##                                        predict = ctreeBag$pred,
> ##                                        aggregate = ctreeBag$aggregate))
> 
> 
> 
> 
> ## An example of pooling posterior probabilities to generate class predictions
> data(mdrr)
> 
> ## remove some zero variance predictors and linear dependencies
> mdrrDescr <- mdrrDescr[, -nearZeroVar(mdrrDescr)]
> mdrrDescr <- mdrrDescr[, -findCorrelation(cor(mdrrDescr), .95)]
> 
> ## basicLDA <- train(mdrrDescr, mdrrClass, "lda")
> 
> ## bagLDA2 <- train(mdrrDescr, mdrrClass, 
> ##                  "bag", 
> ##                  B = 10, 
> ##                  bagControl = bagControl(fit = ldaBag$fit,
> ##                                          predict = ldaBag$pred,
> ##                                          aggregate = ldaBag$aggregate),
> ##                  tuneGrid = data.frame(vars = c((1:10)*10 , ncol(mdrrDescr))))
> 
> 
> 
> cleanEx()
> nameEx("bagEarth")
> ### * bagEarth
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bagEarth
> ### Title: Bagged Earth
> ### Aliases: bagEarth print.bagEarth bagEarth.default bagEarth.formula
> ### Keywords: regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D library(mda)
> ##D library(earth)
> ##D data(trees)
> ##D fit1 <- earth(trees[,-3], trees[,3])
> ##D fit2 <- bagEarth(trees[,-3], trees[,3], B = 10)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bagFDA")
> ### * bagFDA
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bagFDA
> ### Title: Bagged FDA
> ### Aliases: bagFDA print.bagFDA bagFDA.default bagFDA.formula
> ### Keywords: regression
> 
> ### ** Examples
> 
> library(mlbench)
> library(earth)
Loading required package: plotmo
Loading required package: plotrix
> data(Glass)
> 
> set.seed(36)
> inTrain <- sample(1:dim(Glass)[1], 150)
> 
> trainData <- Glass[ inTrain, ]
> testData  <- Glass[-inTrain, ]
> 
> 
> baggedFit <- bagFDA(Type ~ ., trainData)
> confusionMatrix(predict(baggedFit, testData[, -10]),
+                 testData[, 10])
Confusion Matrix and Statistics

          Reference
Prediction  1  2  3  5  6  7
         1 16  3  2  0  0  1
         2  5 17  3  0  0  0
         3  0  0  0  0  0  0
         5  0  2  0  5  1  0
         6  0  0  0  0  3  0
         7  1  0  0  0  0  5

Overall Statistics
                                         
               Accuracy : 0.7188         
                 95% CI : (0.5924, 0.824)
    No Information Rate : 0.3438         
    P-Value [Acc > NIR] : 1.062e-09      
                                         
                  Kappa : 0.6126         
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                     Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7
Sensitivity            0.7273   0.7727  0.00000  1.00000  0.75000  0.83333
Specificity            0.8571   0.8095  1.00000  0.94915  1.00000  0.98276
Pos Pred Value         0.7273   0.6800      NaN  0.62500  1.00000  0.83333
Neg Pred Value         0.8571   0.8718  0.92188  1.00000  0.98361  0.98276
Prevalence             0.3438   0.3438  0.07812  0.07812  0.06250  0.09375
Detection Rate         0.2500   0.2656  0.00000  0.07812  0.04688  0.07812
Detection Prevalence   0.3438   0.3906  0.00000  0.12500  0.04688  0.09375
Balanced Accuracy      0.7922   0.7911  0.50000  0.97458  0.87500  0.90805
> 
> 
> 
> 
> cleanEx()

detaching ‘package:earth’, ‘package:plotrix’, ‘package:plotmo’,
  ‘package:mlbench’

> nameEx("calibration")
> ### * calibration
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calibration
> ### Title: Probability Calibration Plot
> ### Aliases: calibration calibration.formula calibration.default
> ###   xyplot.calibration panel.calibration
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(mdrr)
> ##D mdrrDescr <- mdrrDescr[, -nearZeroVar(mdrrDescr)]
> ##D mdrrDescr <- mdrrDescr[, -findCorrelation(cor(mdrrDescr), .5)]
> ##D 
> ##D 
> ##D inTrain <- createDataPartition(mdrrClass)
> ##D trainX <- mdrrDescr[inTrain[[1]], ]
> ##D trainY <- mdrrClass[inTrain[[1]]]
> ##D testX <- mdrrDescr[-inTrain[[1]], ]
> ##D testY <- mdrrClass[-inTrain[[1]]]
> ##D 
> ##D library(MASS)
> ##D 
> ##D ldaFit <- lda(trainX, trainY)
> ##D qdaFit <- qda(trainX, trainY)
> ##D 
> ##D testProbs <- data.frame(obs = testY,
> ##D                         lda = predict(ldaFit, testX)$posterior[,1],
> ##D                         qda = predict(qdaFit, testX)$posterior[,1])
> ##D 
> ##D calibration(obs ~ lda + qda, data = testProbs)
> ##D 
> ##D calPlotData <- calibration(obs ~ lda + qda, data = testProbs)
> ##D calPlotData
> ##D 
> ##D xyplot(calPlotData, auto.key = list(columns = 2))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("classDist")
> ### * classDist
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: classDist
> ### Title: Compute and predict the distances to class centroids
> ### Aliases: classDist.default classDist predict.classDist
> ### Keywords: manip
> 
> ### ** Examples
> 
> trainSet <- sample(1:150, 100)
> 
> distData <- classDist(iris[trainSet, 1:4], 
+                       iris$Species[trainSet])
> 
> newDist <- predict(distData,
+                    iris[-trainSet, 1:4])
> 
> splom(newDist, groups = iris$Species[-trainSet])
> 
> 
> 
> cleanEx()
> nameEx("confusionMatrix")
> ### * confusionMatrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confusionMatrix
> ### Title: Create a confusion matrix
> ### Aliases: confusionMatrix.table confusionMatrix.default confusionMatrix
> ### Keywords: utilities
> 
> ### ** Examples
> 
> ###################
> ## 2 class example
> 
> lvs <- c("normal", "abnormal")
> truth <- factor(rep(lvs, times = c(86, 258)),
+                 levels = rev(lvs))
> pred <- factor(
+                c(
+                  rep(lvs, times = c(54, 32)),
+                  rep(lvs, times = c(27, 231))),               
+                levels = rev(lvs))
> 
> xtab <- table(pred, truth)
> 
> confusionMatrix(xtab)
Confusion Matrix and Statistics

          truth
pred       abnormal normal
  abnormal      231     32
  normal         27     54
                                          
               Accuracy : 0.8285          
                 95% CI : (0.7844, 0.8668)
    No Information Rate : 0.75            
    P-Value [Acc > NIR] : 0.0003097       
                                          
                  Kappa : 0.5336          
 Mcnemar's Test P-Value : 0.6025370       
                                          
            Sensitivity : 0.8953          
            Specificity : 0.6279          
         Pos Pred Value : 0.8783          
         Neg Pred Value : 0.6667          
             Prevalence : 0.7500          
         Detection Rate : 0.6715          
   Detection Prevalence : 0.7645          
      Balanced Accuracy : 0.7616          
                                          
       'Positive' Class : abnormal        
                                          
> confusionMatrix(pred, truth)
Confusion Matrix and Statistics

          Reference
Prediction abnormal normal
  abnormal      231     32
  normal         27     54
                                          
               Accuracy : 0.8285          
                 95% CI : (0.7844, 0.8668)
    No Information Rate : 0.75            
    P-Value [Acc > NIR] : 0.0003097       
                                          
                  Kappa : 0.5336          
 Mcnemar's Test P-Value : 0.6025370       
                                          
            Sensitivity : 0.8953          
            Specificity : 0.6279          
         Pos Pred Value : 0.8783          
         Neg Pred Value : 0.6667          
             Prevalence : 0.7500          
         Detection Rate : 0.6715          
   Detection Prevalence : 0.7645          
      Balanced Accuracy : 0.7616          
                                          
       'Positive' Class : abnormal        
                                          
> confusionMatrix(xtab, prevalence = 0.25)   
Confusion Matrix and Statistics

          truth
pred       abnormal normal
  abnormal      231     32
  normal         27     54
                                          
               Accuracy : 0.8285          
                 95% CI : (0.7844, 0.8668)
    No Information Rate : 0.75            
    P-Value [Acc > NIR] : 0.0003097       
                                          
                  Kappa : 0.5336          
 Mcnemar's Test P-Value : 0.6025370       
                                          
            Sensitivity : 0.8953          
            Specificity : 0.6279          
         Pos Pred Value : 0.4451          
         Neg Pred Value : 0.9474          
             Prevalence : 0.2500          
         Detection Rate : 0.6715          
   Detection Prevalence : 0.7645          
      Balanced Accuracy : 0.7616          
                                          
       'Positive' Class : abnormal        
                                          
> 
> ###################
> ## 3 class example
> 
> confusionMatrix(iris$Species, sample(iris$Species))
Confusion Matrix and Statistics

            Reference
Prediction   setosa versicolor virginica
  setosa         17         22        11
  versicolor     15         14        21
  virginica      18         14        18

Overall Statistics
                                          
               Accuracy : 0.3267          
                 95% CI : (0.2524, 0.4079)
    No Information Rate : 0.3333          
    P-Value [Acc > NIR] : 0.5990          
                                          
                  Kappa : -0.01           
 Mcnemar's Test P-Value : 0.2201          

Statistics by Class:

                     Class: setosa Class: versicolor Class: virginica
Sensitivity                 0.3400           0.28000           0.3600
Specificity                 0.6700           0.64000           0.6800
Pos Pred Value              0.3400           0.28000           0.3600
Neg Pred Value              0.6700           0.64000           0.6800
Prevalence                  0.3333           0.33333           0.3333
Detection Rate              0.1133           0.09333           0.1200
Detection Prevalence        0.3333           0.33333           0.3333
Balanced Accuracy           0.5050           0.46000           0.5200
> 
> newPrior <- c(.05, .8, .15)
> names(newPrior) <- levels(iris$Species)
> 
> confusionMatrix(iris$Species, sample(iris$Species))
Confusion Matrix and Statistics

            Reference
Prediction   setosa versicolor virginica
  setosa         16         17        17
  versicolor     15         17        18
  virginica      19         16        15

Overall Statistics
                                         
               Accuracy : 0.32           
                 95% CI : (0.2463, 0.401)
    No Information Rate : 0.3333         
    P-Value [Acc > NIR] : 0.6646         
                                         
                  Kappa : -0.02          
 Mcnemar's Test P-Value : 0.9496         

Statistics by Class:

                     Class: setosa Class: versicolor Class: virginica
Sensitivity                 0.3200            0.3400           0.3000
Specificity                 0.6600            0.6700           0.6500
Pos Pred Value              0.3200            0.3400           0.3000
Neg Pred Value              0.6600            0.6700           0.6500
Prevalence                  0.3333            0.3333           0.3333
Detection Rate              0.1067            0.1133           0.1000
Detection Prevalence        0.3333            0.3333           0.3333
Balanced Accuracy           0.4900            0.5050           0.4750
> 
> 
> 
> 
> cleanEx()
> nameEx("confusionMatrix.train")
> ### * confusionMatrix.train
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confusionMatrix.train
> ### Title: Estimate a Resampled Confusion Matrix
> ### Aliases: confusionMatrix.train confusionMatrix.rfe confusionMatrix.sbf
> ### Keywords: utilities
> 
> ### ** Examples
> 
> 
> data(iris)
> TrainData <- iris[,1:4]
> TrainClasses <- iris[,5]
> 
> knnFit <- train(TrainData, TrainClasses,
+                 method = "knn",
+                 preProcess = c("center", "scale"),
+                 tuneLength = 10,
+                 trControl = trainControl(method = "cv"))
> confusionMatrix(knnFit)
Cross-Validated (10 fold) Confusion Matrix 

(entries are percentages of table totals)
 
            Reference
Prediction   setosa versicolor virginica
  setosa       33.3        0.0       0.0
  versicolor    0.0       32.0       2.0
  virginica     0.0        1.3      31.3

> confusionMatrix(knnFit, "average")
Cross-Validated (10 fold) Confusion Matrix 

(entries are cell counts per resample)
 
            Reference
Prediction   setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         48         3
  virginica       0          2        47

> confusionMatrix(knnFit, "none")
Cross-Validated (10 fold) Confusion Matrix 

(entries are un-normalized counts)
 
            Reference
Prediction   setosa versicolor virginica
  setosa        5.0        0.0       0.0
  versicolor    0.0        4.8       0.3
  virginica     0.0        0.2       4.7

> 
> 
> 
> 
> cleanEx()
> nameEx("createDataPartition")
> ### * createDataPartition
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: createDataPartition
> ### Title: Data Splitting functions
> ### Aliases: createDataPartition createResample createFolds
> ###   createMultiFolds createTimeSlices
> ### Keywords: utilities
> 
> ### ** Examples
> 
> data(oil)
> createDataPartition(oilType, 2)
$Resample1
 [1]  2  5  6  7  8 10 11 14 16 18 19 21 24 28 29 30 31 32 36 37 38 42 45 46 47
[26] 49 50 53 54 55 59 60 61 63 67 68 69 72 75 78 83 84 86 87 88 89 90 92 93 96

$Resample2
 [1]  2  3  4  6  8  9 10 11 12 15 16 18 25 26 29 30 31 32 33 34 37 42 43 44 46
[26] 47 53 55 58 59 60 61 63 64 66 68 70 73 77 79 80 81 84 85 86 89 92 93 95 96

> 
> x <- rgamma(50, 3, .5)
> inA <- createDataPartition(x, list = FALSE)
> 
> plot(density(x[inA]))
> rug(x[inA])
> 
> points(density(x[-inA]), type = "l", col = 4)
> rug(x[-inA], col = 4)
> 
> createResample(oilType, 2)
$Resample1
 [1]  3  5  5  6  7  7  8  8 10 10 11 11 12 13 13 14 14 15 16 16 17 17 17 17 18
[26] 19 19 22 22 24 25 26 26 26 27 29 29 30 31 31 32 34 35 36 36 37 37 37 39 39
[51] 39 43 45 46 46 46 47 48 48 48 49 51 51 53 56 58 60 60 64 65 65 66 68 70 73
[76] 74 74 76 76 78 78 79 81 83 85 90 90 90 92 94 94 95 95 95 96 96

$Resample2
 [1]  2  4  6  7  7  7  8  9 11 11 11 14 14 15 15 15 17 18 18 22 23 23 24 27 28
[26] 29 30 30 32 33 34 34 34 35 36 36 37 39 41 41 42 42 42 45 46 46 47 47 48 48
[51] 49 49 50 52 54 55 57 63 64 64 64 67 68 68 68 70 70 72 72 75 76 77 77 77 79
[76] 80 81 81 81 82 84 85 86 87 89 89 90 90 90 91 91 92 93 94 95 96

> 
> createFolds(oilType, 10)
$Fold01
 [1]  1 13 14 16 26 31 60 80 83 92

$Fold02
[1]  5  6 21 30 43 48 59 74 89

$Fold03
[1]  3 32 39 45 58 64 75 88

$Fold04
[1] 10 18 40 42 44 68 76 82 93

$Fold05
[1] 11 17 19 49 54 67 70 81 90

$Fold06
 [1]  2  4 12 22 27 37 50 56 57 62 77 78

$Fold07
 [1]  9 15 25 29 53 66 69 85 86 91

$Fold08
 [1]  7 24 33 35 38 55 61 63 94 96

$Fold09
[1] 34 41 51 65 71 73 79 84 95

$Fold10
 [1]  8 20 23 28 36 46 47 52 72 87

> createFolds(oilType, 5, FALSE)
 [1] 5 4 2 3 4 1 5 1 2 1 4 2 2 3 3 5 4 1 2 1 1 2 1 3 4 4 2 3 1 5 4 4 3 3 5 5 4 2
[39] 5 4 2 3 4 5 3 1 2 2 3 5 1 2 1 5 3 1 3 2 1 1 4 5 5 1 1 3 1 4 3 5 3 4 3 2 5 4
[77] 5 3 2 2 4 3 4 2 1 5 1 5 4 4 5 3 5 4 2 2
> 
> createFolds(rnorm(21))
$Fold01
[1]  6 15

$Fold02
[1]  1  8 13

$Fold03
[1]  5 17

$Fold04
[1] 14 18

$Fold05
[1] 16 21

$Fold06
[1]  9 11

$Fold07
[1]  4 10

$Fold08
[1]  3 12

$Fold09
[1] 19 20

$Fold10
[1] 2 7

> 
> createTimeSlices(1:9, 5, 1, fixedWindow = FALSE)
$train
$train$Training1
[1] 1 2 3 4 5

$train$Training2
[1] 1 2 3 4 5 6

$train$Training3
[1] 1 2 3 4 5 6 7

$train$Training4
[1] 1 2 3 4 5 6 7 8


$test
$test$Testing1
[1] 6

$test$Testing2
[1] 7

$test$Testing3
[1] 8

$test$Testing4
[1] 9


> createTimeSlices(1:9, 5, 1, fixedWindow = TRUE)
$train
$train$Training1
[1] 1 2 3 4 5

$train$Training2
[1] 2 3 4 5 6

$train$Training3
[1] 3 4 5 6 7

$train$Training4
[1] 4 5 6 7 8


$test
$test$Testing1
[1] 6

$test$Testing2
[1] 7

$test$Testing3
[1] 8

$test$Testing4
[1] 9


> createTimeSlices(1:9, 5, 3, fixedWindow = TRUE)
$train
$train$Training1
[1] 1 2 3 4 5

$train$Training2
[1] 2 3 4 5 6


$test
$test$Testing1
[1] 6 7 8

$test$Testing2
[1] 7 8 9


> createTimeSlices(1:9, 5, 3, fixedWindow = FALSE)
$train
$train$Training1
[1] 1 2 3 4 5

$train$Training2
[1] 1 2 3 4 5 6


$test
$test$Testing1
[1] 6 7 8

$test$Testing2
[1] 7 8 9


> 
> createTimeSlices(1:15, 5, 3)
$train
$train$Training1
[1] 1 2 3 4 5

$train$Training2
[1] 2 3 4 5 6

$train$Training3
[1] 3 4 5 6 7

$train$Training4
[1] 4 5 6 7 8

$train$Training5
[1] 5 6 7 8 9

$train$Training6
[1]  6  7  8  9 10

$train$Training7
[1]  7  8  9 10 11

$train$Training8
[1]  8  9 10 11 12


$test
$test$Testing1
[1] 6 7 8

$test$Testing2
[1] 7 8 9

$test$Testing3
[1]  8  9 10

$test$Testing4
[1]  9 10 11

$test$Testing5
[1] 10 11 12

$test$Testing6
[1] 11 12 13

$test$Testing7
[1] 12 13 14

$test$Testing8
[1] 13 14 15


> createTimeSlices(1:15, 5, 3, skip = 2)
$train
$train$Training1
[1] 1 2 3 4 5

$train$Training4
[1] 4 5 6 7 8

$train$Training7
[1]  7  8  9 10 11


$test
$test$Testing1
[1] 6 7 8

$test$Testing4
[1]  9 10 11

$test$Testing7
[1] 12 13 14


> createTimeSlices(1:15, 5, 3, skip = 3)
$train
$train$Training1
[1] 1 2 3 4 5

$train$Training5
[1] 5 6 7 8 9


$test
$test$Testing1
[1] 6 7 8

$test$Testing5
[1] 10 11 12


> 
> 
> 
> cleanEx()
> nameEx("diff.resamples")
> ### * diff.resamples
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: diff.resamples
> ### Title: Inferential Assessments About Model Performance
> ### Aliases: diff.resamples summary.diff.resamples compare_models
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D #load(url("http://topepo.github.io/caret/exampleModels.RData"))
> ##D 
> ##D resamps <- resamples(list(CART = rpartFit,
> ##D                           CondInfTree = ctreeFit,
> ##D                           MARS = earthFit))
> ##D 
> ##D difs <- diff(resamps)
> ##D 
> ##D difs
> ##D 
> ##D summary(difs)
> ##D 
> ##D compare_models(rpartFit, ctreeFit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("dotplot.varImp.train")
> ### * dotplot.varImp.train
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dotPlot
> ### Title: Create a dotplot of variable importance values
> ### Aliases: dotPlot
> ### Keywords: hplot
> 
> ### ** Examples
> 
> 
> data(iris)
> TrainData <- iris[,1:4]
> TrainClasses <- iris[,5]
> 
> knnFit <- train(TrainData, TrainClasses, "knn")
> 
> knnImp <- varImp(knnFit)
> 
> dotPlot(knnImp)
> 
> 
> 
> 
> cleanEx()
> nameEx("downSample")
> ### * downSample
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: downSample
> ### Title: Down- and Up-Sampling Imbalanced Data
> ### Aliases: downSample upSample
> ### Keywords: utilities
> 
> ### ** Examples
> 
> ## A ridiculous example...
> data(oil)
> table(oilType)
oilType
 A  B  C  D  E  F  G 
37 26  3  7 11 10  2 
> downSample(fattyAcids, oilType)
   Palmitic Stearic Oleic Linoleic Linolenic Eicosanoic Eicosenoic Class
1      10.0     4.8  30.4     53.5       0.3        0.4        0.1     A
2       9.7     5.6  35.2     47.8       0.5        0.4        0.2     A
3       6.2     4.2  27.1     61.8       0.8        0.1        0.1     B
4       5.9     4.5  24.1     61.7       0.9        0.6        0.6     B
5       9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
6      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
7      11.4     3.0  73.0     10.6       0.7        0.1        0.1     D
8      10.5     2.8  75.8      8.0       0.7        0.1        0.1     D
9      10.5     4.3  24.6     53.1       7.6        0.1        0.1     E
10     10.9     3.6  26.0     52.6       5.5        0.4        0.2     E
11      5.5     1.7  59.0     21.3       9.3        0.6        1.5     F
12      4.8     1.8  62.6     20.0       9.5        0.1        1.4     F
13     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
14     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
> 
> upSample(fattyAcids, oilType)
    Palmitic Stearic Oleic Linoleic Linolenic Eicosanoic Eicosenoic Class
1        9.7     5.2  31.0     52.7       0.4        0.4        0.1     A
2       11.1     5.0  32.9     49.8       0.3        0.4        0.1     A
3       11.5     5.2  35.0     47.2       0.2        0.4        0.1     A
4       10.0     4.8  30.4     53.5       0.3        0.4        0.1     A
5       12.2     5.0  31.1     50.5       0.3        0.4        0.1     A
6        9.8     4.2  43.0     39.2       2.4        0.4        0.5     A
7       10.5     5.0  31.8     51.3       0.4        0.4        0.1     A
8       10.5     5.0  31.8     51.3       0.4        0.4        0.1     A
9       11.5     5.2  35.0     47.2       0.2        0.4        0.1     A
10      10.0     4.8  30.4     53.5       0.3        0.4        0.1     A
11      11.1     5.0  32.9     49.8       0.3        0.4        0.1     A
12       9.3     4.4  43.3     39.2       2.3        0.4        0.5     A
13      12.2     5.0  31.1     50.5       0.3        0.4        0.1     A
14       9.7     5.6  35.2     47.8       0.5        0.4        0.2     A
15      11.1     4.6  28.3     50.6       4.2        0.4        0.2     A
16      11.0     4.7  29.8     49.6       3.4        0.4        0.2     A
17      11.6     6.0  35.4     45.9       0.2        0.4        0.1     A
18       9.8     5.3  31.7     51.3       0.8        0.4        0.2     A
19      11.2     6.2  37.7     43.6       0.2        0.5        0.2     A
20       7.6     4.7  29.1     56.1       0.7        0.4        0.2     A
21       7.7     4.7  31.1     54.3       0.8        0.4        0.2     A
22       9.9     5.2  37.4     44.2       2.1        0.4        0.3     A
23      11.5     5.1  27.8     54.5       0.2        0.4        0.1     A
24      11.3     5.8  35.2     46.7       0.2        0.4        0.1     A
25      12.2     5.4  29.4     53.0       1.0        0.1        0.1     A
26      11.0     5.3  35.0     45.2       1.3        1.3        0.7     A
27      11.9     5.6  33.6     48.9       1.0        0.1        0.1     A
28      11.4     5.8  34.5     48.3       1.0        0.1        0.1     A
29      10.7     5.4  39.3     43.2       1.4        0.1        0.1     A
30      11.2     6.2  35.8     44.1       0.7        1.1        0.1     A
31      11.4     5.8  33.9     48.1       0.8        0.8        0.1     A
32      11.5     6.2  39.7     42.6       0.8        0.1        0.1     A
33      13.0     6.2  25.8     55.0       0.8        0.1        0.1     A
34      13.0     6.7  29.6     50.8       0.5        0.1        0.1     A
35      13.1     6.3  26.5     53.6       0.5        0.5        0.1     A
36      11.6     6.5  38.4     42.8       0.5        0.7        0.1     A
37      13.1     5.7  31.7     49.5       0.6        0.1        0.1     A
38       6.1     4.1  24.0     64.3       0.1        0.3        0.1     B
39       6.2     3.9  27.1     59.9       1.3        0.3        0.3     B
40       6.0     4.9  22.8     64.4       0.3        0.3        0.2     B
41       7.2     4.5  25.6     61.1       0.2        0.3        0.2     B
42       6.2     4.1  26.8     61.4       0.1        0.3        0.2     B
43       6.1     4.0  25.2     63.2       0.1        0.2        0.2     B
44       6.1     4.1  26.7     61.0       0.6        0.3        0.2     B
45       6.2     4.0  25.8     62.2       0.4        0.3        0.2     B
46       6.0     3.8  29.6     57.7       1.2        0.3        0.3     B
47       6.5     2.8  23.2     66.1       0.1        0.2        0.2     B
48       7.0     3.4  23.2     64.9       0.1        0.2        0.2     B
49       6.0     4.0  28.3     60.1       0.1        0.3        0.2     B
50       6.1     4.1  25.1     63.5       0.5        0.1        0.1     B
51       6.3     4.2  27.4     61.4       0.8        0.1        0.1     B
52       6.2     4.2  27.1     61.8       0.8        0.1        0.1     B
53       6.2     4.2  27.0     60.9       0.5        0.3        0.1     B
54       6.2     4.0  28.3     59.7       0.9        0.1        0.1     B
55       5.6     4.2  25.7     58.9       1.7        2.8        0.9     B
56       6.4     3.9  26.0     63.7       0.5        0.1        0.1     B
57       6.8     4.3  26.4     60.5       1.3        0.1        0.1     B
58       6.0     4.4  27.1     60.9       0.9        0.1        0.1     B
59       6.4     4.8  25.3     61.8       1.0        0.1        0.1     B
60       5.9     4.5  24.1     61.7       0.9        0.6        0.6     B
61       6.2     4.1  29.9     57.8       1.3        0.1        0.1     B
62       6.6     4.7  24.5     62.8       0.3        0.4        0.1     B
63       6.4     4.4  24.4     63.7       0.4        0.4        0.1     B
64       6.0     4.4  27.1     60.9       0.9        0.1        0.1     B
65       6.1     4.1  25.1     63.5       0.5        0.1        0.1     B
66       6.4     3.9  26.0     63.7       0.5        0.1        0.1     B
67       6.4     4.4  24.4     63.7       0.4        0.4        0.1     B
68       6.5     2.8  23.2     66.1       0.1        0.2        0.2     B
69       6.0     4.4  27.1     60.9       0.9        0.1        0.1     B
70       6.6     4.7  24.5     62.8       0.3        0.4        0.1     B
71       6.1     4.0  25.2     63.2       0.1        0.2        0.2     B
72       6.2     4.0  28.3     59.7       0.9        0.1        0.1     B
73       7.2     4.5  25.6     61.1       0.2        0.3        0.2     B
74       6.1     4.1  26.7     61.0       0.6        0.3        0.2     B
75       9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
76      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
77       9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
78      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
79       9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
80      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
81       9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
82      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
83      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
84      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
85      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
86       9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
87       9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
88       9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
89       9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
90       9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
91       9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
92      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
93       9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
94      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
95       9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
96      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
97      10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
98       9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
99       9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
100     10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
101      9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
102      9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
103     10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
104      9.6     3.3  57.7     20.7       0.2        1.5        1.8     C
105     10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
106      9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
107      9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
108      9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
109      9.7     3.4  59.3     20.5       0.1        1.5        1.2     C
110     10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
111     10.0     3.3  60.0     21.3       0.2        1.5        1.3     C
112     14.9     2.6  68.2     12.8       0.6        0.4        0.3     D
113      9.3     2.8  65.0     17.0       3.9        0.5        0.7     D
114     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
115     10.5     2.8  75.8      8.0       0.7        0.1        0.1     D
116     12.0     2.7  75.1      8.5       0.8        0.1        0.1     D
117     11.7     2.9  74.6     10.1       0.6        0.1        0.1     D
118     11.4     3.0  73.0     10.6       0.7        0.1        0.1     D
119     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
120     11.4     3.0  73.0     10.6       0.7        0.1        0.1     D
121     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
122     10.5     2.8  75.8      8.0       0.7        0.1        0.1     D
123     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
124     12.0     2.7  75.1      8.5       0.8        0.1        0.1     D
125      9.3     2.8  65.0     17.0       3.9        0.5        0.7     D
126     10.5     2.8  75.8      8.0       0.7        0.1        0.1     D
127     11.7     2.9  74.6     10.1       0.6        0.1        0.1     D
128     14.9     2.6  68.2     12.8       0.6        0.4        0.3     D
129     11.4     3.0  73.0     10.6       0.7        0.1        0.1     D
130     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
131     11.7     2.9  74.6     10.1       0.6        0.1        0.1     D
132     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
133     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
134     10.5     2.8  75.8      8.0       0.7        0.1        0.1     D
135     11.4     3.0  73.0     10.6       0.7        0.1        0.1     D
136     11.4     3.0  73.0     10.6       0.7        0.1        0.1     D
137     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
138     11.7     2.9  74.6     10.1       0.6        0.1        0.1     D
139     11.4     3.0  73.0     10.6       0.7        0.1        0.1     D
140     10.5     2.8  75.8      8.0       0.7        0.1        0.1     D
141     12.0     2.7  75.1      8.5       0.8        0.1        0.1     D
142     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
143     10.9     2.7  76.7      7.9       0.8        0.1        0.1     D
144     11.7     2.9  74.6     10.1       0.6        0.1        0.1     D
145      9.3     2.8  65.0     17.0       3.9        0.5        0.7     D
146     12.0     2.7  75.1      8.5       0.8        0.1        0.1     D
147     14.9     2.6  68.2     12.8       0.6        0.4        0.3     D
148      9.3     2.8  65.0     17.0       3.9        0.5        0.7     D
149     10.9     3.6  26.0     52.6       5.5        0.4        0.2     E
150      9.6     3.5  30.3     49.2       5.9        0.4        0.3     E
151     10.5     4.2  25.5     52.0       7.8        0.1        0.1     E
152     10.0     4.2  24.9     53.2       6.9        0.4        0.1     E
153     10.4     4.2  25.9     50.8       7.5        0.4        0.4     E
154     10.5     4.2  24.4     52.1       7.5        0.4        0.1     E
155     10.5     4.3  24.6     53.1       7.6        0.1        0.1     E
156     10.2     4.0  23.1     55.1       7.1        0.5        0.1     E
157     10.9     3.8  27.2     49.5       6.4        0.7        0.9     E
158     11.9     3.8  25.7     52.7       5.8        0.1        0.1     E
159      9.7     3.9  25.1     54.2       5.9        0.1        0.1     E
160      9.6     3.5  30.3     49.2       5.9        0.4        0.3     E
161     10.5     4.2  25.5     52.0       7.8        0.1        0.1     E
162     10.9     3.6  26.0     52.6       5.5        0.4        0.2     E
163     10.2     4.0  23.1     55.1       7.1        0.5        0.1     E
164     11.9     3.8  25.7     52.7       5.8        0.1        0.1     E
165     10.9     3.8  27.2     49.5       6.4        0.7        0.9     E
166     10.9     3.8  27.2     49.5       6.4        0.7        0.9     E
167     10.5     4.2  24.4     52.1       7.5        0.4        0.1     E
168     10.4     4.2  25.9     50.8       7.5        0.4        0.4     E
169     10.9     3.8  27.2     49.5       6.4        0.7        0.9     E
170     10.5     4.3  24.6     53.1       7.6        0.1        0.1     E
171     10.2     4.0  23.1     55.1       7.1        0.5        0.1     E
172     10.0     4.2  24.9     53.2       6.9        0.4        0.1     E
173     10.5     4.2  25.5     52.0       7.8        0.1        0.1     E
174      9.7     3.9  25.1     54.2       5.9        0.1        0.1     E
175     10.5     4.3  24.6     53.1       7.6        0.1        0.1     E
176     10.5     4.2  25.5     52.0       7.8        0.1        0.1     E
177      9.6     3.5  30.3     49.2       5.9        0.4        0.3     E
178     10.5     4.2  24.4     52.1       7.5        0.4        0.1     E
179      9.7     3.9  25.1     54.2       5.9        0.1        0.1     E
180     10.5     4.3  24.6     53.1       7.6        0.1        0.1     E
181      9.7     3.9  25.1     54.2       5.9        0.1        0.1     E
182     10.9     3.8  27.2     49.5       6.4        0.7        0.9     E
183     10.0     4.2  24.9     53.2       6.9        0.4        0.1     E
184     10.4     4.2  25.9     50.8       7.5        0.4        0.4     E
185      9.6     3.5  30.3     49.2       5.9        0.4        0.3     E
186      5.1     2.3  55.9     27.4       6.8        0.5        0.5     F
187      4.8     1.8  62.6     20.0       9.5        0.1        1.4     F
188      5.5     1.7  59.0     21.3       9.3        0.6        1.5     F
189      5.1     1.9  59.2     22.3       9.3        0.7        1.6     F
190      4.8     1.9  61.6     20.9       8.0        0.8        1.5     F
191      5.4     2.0  53.2     28.9       7.3        0.6        1.3     F
192      5.1     1.9  59.2     22.4       9.3        0.6        1.5     F
193      4.5     1.7  64.9     18.6       8.3        0.1        0.1     F
194      5.7     2.1  54.6     26.8       8.0        0.1        0.1     F
195      6.2     2.2  52.2     29.0       8.0        0.1        0.1     F
196      5.1     2.3  55.9     27.4       6.8        0.5        0.5     F
197      4.5     1.7  64.9     18.6       8.3        0.1        0.1     F
198      4.8     1.8  62.6     20.0       9.5        0.1        1.4     F
199      4.8     1.9  61.6     20.9       8.0        0.8        1.5     F
200      5.1     1.9  59.2     22.4       9.3        0.6        1.5     F
201      6.2     2.2  52.2     29.0       8.0        0.1        0.1     F
202      4.8     1.9  61.6     20.9       8.0        0.8        1.5     F
203      4.8     1.9  61.6     20.9       8.0        0.8        1.5     F
204      4.8     1.8  62.6     20.0       9.5        0.1        1.4     F
205      4.5     1.7  64.9     18.6       8.3        0.1        0.1     F
206      4.8     1.9  61.6     20.9       8.0        0.8        1.5     F
207      5.4     2.0  53.2     28.9       7.3        0.6        1.3     F
208      5.5     1.7  59.0     21.3       9.3        0.6        1.5     F
209      5.5     1.7  59.0     21.3       9.3        0.6        1.5     F
210      5.4     2.0  53.2     28.9       7.3        0.6        1.3     F
211      5.4     2.0  53.2     28.9       7.3        0.6        1.3     F
212      5.1     2.3  55.9     27.4       6.8        0.5        0.5     F
213      5.1     2.3  55.9     27.4       6.8        0.5        0.5     F
214      5.1     1.9  59.2     22.4       9.3        0.6        1.5     F
215      6.2     2.2  52.2     29.0       8.0        0.1        0.1     F
216      5.4     2.0  53.2     28.9       7.3        0.6        1.3     F
217      5.4     2.0  53.2     28.9       7.3        0.6        1.3     F
218      5.4     2.0  53.2     28.9       7.3        0.6        1.3     F
219      6.2     2.2  52.2     29.0       8.0        0.1        0.1     F
220      5.4     2.0  53.2     28.9       7.3        0.6        1.3     F
221      5.1     1.9  59.2     22.4       9.3        0.6        1.5     F
222      5.1     1.9  59.2     22.4       9.3        0.6        1.5     F
223     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
224     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
225     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
226     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
227     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
228     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
229     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
230     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
231     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
232     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
233     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
234     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
235     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
236     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
237     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
238     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
239     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
240     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
241     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
242     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
243     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
244     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
245     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
246     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
247     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
248     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
249     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
250     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
251     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
252     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
253     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
254     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
255     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
256     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
257     10.0     2.3  36.9     47.1       2.2        0.5        0.5     G
258     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
259     10.7     1.8  30.2     55.5       0.9        0.5        0.3     G
> 
> 
> 
> 
> cleanEx()
> nameEx("dummyVars")
> ### * dummyVars
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dummyVars
> ### Title: Create A Full Set of Dummy Variables
> ### Aliases: dummyVars dummyVars.default predict.dummyVars contr.dummy
> ###   contr.ltfr class2ind
> ### Keywords: models
> 
> ### ** Examples
> 
> 
> when <- data.frame(time = c("afternoon", "night", "afternoon",
+                             "morning", "morning", "morning",
+                             "morning", "afternoon", "afternoon"),
+                    day = c("Mon", "Mon", "Mon",
+                            "Wed", "Wed", "Fri",
+                            "Sat", "Sat", "Fri"))
> 
> levels(when$time) <- list(morning="morning",
+                           afternoon="afternoon",
+                           night="night")
> levels(when$day) <- list(Mon="Mon", Tue="Tue", Wed="Wed", Thu="Thu",
+                          Fri="Fri", Sat="Sat", Sun="Sun")
> 
> ## Default behavior:
> model.matrix(~day, when)
  (Intercept) dayTue dayWed dayThu dayFri daySat daySun
1           1      0      0      0      0      0      0
2           1      0      0      0      0      0      0
3           1      0      0      0      0      0      0
4           1      0      1      0      0      0      0
5           1      0      1      0      0      0      0
6           1      0      0      0      1      0      0
7           1      0      0      0      0      1      0
8           1      0      0      0      0      1      0
9           1      0      0      0      1      0      0
attr(,"assign")
[1] 0 1 1 1 1 1 1
attr(,"contrasts")
attr(,"contrasts")$day
[1] "contr.treatment"

> 
> mainEffects <- dummyVars(~ day + time, data = when)
> mainEffects
Dummy Variable Object

Formula: ~day + time
2 variables, 2 factors
Variables and levels will be separated by '.'
A less than full rank encoding is used
> predict(mainEffects, when[1:3,])
  day.Mon day.Tue day.Wed day.Thu day.Fri day.Sat day.Sun time.morning
1       1       0       0       0       0       0       0            0
2       1       0       0       0       0       0       0            0
3       1       0       0       0       0       0       0            0
  time.afternoon time.night
1              1          0
2              0          1
3              1          0
> 
> when2 <- when
> when2[1, 1] <- NA
> predict(mainEffects, when2[1:3,])
  day.Mon day.Tue day.Wed day.Thu day.Fri day.Sat day.Sun time.morning
1       1       0       0       0       0       0       0           NA
2       1       0       0       0       0       0       0            0
3       1       0       0       0       0       0       0            0
  time.afternoon time.night
1             NA         NA
2              0          1
3              1          0
> predict(mainEffects, when2[1:3,], na.action = na.omit)
  day.Mon day.Tue day.Wed day.Thu day.Fri day.Sat day.Sun time.morning
2       1       0       0       0       0       0       0            0
3       1       0       0       0       0       0       0            0
  time.afternoon time.night
2              0          1
3              1          0
> 
> 
> interactionModel <- dummyVars(~ day + time + day:time,
+                               data = when,
+                               sep = ".")
> predict(interactionModel, when[1:3,])
  day.Mon day.Tue day.Wed day.Thu day.Fri day.Sat day.Sun time.morning
1       1       0       0       0       0       0       0            0
2       1       0       0       0       0       0       0            0
3       1       0       0       0       0       0       0            0
  time.afternoon time.night day.Mon:time.morning day.Tue:time.morning
1              1          0                    0                    0
2              0          1                    0                    0
3              1          0                    0                    0
  day.Wed:time.morning day.Thu:time.morning day.Fri:time.morning
1                    0                    0                    0
2                    0                    0                    0
3                    0                    0                    0
  day.Sat:time.morning day.Sun:time.morning day.Mon:time.afternoon
1                    0                    0                      1
2                    0                    0                      0
3                    0                    0                      1
  day.Tue:time.afternoon day.Wed:time.afternoon day.Thu:time.afternoon
1                      0                      0                      0
2                      0                      0                      0
3                      0                      0                      0
  day.Fri:time.afternoon day.Sat:time.afternoon day.Sun:time.afternoon
1                      0                      0                      0
2                      0                      0                      0
3                      0                      0                      0
  day.Mon:time.night day.Tue:time.night day.Wed:time.night day.Thu:time.night
1                  0                  0                  0                  0
2                  1                  0                  0                  0
3                  0                  0                  0                  0
  day.Fri:time.night day.Sat:time.night day.Sun:time.night
1                  0                  0                  0
2                  0                  0                  0
3                  0                  0                  0
> 
> noNames <- dummyVars(~ day + time + day:time,
+                      data = when,
+                      levelsOnly = TRUE)
> predict(noNames, when)
  Mon Tue Wed Thu Fri Sat Sun morning afternoon night Mon:morning Tue:morning
1   1   0   0   0   0   0   0       0         1     0           0           0
2   1   0   0   0   0   0   0       0         0     1           0           0
3   1   0   0   0   0   0   0       0         1     0           0           0
4   0   0   1   0   0   0   0       1         0     0           0           0
5   0   0   1   0   0   0   0       1         0     0           0           0
6   0   0   0   0   1   0   0       1         0     0           0           0
7   0   0   0   0   0   1   0       1         0     0           0           0
8   0   0   0   0   0   1   0       0         1     0           0           0
9   0   0   0   0   1   0   0       0         1     0           0           0
  Wed:morning Thu:morning Fri:morning Sat:morning Sun:morning Mon:afternoon
1           0           0           0           0           0             1
2           0           0           0           0           0             0
3           0           0           0           0           0             1
4           1           0           0           0           0             0
5           1           0           0           0           0             0
6           0           0           1           0           0             0
7           0           0           0           1           0             0
8           0           0           0           0           0             0
9           0           0           0           0           0             0
  Tue:afternoon Wed:afternoon Thu:afternoon Fri:afternoon Sat:afternoon
1             0             0             0             0             0
2             0             0             0             0             0
3             0             0             0             0             0
4             0             0             0             0             0
5             0             0             0             0             0
6             0             0             0             0             0
7             0             0             0             0             0
8             0             0             0             0             1
9             0             0             0             1             0
  Sun:afternoon Mon:night Tue:night Wed:night Thu:night Fri:night Sat:night
1             0         0         0         0         0         0         0
2             0         1         0         0         0         0         0
3             0         0         0         0         0         0         0
4             0         0         0         0         0         0         0
5             0         0         0         0         0         0         0
6             0         0         0         0         0         0         0
7             0         0         0         0         0         0         0
8             0         0         0         0         0         0         0
9             0         0         0         0         0         0         0
  Sun:night
1         0
2         0
3         0
4         0
5         0
6         0
7         0
8         0
9         0
> 
> 
> 
> cleanEx()
> nameEx("extractPrediction")
> ### * extractPrediction
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.train
> ### Title: Extract predictions and class probabilities from train objects
> ### Aliases: predict.list predict.train extractPrediction extractProb
> ### Keywords: manip
> 
> ### ** Examples
> 
>    ## Not run: 
> ##D 
> ##D knnFit <- train(Species ~ ., data = iris, method = "knn", 
> ##D                 trControl = trainControl(method = "cv"))
> ##D 
> ##D rdaFit <- train(Species ~ ., data = iris, method = "rda", 
> ##D                 trControl = trainControl(method = "cv"))
> ##D 
> ##D predict(knnFit)
> ##D predict(knnFit, type = "prob")
> ##D 
> ##D bothModels <- list(knn = knnFit,
> ##D                    tree = rdaFit)
> ##D 
> ##D predict(bothModels)
> ##D 
> ##D extractPrediction(bothModels, testX = iris[1:10, -5])
> ##D extractProb(bothModels, testX = iris[1:10, -5])
> ##D   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("featurePlot")
> ### * featurePlot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: featurePlot
> ### Title: Wrapper for Lattice Plotting of Predictor Variables
> ### Aliases: featurePlot
> ### Keywords: hplot
> 
> ### ** Examples
> 
> x <- matrix(rnorm(50*5),ncol=5)
> y <- factor(rep(c("A", "B"),  25))
> 
> trellis.par.set(theme = col.whitebg(), warn = FALSE)
> featurePlot(x, y, "ellipse")
> featurePlot(x, y, "strip", jitter = TRUE)
> featurePlot(x, y, "box")
> featurePlot(x, y, "pairs")
> 
> 
> 
> cleanEx()
> nameEx("filterVarImp")
> ### * filterVarImp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: filterVarImp
> ### Title: Calculation of filter-based variable importance
> ### Aliases: filterVarImp
> ### Keywords: models
> 
> ### ** Examples
> 
> data(mdrr)
> filterVarImp(mdrrDescr[, 1:5], mdrrClass)
       Active  Inactive
MW  0.7892982 0.7892982
AMW 0.5004158 0.5004158
Sv  0.8252991 0.8252991
Se  0.7946819 0.7946819
Sp  0.8276116 0.8276116
> 
> data(BloodBrain)
> 
> filterVarImp(bbbDescr[, 1:5], logBBB, nonpara = FALSE)
           Overall
tpsa     9.6270929
nbasic   0.5097859
negative 1.0435202
vsa_hyd  4.4681448
a_aro    2.6664055
> apply(bbbDescr[, 1:5],
+       2,
+       function(x, y) summary(lm(y~x))$coefficients[2,3],
+       y = logBBB)
      tpsa     nbasic   negative    vsa_hyd      a_aro 
-9.6270929 -0.5097859 -1.0435202  4.4681448  2.6664055 
> 
> filterVarImp(bbbDescr[, 1:5], logBBB, nonpara = TRUE)
             Overall
tpsa     0.358428014
nbasic   0.001259972
negative 0.005258293
vsa_hyd  0.105859830
a_aro    0.033361774
> 
> 
> 
> cleanEx()
> nameEx("findCorrelation")
> ### * findCorrelation
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: findCorrelation
> ### Title: Determine highly correlated variables
> ### Aliases: findCorrelation
> ### Keywords: manip
> 
> ### ** Examples
> 
> R1 <- structure(c(1, 0.86, 0.56, 0.32, 0.85, 0.86, 1, 0.01, 0.74, 0.32, 
+                   0.56, 0.01, 1, 0.65, 0.91, 0.32, 0.74, 0.65, 1, 0.36,
+                   0.85, 0.32, 0.91, 0.36, 1), 
+                 .Dim = c(5L, 5L))
> colnames(R1) <- rownames(R1) <- paste0("x", 1:ncol(R1))
> R1
     x1   x2   x3   x4   x5
x1 1.00 0.86 0.56 0.32 0.85
x2 0.86 1.00 0.01 0.74 0.32
x3 0.56 0.01 1.00 0.65 0.91
x4 0.32 0.74 0.65 1.00 0.36
x5 0.85 0.32 0.91 0.36 1.00
> 
> findCorrelation(R1, cutoff = .6, exact = FALSE)
[1] 4 5 1 3
> findCorrelation(R1, cutoff = .6, exact = TRUE)
[1] 1 5 4
> findCorrelation(R1, cutoff = .6, exact = TRUE, names = FALSE)
[1] 1 5 4
> 
> 
> R2 <- diag(rep(1, 5))
> R2[2, 3] <- R2[3, 2] <- .7
> R2[5, 3] <- R2[3, 5] <- -.7
> R2[4, 1] <- R2[1, 4] <- -.67
> 
> corrDF <- expand.grid(row = 1:5, col = 1:5)
> corrDF$correlation <- as.vector(R2)
> levelplot(correlation ~ row + col, corrDF)
> 
> findCorrelation(R2, cutoff = .65, verbose = TRUE)
Compare row 3  and column  2 with corr  0.7 
  Means:  0.35 vs 0.215 so flagging column 3 
Compare row 1  and column  4 with corr  0.67 
  Means:  0.223 vs 0.074 so flagging column 1 
[1] 3 1
> 
> findCorrelation(R2, cutoff = .99, verbose = TRUE)
All correlations <= 0.99 
integer(0)
> 
> 
> 
> cleanEx()
> nameEx("findLinearCombos")
> ### * findLinearCombos
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: findLinearCombos
> ### Title: Determine linear combinations in a matrix
> ### Aliases: findLinearCombos
> ### Keywords: manip
> 
> ### ** Examples
> 
> testData1 <- matrix(0, nrow=20, ncol=8)
> testData1[,1] <- 1
> testData1[,2] <- round(rnorm(20), 1)
> testData1[,3] <- round(rnorm(20), 1)
> testData1[,4] <- round(rnorm(20), 1)
> testData1[,5] <- 0.5 * testData1[,2] - 0.25 * testData1[,3] - 0.25 * testData1[,4]
> testData1[1:4,6] <- 1
> testData1[5:10,7] <- 1
> testData1[11:20,8] <- 1
> 
> findLinearCombos(testData1)
$linearCombos
$linearCombos[[1]]
[1] 5 2 3 4

$linearCombos[[2]]
[1] 8 1 6 7


$remove
[1] 5 8

> 
> testData2 <- matrix(0, nrow=6, ncol=6)
> testData2[,1] <- c(1, 1, 1, 1, 1, 1)
> testData2[,2] <- c(1, 1, 1, 0, 0, 0)
> testData2[,3] <- c(0, 0, 0, 1, 1, 1)
> testData2[,4] <- c(1, 0, 0, 1, 0, 0)
> testData2[,5] <- c(0, 1, 0, 0, 1, 0)
> testData2[,6] <- c(0, 0, 1, 0, 0, 1)
> 
> findLinearCombos(testData2)
$linearCombos
$linearCombos[[1]]
[1] 3 1 2

$linearCombos[[2]]
[1] 6 1 4 5


$remove
[1] 3 6

> 
> 
> 
> cleanEx()
> nameEx("format.bagEarth")
> ### * format.bagEarth
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: format.bagEarth
> ### Title: Format 'bagEarth' objects
> ### Aliases: format.bagEarth
> ### Keywords: models
> 
> ### ** Examples
> 
> a <- bagEarth(Volume ~ ., data = trees, B= 3)
> format(a)
(   27.72291
  -  2.641467 * h(13.3-Girth)
  +  5.984334 * h(Girth-13.3)
  - 0.4128273 * h(80-Height)
+  28.45178
  -  3.722442 * h(13.7-Girth)
  +  5.735805 * h(Girth-13.7)
  + 0.5571811 * h(Height-75)
+  24.48584
  +  4.215641 * h(Girth-12)
  -  1.724185 * h(14.5-Girth)
  - 0.3803952 * h(75-Height)
  +  1.084108 * h(Height-75)
 ) / 3 
> 
> # yields:
> # (
> #   31.61075 
> #   +  6.587273 * pmax(0,  Girth -   14.2) 
> #   -  3.229363 * pmax(0,   14.2 -  Girth) 
> #   - 0.3167140 * pmax(0,     79 - Height) 
> #   +
> #    22.80225 
> #   +  5.309866 * pmax(0,  Girth -     12) 
> #   -  2.378658 * pmax(0,     12 -  Girth) 
> #   +  0.793045 * pmax(0, Height -     80) 
> #   - 0.3411915 * pmax(0,     80 - Height) 
> #   +
> #    31.39772 
> #   +   6.18193 * pmax(0,  Girth -   14.2) 
> #   -  3.660456 * pmax(0,   14.2 -  Girth) 
> #   + 0.6489774 * pmax(0, Height -     80) 
> # )/3
> 
> 
> 
> 
> cleanEx()
> nameEx("ga_functions")
> ### * ga_functions
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gafs_initial
> ### Title: Ancillary genetic algorithm functions
> ### Aliases: gafs_initial gafs_lrSelection gafs_rwSelection
> ###   gafs_tourSelection gafs_uCrossover gafs_spCrossover gafs_raMutation
> ###   caretGA rfGA treebagGA
> 
> ### ** Examples
> 
> pop <- gafs_initial(vars = 10, popSize = 10)
> pop
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]    0    0    0    1    0    0    1    0    0     0
 [2,]    0    0    0    0    0    0    0    1    0     0
 [3,]    1    0    0    0    0    0    0    0    1     0
 [4,]    0    0    0    0    1    1    1    0    1     0
 [5,]    1    1    1    1    0    1    0    0    1     1
 [6,]    1    0    1    1    1    1    1    1    0     1
 [7,]    0    1    1    1    0    1    1    0    1     0
 [8,]    1    0    1    1    1    0    0    1    0     0
 [9,]    1    1    1    1    1    1    1    1    1     1
[10,]    1    1    1    1    1    1    1    1    1     1
> 
> gafs_lrSelection(population = pop, fitness = 1:10)
$population
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]    1    0    0    0    0    0    0    0    1     0
 [2,]    1    0    1    1    1    1    1    1    0     1
 [3,]    1    1    1    1    1    1    1    1    1     1
 [4,]    1    0    1    1    1    0    0    1    0     0
 [5,]    1    1    1    1    1    1    1    1    1     1
 [6,]    1    1    1    1    1    1    1    1    1     1
 [7,]    1    0    1    1    1    1    1    1    0     1
 [8,]    1    1    1    1    1    1    1    1    1     1
 [9,]    1    0    1    1    1    0    0    1    0     0
[10,]    0    1    1    1    0    1    1    0    1     0

$fitness
 [1]  3  6  9  8 10 10  6 10  8  7

> 
> gafs_spCrossover(population = pop, fitness = 1:10, parents = 1:2)
$children
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    0    0    0    1    0    0    1    0    0     0
[2,]    0    0    0    0    0    0    0    1    0     0

$fitness
[1] 1 2

> 
> 
> ## Not run: 
> ##D ## Hypothetical examples
> ##D lda_ga <- gafs(x = predictors,
> ##D                y = classes,
> ##D                gafsControl = gafsControl(functions = caretGA),
> ##D                ## now pass arguments to `train`
> ##D                method = "lda",
> ##D                metric = "Accuracy"
> ##D                trControl = trainControl(method = "cv", classProbs = TRUE))
> ##D 
> ##D rf_ga <- gafs(x = predictors,
> ##D               y = classes,
> ##D               gafsControl = gafsControl(functions = rfGA),
> ##D               ## these are arguments to `randomForest`
> ##D               ntree = 1000,
> ##D               importance = TRUE)
> ##D 	
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("gafs.default")
> ### * gafs.default
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gafs.default
> ### Title: Genetic algorithm feature selection
> ### Aliases: gafs.default gafs
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1)
> ##D train_data <- twoClassSim(100, noiseVars = 10)
> ##D test_data  <- twoClassSim(10,  noiseVars = 10)
> ##D 
> ##D ## A short example 
> ##D ctrl <- gafsControl(functions = rfGA, 
> ##D                     method = "cv",
> ##D                     number = 3)
> ##D 
> ##D rf_search <- gafs(x = train_data[, -ncol(train_data)],
> ##D                   y = train_data$Class,
> ##D                   iters = 3,
> ##D                   gafsControl = ctrl)
> ##D 
> ##D rf_search  
> ##D   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("icr")
> ### * icr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: icr.formula
> ### Title: Independent Component Regression
> ### Aliases: icr.formula icr.default icr predict.icr
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> data(BloodBrain)
> 
> icrFit <- icr(bbbDescr, logBBB, n.comp = 5)
> 
> icrFit
Independent Component Regression

Created from 208 samples and 5 variables

Coefficients:
(Intercept)         ICA1         ICA2         ICA3         ICA4         ICA5  
   -0.01889      0.25135      0.26362     -0.21473     -0.08874      0.20896  

> 
> predict(icrFit, bbbDescr[1:5,])
         1          2          3          4          5 
 0.3826363 -0.3495597  0.1480733  0.2969892  0.1946560 
> 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("index2vec")
> ### * index2vec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: index2vec
> ### Title: Convert indicies to a binary vector
> ### Aliases: index2vec
> 
> ### ** Examples
> 
> index2vec(x = 1:2, vars = 5)
[1] 1 1 0 0 0
> index2vec(x = 1:2, vars = 5, sign = TRUE)
[1]  1  1 -1 -1 -1
> 
> 
> 
> cleanEx()
> nameEx("knn3")
> ### * knn3
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: knn3
> ### Title: k-Nearest Neighbour Classification
> ### Aliases: knn3 knn3.formula knn3.matrix knn3.data.frame knn3Train
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> irisFit1 <- knn3(Species ~ ., iris)
> 
> irisFit2 <- knn3(as.matrix(iris[, -5]), iris[,5])
> 
> data(iris3)
> train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
> test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
> cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
> knn3Train(train, test, cl, k = 5, prob = TRUE) 
 [1] "s" "s" "s" "s" "s" "s" "s" "s" "s" "s" "s" "s" "s" "s" "s" "s" "s" "s" "s"
[20] "s" "s" "s" "s" "s" "s" "c" "c" "v" "c" "c" "c" "c" "c" "v" "c" "c" "c" "c"
[39] "c" "c" "c" "c" "c" "c" "c" "c" "c" "c" "c" "c" "v" "c" "c" "v" "v" "v" "v"
[58] "v" "c" "v" "v" "v" "v" "c" "v" "v" "v" "v" "v" "v" "v" "v" "v" "v" "v"
attr(,"prob")
              c s         v
 [1,] 0.0000000 1 0.0000000
 [2,] 0.0000000 1 0.0000000
 [3,] 0.0000000 1 0.0000000
 [4,] 0.0000000 1 0.0000000
 [5,] 0.0000000 1 0.0000000
 [6,] 0.0000000 1 0.0000000
 [7,] 0.0000000 1 0.0000000
 [8,] 0.0000000 1 0.0000000
 [9,] 0.0000000 1 0.0000000
[10,] 0.0000000 1 0.0000000
[11,] 0.0000000 1 0.0000000
[12,] 0.0000000 1 0.0000000
[13,] 0.0000000 1 0.0000000
[14,] 0.0000000 1 0.0000000
[15,] 0.0000000 1 0.0000000
[16,] 0.0000000 1 0.0000000
[17,] 0.0000000 1 0.0000000
[18,] 0.0000000 1 0.0000000
[19,] 0.0000000 1 0.0000000
[20,] 0.0000000 1 0.0000000
[21,] 0.0000000 1 0.0000000
[22,] 0.0000000 1 0.0000000
[23,] 0.0000000 1 0.0000000
[24,] 0.0000000 1 0.0000000
[25,] 0.0000000 1 0.0000000
[26,] 1.0000000 0 0.0000000
[27,] 1.0000000 0 0.0000000
[28,] 0.4000000 0 0.6000000
[29,] 1.0000000 0 0.0000000
[30,] 1.0000000 0 0.0000000
[31,] 1.0000000 0 0.0000000
[32,] 1.0000000 0 0.0000000
[33,] 1.0000000 0 0.0000000
[34,] 0.4000000 0 0.6000000
[35,] 0.8000000 0 0.2000000
[36,] 1.0000000 0 0.0000000
[37,] 1.0000000 0 0.0000000
[38,] 1.0000000 0 0.0000000
[39,] 1.0000000 0 0.0000000
[40,] 1.0000000 0 0.0000000
[41,] 1.0000000 0 0.0000000
[42,] 1.0000000 0 0.0000000
[43,] 1.0000000 0 0.0000000
[44,] 1.0000000 0 0.0000000
[45,] 1.0000000 0 0.0000000
[46,] 1.0000000 0 0.0000000
[47,] 1.0000000 0 0.0000000
[48,] 1.0000000 0 0.0000000
[49,] 1.0000000 0 0.0000000
[50,] 1.0000000 0 0.0000000
[51,] 0.0000000 0 1.0000000
[52,] 0.8000000 0 0.2000000
[53,] 0.6000000 0 0.4000000
[54,] 0.0000000 0 1.0000000
[55,] 0.0000000 0 1.0000000
[56,] 0.0000000 0 1.0000000
[57,] 0.0000000 0 1.0000000
[58,] 0.0000000 0 1.0000000
[59,] 0.6666667 0 0.3333333
[60,] 0.2000000 0 0.8000000
[61,] 0.0000000 0 1.0000000
[62,] 0.0000000 0 1.0000000
[63,] 0.0000000 0 1.0000000
[64,] 0.6000000 0 0.4000000
[65,] 0.0000000 0 1.0000000
[66,] 0.0000000 0 1.0000000
[67,] 0.0000000 0 1.0000000
[68,] 0.0000000 0 1.0000000
[69,] 0.0000000 0 1.0000000
[70,] 0.0000000 0 1.0000000
[71,] 0.0000000 0 1.0000000
[72,] 0.2000000 0 0.8000000
[73,] 0.0000000 0 1.0000000
[74,] 0.0000000 0 1.0000000
[75,] 0.2000000 0 0.8000000
> 
> 
> 
> cleanEx()
> nameEx("knnreg")
> ### * knnreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: knnreg
> ### Title: k-Nearest Neighbour Regression
> ### Aliases: knnreg knnregTrain knnreg.formula knnreg.default knnreg.matrix
> ###   knnreg.data.frame knnreg
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> data(BloodBrain)
> 
> inTrain <- createDataPartition(logBBB, p = .8)[[1]]
> 
> trainX <- bbbDescr[inTrain,]
> trainY <- logBBB[inTrain]
> 
> testX <- bbbDescr[-inTrain,]
> testY <- logBBB[-inTrain]
> 
> fit <- knnreg(trainX, trainY, k = 3)
> 
> plot(testY, predict(fit, testX))   
> 
> 
> 
> cleanEx()
> nameEx("lattice")
> ### * lattice
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: histogram.train
> ### Title: Lattice functions for plotting resampling results
> ### Aliases: stripplot.train xyplot.train densityplot.train histogram.train
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D 
> ##D library(mlbench)
> ##D data(BostonHousing)
> ##D 
> ##D library(rpart)
> ##D rpartFit <- train(medv ~ .,
> ##D                   data = BostonHousing,
> ##D                   "rpart", 
> ##D                   tuneLength = 9,
> ##D                   trControl = trainControl(
> ##D                     method = "boot", 
> ##D                     returnResamp = "all"))
> ##D 
> ##D densityplot(rpartFit,
> ##D             adjust = 1.25)
> ##D 
> ##D xyplot(rpartFit,
> ##D        metric = "Rsquared",
> ##D        type = c("p", "a"))
> ##D 
> ##D stripplot(rpartFit,
> ##D           horizontal = FALSE,
> ##D           jitter = TRUE)
> ##D 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lattice.diff.resamples")
> ### * lattice.diff.resamples
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dotplot.diff.resamples
> ### Title: Lattice Functions for Visualizing Resampling Differences
> ### Aliases: levelplot.diff.resamples densityplot.diff.resamples
> ###   bwplot.diff.resamples dotplot.diff.resamples
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D #load(url("http://topepo.github.io/caret/exampleModels.RData"))
> ##D 
> ##D resamps <- resamples(list(CART = rpartFit,
> ##D                           CondInfTree = ctreeFit,
> ##D                           MARS = earthFit))
> ##D difs <- diff(resamps)
> ##D 
> ##D dotplot(difs)
> ##D 
> ##D densityplot(difs,
> ##D             metric = "RMSE",
> ##D             auto.key = TRUE,
> ##D             pch = "|")
> ##D 
> ##D bwplot(difs,
> ##D        metric = "RMSE")
> ##D 
> ##D levelplot(difs, what = "differences")
> ##D 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lattice.resamples")
> ### * lattice.resamples
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: xyplot.resamples
> ### Title: Lattice Functions for Visualizing Resampling Results
> ### Aliases: xyplot.resamples densityplot.resamples bwplot.resamples
> ###   splom.resamples parallelplot.resamples dotplot.resamples
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D #load(url("http://topepo.github.io/caret/exampleModels.RData"))
> ##D 
> ##D resamps <- resamples(list(CART = rpartFit,
> ##D                           CondInfTree = ctreeFit,
> ##D                           MARS = earthFit))
> ##D 
> ##D dotplot(resamps, 
> ##D         scales =list(x = list(relation = "free")), 
> ##D         between = list(x = 2))
> ##D 
> ##D bwplot(resamps,
> ##D        metric = "RMSE")
> ##D 
> ##D densityplot(resamps,
> ##D             auto.key = list(columns = 3),
> ##D             pch = "|")
> ##D 
> ##D xyplot(resamps,
> ##D        models = c("CART", "MARS"),
> ##D        metric = "RMSE")
> ##D 
> ##D splom(resamps, metric = "RMSE")
> ##D splom(resamps, variables = "metrics")
> ##D 
> ##D parallelplot(resamps, metric = "RMSE")
> ##D 
> ##D 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lattice.rfe")
> ### * lattice.rfe
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lattice.rfe
> ### Title: Lattice functions for plotting resampling results of recursive
> ###   feature selection
> ### Aliases: xyplot.rfe stripplot.rfe densityplot.rfe histogram.rfe
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D library(mlbench)
> ##D n <- 100
> ##D p <- 40
> ##D sigma <- 1
> ##D set.seed(1)
> ##D sim <- mlbench.friedman1(n, sd = sigma)
> ##D x <- cbind(sim$x,  matrix(rnorm(n * p), nrow = n))
> ##D y <- sim$y
> ##D colnames(x) <- paste("var", 1:ncol(x), sep = "")
> ##D 
> ##D normalization <- preProcess(x)
> ##D x <- predict(normalization, x)
> ##D x <- as.data.frame(x)
> ##D subsets <- c(10, 15, 20, 25)
> ##D 
> ##D ctrl <- rfeControl(
> ##D                    functions = lmFuncs,
> ##D                    method = "cv",
> ##D                    verbose = FALSE,
> ##D                    returnResamp = "all")
> ##D 
> ##D lmProfile <- rfe(x, y,
> ##D                  sizes = subsets,
> ##D                  rfeControl = ctrl)
> ##D xyplot(lmProfile)
> ##D stripplot(lmProfile)
> ##D 
> ##D histogram(lmProfile)
> ##D densityplot(lmProfile)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("learning_curve")
> ### * learning_curve
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: learing_curve_dat
> ### Title: Create Data to Plot a Learning Curve
> ### Aliases: learing_curve_dat
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1412)
> ##D class_dat <- twoClassSim(1000)
> ##D 
> ##D set.seed(29510)
> ##D lda_data <- learing_curve_dat(dat = class_dat, 
> ##D                               outcome = "Class",
> ##D                               test_prop = 1/4, 
> ##D                               ## `train` arguments:
> ##D                               method = "lda", 
> ##D                               metric = "ROC",
> ##D                               trControl = trainControl(classProbs = TRUE, 
> ##D                                                        summaryFunction = twoClassSummary))
> ##D 
> ##D 
> ##D 
> ##D ggplot(lda_data, aes(x = Training_Size, y = ROC, color = Data)) + 
> ##D   geom_smooth(method = loess, span = .8) + 
> ##D   theme_bw()
> ##D  
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lift")
> ### * lift
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lift
> ### Title: Lift Plot
> ### Aliases: lift lift.formula lift.default xyplot.lift
> ### Keywords: hplot
> 
> ### ** Examples
> 
> set.seed(1)
> simulated <- data.frame(obs = factor(rep(letters[1:2], each = 100)),
+                         perfect = sort(runif(200), decreasing = TRUE),
+                         random = runif(200))
> 
> lift1 <- lift(obs ~ random, data = simulated)
> lift1

Call:
lift.formula(x = obs ~ random, data = simulated)

Models: random 
Event: a (50%)
> xyplot(lift1)
> 
> lift2 <- lift(obs ~ random + perfect, data = simulated)
> lift2

Call:
lift.formula(x = obs ~ random + perfect, data = simulated)

Models: random, perfect 
Event: a (50%)
> xyplot(lift2, auto.key = list(columns = 2))
> 
> xyplot(lift2, auto.key = list(columns = 2), value = c(10, 30))
> 
> xyplot(lift2, plot = "lift", auto.key = list(columns = 2))
> 
> 
> 
> cleanEx()
> nameEx("maxDissim")
> ### * maxDissim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: maxDissim
> ### Title: Maximum Dissimilarity Sampling
> ### Aliases: maxDissim minDiss sumDiss
> ### Keywords: utilities
> 
> ### ** Examples
> 
> 
> example <- function(pct = 1, obj = minDiss, ...)
+ {
+   tmp <- matrix(rnorm(200 * 2), nrow = 200)
+ 
+   ## start with 15 data points
+   start <- sample(1:dim(tmp)[1], 15)
+   base <- tmp[start,]
+   pool <- tmp[-start,]
+   
+   ## select 9 for addition
+   newSamp <- maxDissim(
+                        base, pool, 
+                        n = 9, 
+                        randomFrac = pct, obj = obj, ...)
+   
+   allSamp <- c(start, newSamp)
+   
+   plot(
+        tmp[-newSamp,], 
+        xlim = extendrange(tmp[,1]), ylim = extendrange(tmp[,2]), 
+        col = "darkgrey", 
+        xlab = "variable 1", ylab = "variable 2")
+   points(base, pch = 16, cex = .7)
+   
+   for(i in seq(along = newSamp))
+     points(
+            pool[newSamp[i],1], 
+            pool[newSamp[i],2], 
+            pch = paste(i), col = "darkred") 
+ }
> 
> par(mfrow=c(2,2))
> 
> set.seed(414)
> example(1, minDiss)
> title("No Random Sampling, Min Score")
> 
> set.seed(414)
> example(.1, minDiss)
> title("10 Pct Random Sampling, Min Score")
> 
> set.seed(414)
> example(1, sumDiss)
> title("No Random Sampling, Sum Score")
> 
> set.seed(414)
> example(.1, sumDiss)
> title("10 Pct Random Sampling, Sum Score")
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("modelLookup")
> ### * modelLookup
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: modelLookup
> ### Title: Tools for Models Available in 'train'
> ### Aliases: modelLookup getModelInfo checkInstall
> ### Keywords: utilities
> 
> ### ** Examples
> 
> modelLookup()
                  model           parameter
10                ANFIS          num.labels
11                ANFIS            max.iter
4                AdaBag              mfinal
5                AdaBag            maxdepth
6           AdaBoost.M1              mfinal
7           AdaBoost.M1            maxdepth
8           AdaBoost.M1           coeflearn
40               Boruta                mtry
43                BstLm               mstop
44                BstLm                  nu
50                 C5.0              trials
51                 C5.0               model
52                 C5.0              winnow
53             C5.0Cost              trials
54             C5.0Cost               model
55             C5.0Cost              winnow
56             C5.0Cost                cost
57            C5.0Rules           parameter
58             C5.0Tree           parameter
63               CSimca           parameter
68               DENFIS                Dthr
69               DENFIS            max.iter
98              FH.GBML        max.num.rule
99              FH.GBML           popu.size
100             FH.GBML             max.gen
101              FIR.DM          num.labels
102              FIR.DM            max.iter
105           FRBCS.CHI          num.labels
106           FRBCS.CHI             type.mf
107             FRBCS.W          num.labels
108             FRBCS.W             type.mf
109              FS.HGD          num.labels
110              FS.HGD            max.iter
127        GFS.FR.MOGUL             max.gen
128        GFS.FR.MOGUL            max.iter
129        GFS.FR.MOGUL            max.tune
130            GFS.GCCL          num.labels
131            GFS.GCCL           popu.size
132            GFS.GCCL             max.gen
133           GFS.LT.RS           popu.size
134           GFS.LT.RS          num.labels
135           GFS.LT.RS             max.gen
136          GFS.THRIFT           popu.size
137          GFS.THRIFT          num.labels
138          GFS.THRIFT             max.gen
151               HYFIS          num.labels
152               HYFIS            max.iter
154                 J48                   C
155                JRip              NumOpt
176                 LMT                iter
173               Linda           parameter
180          LogitBoost               nIter
189                  M5              pruned
190                  M5            smoothed
191                  M5               rules
192             M5Rules              pruned
193             M5Rules            smoothed
195                Mlda           parameter
226              ORFlog                mtry
227              ORFpls                mtry
228            ORFridge                mtry
229              ORFsvm                mtry
225                OneR           parameter
233                PART           threshold
234                PART              pruned
244        PenalizedLDA              lambda
245        PenalizedLDA                   K
262              QdaCov           parameter
277               RFlda                   q
302                 RRF                mtry
303                 RRF             coefReg
304                 RRF             coefImp
305           RRFglobal                mtry
306           RRFglobal             coefReg
310              RSimca           parameter
315                 SBC                 r.a
316                 SBC            eps.high
317                 SBC             eps.low
325               SLAVE          num.labels
326               SLAVE            max.iter
327               SLAVE             max.gen
373                  WM          num.labels
374                  WM             type.mf
1                   ada                iter
2                   ada            maxdepth
3                   ada                  nu
9                 amdai               model
12               avNNet                size
13               avNNet               decay
14               avNNet                 bag
15                 awnb              smooth
16                awtan               score
17                awtan              smooth
18                  bag                vars
19             bagEarth              nprune
20             bagEarth              degree
21          bagEarthGCV              degree
22               bagFDA              degree
23               bagFDA              nprune
24            bagFDAGCV              degree
25          bartMachine           num_trees
26          bartMachine                   k
27          bartMachine               alpha
28          bartMachine                beta
29          bartMachine                  nu
30             bayesglm           parameter
31                  bdk                xdim
32                  bdk                ydim
33                  bdk             xweight
34                  bdk                topo
35                binda        lambda.freqs
36           blackboost               mstop
37           blackboost            maxdepth
38               blasso            sparsity
39       blassoAveraged           parameter
41               bridge           parameter
42                 brnn             neurons
45                bstSm               mstop
46                bstSm                  nu
47              bstTree               mstop
48              bstTree            maxdepth
49              bstTree                  nu
59              cforest                mtry
60                chaid              alpha2
61                chaid              alpha3
62                chaid              alpha4
64                ctree        mincriterion
65               ctree2            maxdepth
66               cubist          committees
67               cubist           neighbors
70                  dnn              layer1
71                  dnn              layer2
72                  dnn              layer3
73                  dnn      hidden_dropout
74                  dnn     visible_dropout
75            dwdLinear              lambda
76            dwdLinear                qval
77              dwdPoly              lambda
78              dwdPoly                qval
79              dwdPoly              degree
80              dwdPoly               scale
81            dwdRadial              lambda
82            dwdRadial                qval
83            dwdRadial               sigma
84                earth              nprune
85                earth              degree
86                  elm                nhid
87                  elm              actfun
88                 enet            fraction
89                 enet              lambda
92                enpls             maxcomp
90             enpls.fs             maxcomp
91             enpls.fs           threshold
93               evtree               alpha
94           extraTrees                mtry
95           extraTrees       numRandomCuts
96                  fda              degree
97                  fda              nprune
103                foba                   k
104                foba              lambda
111                 gam              select
112                 gam              method
115            gamLoess                span
116            gamLoess              degree
117           gamSpline                  df
113            gamboost               mstop
114            gamboost               prune
118       gaussprLinear           parameter
119         gaussprPoly              degree
120         gaussprPoly               scale
121       gaussprRadial               sigma
122                 gbm             n.trees
123                 gbm   interaction.depth
124                 gbm           shrinkage
125                 gbm      n.minobsinnode
126            gcvEarth              degree
139                 glm           parameter
144          glmStepAIC           parameter
140            glmboost               mstop
141            glmboost               prune
142              glmnet               alpha
143              glmnet              lambda
145                gpls              K.prov
146                 hda               gamma
147                 hda              lambda
148                 hda              newdim
149                hdda           threshold
150                hdda               model
153                 icr              n.comp
156           kernelpls               ncomp
157                kknn                kmax
158                kknn            distance
159                kknn              kernel
160                 knn                   k
161            krlsPoly              lambda
162            krlsPoly              degree
163          krlsRadial              lambda
164          krlsRadial               sigma
165                lars            fraction
166               lars2                step
167               lasso            fraction
168                 lda           parameter
169                lda2               dimen
170        leapBackward               nvmax
171         leapForward               nvmax
172             leapSeq               nvmax
174                  lm           parameter
175           lmStepAIC           parameter
177              loclda                   k
178            logicBag             nleaves
179            logicBag              ntrees
181              logreg            treesize
182              logreg              ntrees
183         lssvmLinear           parameter
184           lssvmPoly              degree
185           lssvmPoly               scale
186         lssvmRadial               sigma
187                 lvq                size
188                 lvq                   k
194                 mda          subclasses
196                 mlp                size
197               mlpML              layer1
198               mlpML              layer2
199               mlpML              layer3
200      mlpWeightDecay                size
201      mlpWeightDecay               decay
202    mlpWeightDecayML              layer1
203    mlpWeightDecayML              layer2
204    mlpWeightDecayML              layer3
205    mlpWeightDecayML               decay
206            multinom               decay
207                  nb                  fL
208                  nb           usekernel
209          nbDiscrete              smooth
210            nbSearch                   k
211            nbSearch             epsilon
212            nbSearch              smooth
213            nbSearch        final_smooth
214            nbSearch           direction
215           neuralnet              layer1
216           neuralnet              layer2
217           neuralnet              layer3
218                nnet                size
219                nnet               decay
220                nnls           parameter
221         nodeHarvest            maxinter
222         nodeHarvest                mode
223        oblique.tree      oblique.splits
224        oblique.tree  variable.selection
230                ownn                   K
231                 pam           threshold
232               parRF                mtry
235             partDSA      cut.off.growth
236             partDSA                 MPD
237             pcaNNet                size
238             pcaNNet               decay
239                 pcr               ncomp
240                 pda              lambda
241                pda2                  df
242           penalized             lambda1
243           penalized             lambda2
246                 plr              lambda
247                 plr                  cp
248                 pls               ncomp
249             plsRglm                  nt
250             plsRglm   alpha.pvals.expli
251                polr           parameter
252                 ppr              nterms
253          protoclass                 eps
254          protoclass           Minkowski
255        pythonKnnReg         n_neighbors
256        pythonKnnReg             weights
257        pythonKnnReg           algorithm
258        pythonKnnReg           leaf_size
259        pythonKnnReg              metric
260        pythonKnnReg                   p
261                 qda           parameter
263                 qrf                mtry
264                qrnn            n.hidden
265                qrnn             penalty
266                qrnn                 bag
276              rFerns               depth
267           randomGLM maxInteractionOrder
268              ranger                mtry
269                 rbf                size
270              rbfDDA   negativeThreshold
271                 rda               gamma
272                 rda              lambda
273              relaxo              lambda
274              relaxo                 phi
275                  rf                mtry
278             rfRules                mtry
279             rfRules            maxdepth
280               ridge              lambda
281                rknn                   k
282                rknn                mtry
283             rknnBel                   k
284             rknnBel                mtry
285             rknnBel                   d
286                 rlm           parameter
287                rmda                   K
288                rmda               model
289                rocc              xgenes
290      rotationForest                   K
291      rotationForest                   L
292    rotationForestCp                   K
293    rotationForestCp                   L
294    rotationForestCp                  cp
295               rpart                  cp
296              rpart2            maxdepth
297           rpartCost                  cp
298           rpartCost                Cost
299             rqlasso              lambda
300                rqnc              lambda
301                rqnc             penalty
307               rrlda              lambda
308               rrlda                  hp
309               rrlda             penalty
311           rvmLinear           parameter
312             rvmPoly               scale
313             rvmPoly              degree
314           rvmRadial               sigma
318                 sda            diagonal
319                 sda              lambda
320             sddaLDA           parameter
321             sddaQDA           parameter
322                sdwd              lambda
323                sdwd             lambda2
324              simpls               ncomp
328                slda           parameter
329                smda             NumVars
330                smda              lambda
331                smda                   R
332                 snn              lambda
333           sparseLDA             NumVars
334           sparseLDA              lambda
335           spikeslab                vars
336                spls                   K
337                spls                 eta
338                spls               kappa
339             stepLDA              maxvar
340             stepLDA           direction
341             stepQDA              maxvar
342             stepQDA           direction
343             superpc           threshold
344             superpc        n.components
345 svmBoundrangeString              length
346 svmBoundrangeString                   C
347       svmExpoString              lambda
348       svmExpoString                   C
349           svmLinear                   C
350          svmLinear2                cost
351          svmLinear2               gamma
352             svmPoly              degree
353             svmPoly               scale
354             svmPoly                   C
355           svmRadial               sigma
356           svmRadial                   C
357       svmRadialCost                   C
358    svmRadialWeights               sigma
359    svmRadialWeights                   C
360    svmRadialWeights              Weight
361   svmSpectrumString              length
362   svmSpectrumString                   C
363                 tan               score
364                 tan              smooth
365           tanSearch                   k
366           tanSearch             epsilon
367           tanSearch              smooth
368           tanSearch        final_smooth
369           tanSearch                  sp
370             treebag           parameter
371          vbmpRadial       estimateTheta
372       widekernelpls               ncomp
375                wsrf                mtry
376           xgbLinear             nrounds
377           xgbLinear              lambda
378           xgbLinear               alpha
379             xgbTree             nrounds
380             xgbTree           max_depth
381             xgbTree                 eta
382             xgbTree               gamma
383             xgbTree    colsample_bytree
384             xgbTree    min_child_weight
385                 xyf                xdim
386                 xyf                ydim
387                 xyf             xweight
388                 xyf                topo
                                                                                      label
10                                                                             #Fuzzy Terms
11                                                                          Max. Iterations
4                                                                                    #Trees
5                                                                            Max Tree Depth
6                                                                                    #Trees
7                                                                            Max Tree Depth
8                                                                          Coefficient Type
40                                                            #Randomly Selected Predictors
43                                                                    # Boosting Iterations
44                                                                                Shrinkage
50                                                                    # Boosting Iterations
51                                                                               Model Type
52                                                                                   Winnow
53                                                                    # Boosting Iterations
54                                                                               Model Type
55                                                                                   Winnow
56                                                                                     Cost
57                                                                                     none
58                                                                                     none
63                                                                                parameter
68                                                                                Threshold
69                                                                          Max. Iterations
98                                                                              Max. #Rules
99                                                                          Population Size
100                                                                        Max. Generations
101                                                                            #Fuzzy Terms
102                                                                         Max. Iterations
105                                                                            #Fuzzy Terms
106                                                                     Membership Function
107                                                                            #Fuzzy Terms
108                                                                     Membership Function
109                                                                            #Fuzzy Terms
110                                                                         Max. Iterations
127                                                                        Max. Generations
128                                                                         Max. Iterations
129                                                                  Max. Tuning Iterations
130                                                                            #Fuzzy Terms
131                                                                         Population Size
132                                                                        Max. Generations
133                                                                         Population Size
134                                                                          # Fuzzy Labels
135                                                                        Max. Generations
136                                                                         Population Size
137                                                                          # Fuzzy Labels
138                                                                        Max. Generations
151                                                                            #Fuzzy Terms
152                                                                         Max. Iterations
154                                                                    Confidence Threshold
155                                                                         # Optimizations
176                                                                             # Iteratons
173                                                                                    none
180                                                                   # Boosting Iterations
189                                                                                  Pruned
190                                                                                Smoothed
191                                                                                   Rules
192                                                                                  Pruned
193                                                                                Smoothed
195                                                                               parameter
226                                                           #Randomly Selected Predictors
227                                                           #Randomly Selected Predictors
228                                                           #Randomly Selected Predictors
229                                                           #Randomly Selected Predictors
225                                                                                    none
233                                                                    Confidence Threshold
234                                                                    Confidence Threshold
244                                                                              L1 Penalty
245                                                                 #Discriminant Functions
262                                                                               parameter
277                                                                               # Factors
302                                                           #Randomly Selected Predictors
303                                                                    Regularization Value
304                                                                  Importance Coefficient
305                                                           #Randomly Selected Predictors
306                                                                    Regularization Value
310                                                                               parameter
315                                                                                  Radius
316                                                                         Upper Threshold
317                                                                         Lower Threshold
325                                                                            #Fuzzy Terms
326                                                                         Max. Iterations
327                                                                        Max. Generations
373                                                                            #Fuzzy Terms
374                                                                     Membership Function
1                                                                                    #Trees
2                                                                            Max Tree Depth
3                                                                             Learning Rate
9                                                                                Model Type
12                                                                            #Hidden Units
13                                                                             Weight Decay
14                                                                                  Bagging
15                                                                      Smoothing Parameter
16                                                                           Score Function
17                                                                      Smoothing Parameter
18                                                            #Randomly Selected Predictors
19                                                                                   #Terms
20                                                                           Product Degree
21                                                                           Product Degree
22                                                                           Product Degree
23                                                                                   #Terms
24                                                                           Product Degree
25                                                                                   #Trees
26                                                                           Prior Boundary
27                                                        Base Terminal Node Hyperparameter
28                                                       Power Terminal Node Hyperparameter
29                                                                       Degrees of Freedom
30                                                                                parameter
31                                                                                      Row
32                                                                                  Columns
33                                                                                 X Weight
34                                                                                 Topology
35                                                                      Shrinkage Intensity
36                                                                                   #Trees
37                                                                           Max Tree Depth
38                                                                       Sparsity Threshold
39                                                                                parameter
41                                                                                parameter
42                                                                                # Neurons
45                                                                    # Boosting Iterations
46                                                                                Shrinkage
47                                                                    # Boosting Iterations
48                                                                           Max Tree Depth
49                                                                                Shrinkage
59                                                            #Randomly Selected Predictors
60                                                                        Merging Threshold
61                                                        Splitting former Merged Threshold
62  \n                                                    Splitting former Merged Threshold
64                                                                    1 - P-Value Threshold
65                                                                           Max Tree Depth
66                                                                              #Committees
67                                                                               #Instances
70                                                                           Hidden Layer 1
71                                                                           Hidden Layer 2
72                                                                           Hidden Layer 3
73                                                                          Hidden Dropouts
74                                                                          Visible Dropout
75                                                                 Regularization Parameter
76                                                                                        q
77                                                                 Regularization Parameter
78                                                                                        q
79                                                                        Polynomial Degree
80                                                                                    Scale
81                                                                 Regularization Parameter
82                                                                                        q
83                                                                                    Sigma
84                                                                                   #Terms
85                                                                           Product Degree
86                                                                            #Hidden Units
87                                                                      Activation Function
88                                                                Fraction of Full Solution
89                                                                             Weight Decay
92                                                                         Max. #Components
90                                                                         Max. #Components
91                                                                        Importance Cutoff
93                                                                     Complexity Parameter
94                                                           # Randomly Selected Predictors
95                                                                            # Random Cuts
96                                                                           Product Degree
97                                                                                   #Terms
103                                                                     #Variables Retained
104                                                                              L2 Penalty
111                                                                       Feature Selection
112                                                                                  Method
115                                                                                    Span
116                                                                                  Degree
117                                                                      Degrees of Freedom
113                                                                   # Boosting Iterations
114                                                                              AIC Prune?
118                                                                               Parameter
119                                                                       Polynomial Degree
120                                                                                   Scale
121                                                                                   Sigma
122                                                                   # Boosting Iterations
123                                                                          Max Tree Depth
124                                                                               Shrinkage
125                                                                 Min. Terminal Node Size
126                                                                          Product Degree
139                                                                               parameter
144                                                                               parameter
140                                                                   # Boosting Iterations
141                                                                              AIC Prune?
142                                                                       Mixing Percentage
143                                                                Regularization Parameter
145                                                                             #Components
146                                                                                   Gamma
147                                                                                  Lambda
148                                                Dimension of the Discriminative Subspace
149                                                                               Threshold
150                                                                              Model Type
153                                                                             #Components
156                                                                             #Components
157                                                                         Max. #Neighbors
158                                                                                Distance
159                                                                                  Kernel
160                                                                              #Neighbors
161                                                                Regularization Parameter
162                                                                       Polynomial Degree
163                                                                Regularization Parameter
164                                                                                   Sigma
165                                                                                Fraction
166                                                                                  #Steps
167                                                               Fraction of Full Solution
168                                                                               parameter
169                                                                 #Discriminant Functions
170                                                            Maximum Number of Predictors
171                                                            Maximum Number of Predictors
172                                                            Maximum Number of Predictors
174                                                                               parameter
175                                                                               parameter
177                                                                      #Nearest Neighbors
178                                                                Maximum Number of Leaves
179                                                                         Number of Trees
181                                                                Maximum Number of Leaves
182                                                                         Number of Trees
183                                                                               Parameter
184                                                                       Polynomial Degree
185                                                                                   Scale
186                                                                                   Sigma
187                                                                           Codebook Size
188                                                                             #Prototypes
194                                                                   #Subclasses Per Class
196                                                                           #Hidden Units
197                                                                    #Hidden Units layer1
198                                                                    #Hidden Units layer2
199                                                                    #Hidden Units layer3
200                                                                           #Hidden Units
201                                                                            Weight Decay
202                                                                    #Hidden Units layer1
203                                                                    #Hidden Units layer2
204                                                                    #Hidden Units layer3
205                                                                            Weight Decay
206                                                                            Weight Decay
207                                                                      Laplace Correction
208                                                                       Distribution Type
209                                                                     Smoothing Parameter
210                                                                                  #Folds
211                                                            Minimum Absolute Improvement
212                                                                     Smoothing Parameter
213                                                               Final Smoothing Parameter
214                                                                        Search Direction
215                                                                #Hidden Units in Layer 1
216                                                                #Hidden Units in Layer 2
217                                                                #Hidden Units in Layer 3
218                                                                           #Hidden Units
219                                                                            Weight Decay
220                                                                               parameter
221                                                               Maximum Interaction Depth
222                                                                         Prediction Mode
223                                                                          Oblique Splits
224                                                               Variable Selection Method
230                                                                              #Neighbors
231                                                                     Shrinkage Threshold
232                                                           #Randomly Selected Predictors
235                                                           Number of Terminal Partitions
236                                                              Minimum Percent Difference
237                                                                           #Hidden Units
238                                                                            Weight Decay
239                                                                             #Components
240                                                           Shrinkage Penalty Coefficient
241                                                                      Degrees of Freedom
242                                                                              L1 Penalty
243                                                                              L2 Penalty
246                                                                              L2 Penalty
247                                                                    Complexity Parameter
248                                                                             #Components
249                                                                         #PLS Components
250                                                                       p-Value threshold
251                                                                               parameter
252                                                                                 # Terms
253                                                                               Ball Size
254                                                                          Distance Order
255                                                                              #Neighbors
256                                                                         Weight Function
257                                                                               Algorithm
258                                                                               Leaf Size
259                                                                         Distance Metric
260                                                                                       p
261                                                                               parameter
263                                                           #Randomly Selected Predictors
264                                                                           #Hidden Units
265                                                                            Weight Decay
266                                                                          Bagged Models?
276                                                                              Fern Depth
267                                                                       Interaction Order
268                                                           #Randomly Selected Predictors
269                                                                           #Hidden Units
270                                                Activation Limit for Conflicting Classes
271                                                                                   Gamma
272                                                                                  Lambda
273                                                                       Penalty Parameter
274                                                                    Relaxation Parameter
275                                                           #Randomly Selected Predictors
278                                                           #Randomly Selected Predictors
279                                                                      Maximum Rule Depth
280                                                                            Weight Decay
281                                                                              #Neighbors
282                                                           #Randomly Selected Predictors
283                                                                              #Neighbors
284                                                           #Randomly Selected Predictors
285                                                                       #Features Dropped
286                                                                               parameter
287                                                                   #Subclasses Per Class
288                                                                                   Model
289                                                                     #Variables Retained
290                                                                       #Variable Subsets
291                                                                           Ensemble Size
292                                                                       #Variable Subsets
293                                                                           Ensemble Size
294                                                                    Complexity Parameter
295                                                                    Complexity Parameter
296                                                                          Max Tree Depth
297                                                                    Complexity Parameter
298                                                                                    Cost
299                                                                              L1 Penalty
300                                                                              L1 Penalty
301                                                                            Penalty Type
307                                                                       Penalty Parameter
308                                                                    Robustness Parameter
309                                                                            Penalty Type
311                                                                               parameter
312                                                                                   Scale
313                                                                       Polynomial Degree
314                                                                                   Sigma
318                                                                             Diagonalize
319                                                                               shrinkage
320                                                                               parameter
321                                                                               parameter
322                                                                              L1 Penalty
323                                                                              L2 Penalty
324                                                                             #Components
328                                                                                    none
329                                                                            # Predictors
330                                                                                  Lambda
331                                                                            # Subclasses
332                                                                 Stabilization Parameter
333                                                                            # Predictors
334                                                                                  Lambda
335                                                                      Variables Retained
336                                                                             #Components
337                                                                               Threshold
338                                                                                   Kappa
339                                                                      Maximum #Variables
340                                                                        Search Direction
341                                                                      Maximum #Variables
342                                                                        Search Direction
343                                                                               Threshold
344                                                                             #Components
345                                                                                  length
346                                                                                    Cost
347                                                                                  lambda
348                                                                                    Cost
349                                                                                    Cost
350                                                                                    Cost
351                                                                                   Gamma
352                                                                       Polynomial Degree
353                                                                                   Scale
354                                                                                    Cost
355                                                                                   Sigma
356                                                                                    Cost
357                                                                                    Cost
358                                                                                   Sigma
359                                                                                    Cost
360                                                                                  Weight
361                                                                                  length
362                                                                                    Cost
363                                                                          Score Function
364                                                                     Smoothing Parameter
365                                                                                  #Folds
366                                                            Minimum Absolute Improvement
367                                                                     Smoothing Parameter
368                                                               Final Smoothing Parameter
369                                                                            Super-Parent
370                                                                               parameter
371                                                                         Theta Estimated
372                                                                             #Components
375                                                           #Randomly Selected Predictors
376                                                                   # Boosting Iterations
377                                                                       L2 Regularization
378                                                                       L2 Regularization
379                                                                   # Boosting Iterations
380                                                                          Max Tree Depth
381                                                                               Shrinkage
382                                                                  Minimum Loss Reduction
383                                                              Subsample Ratio of Columns
384                                                          Minimum Sum of Instance Weight
385                                                                                     Row
386                                                                                 Columns
387                                                                                X Weight
388                                                                                Topology
    forReg forClass probModel
10    TRUE    FALSE     FALSE
11    TRUE    FALSE     FALSE
4    FALSE     TRUE      TRUE
5    FALSE     TRUE      TRUE
6    FALSE     TRUE      TRUE
7    FALSE     TRUE      TRUE
8    FALSE     TRUE      TRUE
40    TRUE     TRUE      TRUE
43    TRUE     TRUE     FALSE
44    TRUE     TRUE     FALSE
50   FALSE     TRUE      TRUE
51   FALSE     TRUE      TRUE
52   FALSE     TRUE      TRUE
53   FALSE     TRUE     FALSE
54   FALSE     TRUE     FALSE
55   FALSE     TRUE     FALSE
56   FALSE     TRUE     FALSE
57   FALSE     TRUE      TRUE
58   FALSE     TRUE      TRUE
63   FALSE     TRUE     FALSE
68    TRUE    FALSE     FALSE
69    TRUE    FALSE     FALSE
98   FALSE     TRUE     FALSE
99   FALSE     TRUE     FALSE
100  FALSE     TRUE     FALSE
101   TRUE    FALSE     FALSE
102   TRUE    FALSE     FALSE
105  FALSE     TRUE     FALSE
106  FALSE     TRUE     FALSE
107  FALSE     TRUE     FALSE
108  FALSE     TRUE     FALSE
109   TRUE    FALSE     FALSE
110   TRUE    FALSE     FALSE
127   TRUE    FALSE     FALSE
128   TRUE    FALSE     FALSE
129   TRUE    FALSE     FALSE
130  FALSE     TRUE     FALSE
131  FALSE     TRUE     FALSE
132  FALSE     TRUE     FALSE
133   TRUE    FALSE     FALSE
134   TRUE    FALSE     FALSE
135   TRUE    FALSE     FALSE
136   TRUE    FALSE     FALSE
137   TRUE    FALSE     FALSE
138   TRUE    FALSE     FALSE
151   TRUE    FALSE     FALSE
152   TRUE    FALSE     FALSE
154  FALSE     TRUE      TRUE
155  FALSE     TRUE      TRUE
176  FALSE     TRUE      TRUE
173  FALSE     TRUE      TRUE
180  FALSE     TRUE      TRUE
189   TRUE    FALSE     FALSE
190   TRUE    FALSE     FALSE
191   TRUE    FALSE     FALSE
192   TRUE    FALSE     FALSE
193   TRUE    FALSE     FALSE
195  FALSE     TRUE     FALSE
226  FALSE     TRUE      TRUE
227  FALSE     TRUE      TRUE
228  FALSE     TRUE      TRUE
229  FALSE     TRUE      TRUE
225  FALSE     TRUE      TRUE
233  FALSE     TRUE      TRUE
234  FALSE     TRUE      TRUE
244  FALSE     TRUE     FALSE
245  FALSE     TRUE     FALSE
262  FALSE     TRUE      TRUE
277  FALSE     TRUE     FALSE
302   TRUE     TRUE      TRUE
303   TRUE     TRUE      TRUE
304   TRUE     TRUE      TRUE
305   TRUE     TRUE      TRUE
306   TRUE     TRUE      TRUE
310  FALSE     TRUE     FALSE
315   TRUE    FALSE     FALSE
316   TRUE    FALSE     FALSE
317   TRUE    FALSE     FALSE
325  FALSE     TRUE     FALSE
326  FALSE     TRUE     FALSE
327  FALSE     TRUE     FALSE
373   TRUE    FALSE     FALSE
374   TRUE    FALSE     FALSE
1    FALSE     TRUE      TRUE
2    FALSE     TRUE      TRUE
3    FALSE     TRUE      TRUE
9    FALSE     TRUE      TRUE
12    TRUE     TRUE      TRUE
13    TRUE     TRUE      TRUE
14    TRUE     TRUE      TRUE
15   FALSE     TRUE      TRUE
16   FALSE     TRUE      TRUE
17   FALSE     TRUE      TRUE
18    TRUE     TRUE      TRUE
19    TRUE     TRUE      TRUE
20    TRUE     TRUE      TRUE
21    TRUE     TRUE      TRUE
22   FALSE     TRUE      TRUE
23   FALSE     TRUE      TRUE
24   FALSE     TRUE      TRUE
25    TRUE     TRUE      TRUE
26    TRUE     TRUE      TRUE
27    TRUE     TRUE      TRUE
28    TRUE     TRUE      TRUE
29    TRUE     TRUE      TRUE
30    TRUE     TRUE      TRUE
31    TRUE     TRUE      TRUE
32    TRUE     TRUE      TRUE
33    TRUE     TRUE      TRUE
34    TRUE     TRUE      TRUE
35   FALSE     TRUE      TRUE
36    TRUE     TRUE      TRUE
37    TRUE     TRUE      TRUE
38    TRUE    FALSE     FALSE
39    TRUE    FALSE     FALSE
41    TRUE    FALSE     FALSE
42    TRUE    FALSE     FALSE
45    TRUE     TRUE     FALSE
46    TRUE     TRUE     FALSE
47    TRUE     TRUE     FALSE
48    TRUE     TRUE     FALSE
49    TRUE     TRUE     FALSE
59    TRUE     TRUE      TRUE
60   FALSE     TRUE      TRUE
61   FALSE     TRUE      TRUE
62   FALSE     TRUE      TRUE
64    TRUE     TRUE      TRUE
65    TRUE     TRUE      TRUE
66    TRUE    FALSE     FALSE
67    TRUE    FALSE     FALSE
70    TRUE     TRUE      TRUE
71    TRUE     TRUE      TRUE
72    TRUE     TRUE      TRUE
73    TRUE     TRUE      TRUE
74    TRUE     TRUE      TRUE
75   FALSE     TRUE      TRUE
76   FALSE     TRUE      TRUE
77   FALSE     TRUE      TRUE
78   FALSE     TRUE      TRUE
79   FALSE     TRUE      TRUE
80   FALSE     TRUE      TRUE
81   FALSE     TRUE      TRUE
82   FALSE     TRUE      TRUE
83   FALSE     TRUE      TRUE
84    TRUE     TRUE      TRUE
85    TRUE     TRUE      TRUE
86    TRUE     TRUE     FALSE
87    TRUE     TRUE     FALSE
88    TRUE    FALSE     FALSE
89    TRUE    FALSE     FALSE
92    TRUE    FALSE     FALSE
90    TRUE    FALSE     FALSE
91    TRUE    FALSE     FALSE
93    TRUE     TRUE      TRUE
94    TRUE     TRUE      TRUE
95    TRUE     TRUE      TRUE
96   FALSE     TRUE      TRUE
97   FALSE     TRUE      TRUE
103   TRUE    FALSE     FALSE
104   TRUE    FALSE     FALSE
111   TRUE     TRUE      TRUE
112   TRUE     TRUE      TRUE
115   TRUE     TRUE      TRUE
116   TRUE     TRUE      TRUE
117   TRUE     TRUE      TRUE
113   TRUE     TRUE      TRUE
114   TRUE     TRUE      TRUE
118   TRUE     TRUE      TRUE
119   TRUE     TRUE      TRUE
120   TRUE     TRUE      TRUE
121   TRUE     TRUE      TRUE
122   TRUE     TRUE      TRUE
123   TRUE     TRUE      TRUE
124   TRUE     TRUE      TRUE
125   TRUE     TRUE      TRUE
126   TRUE     TRUE      TRUE
139   TRUE     TRUE      TRUE
144   TRUE     TRUE      TRUE
140   TRUE     TRUE      TRUE
141   TRUE     TRUE      TRUE
142   TRUE     TRUE      TRUE
143   TRUE     TRUE      TRUE
145  FALSE     TRUE      TRUE
146  FALSE     TRUE      TRUE
147  FALSE     TRUE      TRUE
148  FALSE     TRUE      TRUE
149  FALSE     TRUE      TRUE
150  FALSE     TRUE      TRUE
153   TRUE    FALSE     FALSE
156   TRUE     TRUE      TRUE
157   TRUE     TRUE     FALSE
158   TRUE     TRUE     FALSE
159   TRUE     TRUE     FALSE
160   TRUE     TRUE      TRUE
161   TRUE    FALSE     FALSE
162   TRUE    FALSE     FALSE
163   TRUE    FALSE     FALSE
164   TRUE    FALSE     FALSE
165   TRUE    FALSE     FALSE
166   TRUE    FALSE     FALSE
167   TRUE    FALSE     FALSE
168  FALSE     TRUE      TRUE
169  FALSE     TRUE      TRUE
170   TRUE    FALSE     FALSE
171   TRUE    FALSE     FALSE
172   TRUE    FALSE     FALSE
174   TRUE    FALSE     FALSE
175   TRUE    FALSE     FALSE
177  FALSE     TRUE      TRUE
178   TRUE     TRUE      TRUE
179   TRUE     TRUE      TRUE
181   TRUE     TRUE      TRUE
182   TRUE     TRUE      TRUE
183  FALSE     TRUE     FALSE
184  FALSE     TRUE     FALSE
185  FALSE     TRUE     FALSE
186  FALSE     TRUE     FALSE
187  FALSE     TRUE     FALSE
188  FALSE     TRUE     FALSE
194  FALSE     TRUE      TRUE
196   TRUE     TRUE      TRUE
197   TRUE     TRUE      TRUE
198   TRUE     TRUE      TRUE
199   TRUE     TRUE      TRUE
200   TRUE     TRUE      TRUE
201   TRUE     TRUE      TRUE
202   TRUE     TRUE      TRUE
203   TRUE     TRUE      TRUE
204   TRUE     TRUE      TRUE
205   TRUE     TRUE      TRUE
206  FALSE     TRUE      TRUE
207  FALSE     TRUE      TRUE
208  FALSE     TRUE      TRUE
209  FALSE     TRUE      TRUE
210  FALSE     TRUE      TRUE
211  FALSE     TRUE      TRUE
212  FALSE     TRUE      TRUE
213  FALSE     TRUE      TRUE
214  FALSE     TRUE      TRUE
215   TRUE    FALSE     FALSE
216   TRUE    FALSE     FALSE
217   TRUE    FALSE     FALSE
218   TRUE     TRUE      TRUE
219   TRUE     TRUE      TRUE
220   TRUE    FALSE     FALSE
221   TRUE     TRUE      TRUE
222   TRUE     TRUE      TRUE
223  FALSE     TRUE      TRUE
224  FALSE     TRUE      TRUE
230  FALSE     TRUE     FALSE
231  FALSE     TRUE      TRUE
232   TRUE     TRUE      TRUE
235   TRUE     TRUE     FALSE
236   TRUE     TRUE     FALSE
237   TRUE     TRUE      TRUE
238   TRUE     TRUE      TRUE
239   TRUE    FALSE     FALSE
240  FALSE     TRUE      TRUE
241  FALSE     TRUE      TRUE
242   TRUE    FALSE     FALSE
243   TRUE    FALSE     FALSE
246  FALSE     TRUE      TRUE
247  FALSE     TRUE      TRUE
248   TRUE     TRUE      TRUE
249   TRUE     TRUE      TRUE
250   TRUE     TRUE      TRUE
251  FALSE     TRUE      TRUE
252   TRUE    FALSE     FALSE
253  FALSE     TRUE     FALSE
254  FALSE     TRUE     FALSE
255   TRUE    FALSE     FALSE
256   TRUE    FALSE     FALSE
257   TRUE    FALSE     FALSE
258   TRUE    FALSE     FALSE
259   TRUE    FALSE     FALSE
260   TRUE    FALSE     FALSE
261  FALSE     TRUE      TRUE
263   TRUE    FALSE     FALSE
264   TRUE    FALSE     FALSE
265   TRUE    FALSE     FALSE
266   TRUE    FALSE     FALSE
276  FALSE     TRUE     FALSE
267   TRUE     TRUE      TRUE
268   TRUE     TRUE      TRUE
269   TRUE     TRUE      TRUE
270   TRUE     TRUE      TRUE
271  FALSE     TRUE      TRUE
272  FALSE     TRUE      TRUE
273   TRUE    FALSE     FALSE
274   TRUE    FALSE     FALSE
275   TRUE     TRUE      TRUE
278   TRUE     TRUE     FALSE
279   TRUE     TRUE     FALSE
280   TRUE    FALSE     FALSE
281   TRUE     TRUE     FALSE
282   TRUE     TRUE     FALSE
283   TRUE     TRUE     FALSE
284   TRUE     TRUE     FALSE
285   TRUE     TRUE     FALSE
286   TRUE    FALSE     FALSE
287  FALSE     TRUE      TRUE
288  FALSE     TRUE      TRUE
289  FALSE     TRUE     FALSE
290  FALSE     TRUE      TRUE
291  FALSE     TRUE      TRUE
292  FALSE     TRUE      TRUE
293  FALSE     TRUE      TRUE
294  FALSE     TRUE      TRUE
295   TRUE     TRUE      TRUE
296   TRUE     TRUE      TRUE
297  FALSE     TRUE     FALSE
298  FALSE     TRUE     FALSE
299   TRUE    FALSE     FALSE
300   TRUE    FALSE     FALSE
301   TRUE    FALSE     FALSE
307  FALSE     TRUE      TRUE
308  FALSE     TRUE      TRUE
309  FALSE     TRUE      TRUE
311   TRUE    FALSE     FALSE
312   TRUE    FALSE     FALSE
313   TRUE    FALSE     FALSE
314   TRUE    FALSE     FALSE
318  FALSE     TRUE      TRUE
319  FALSE     TRUE      TRUE
320  FALSE     TRUE      TRUE
321  FALSE     TRUE      TRUE
322  FALSE     TRUE      TRUE
323  FALSE     TRUE      TRUE
324   TRUE     TRUE      TRUE
328  FALSE     TRUE      TRUE
329  FALSE     TRUE     FALSE
330  FALSE     TRUE     FALSE
331  FALSE     TRUE     FALSE
332  FALSE     TRUE     FALSE
333  FALSE     TRUE      TRUE
334  FALSE     TRUE      TRUE
335   TRUE    FALSE     FALSE
336   TRUE     TRUE      TRUE
337   TRUE     TRUE      TRUE
338   TRUE     TRUE      TRUE
339  FALSE     TRUE      TRUE
340  FALSE     TRUE      TRUE
341  FALSE     TRUE      TRUE
342  FALSE     TRUE      TRUE
343   TRUE    FALSE     FALSE
344   TRUE    FALSE     FALSE
345   TRUE     TRUE      TRUE
346   TRUE     TRUE      TRUE
347   TRUE     TRUE      TRUE
348   TRUE     TRUE      TRUE
349   TRUE     TRUE      TRUE
350   TRUE     TRUE      TRUE
351   TRUE     TRUE      TRUE
352   TRUE     TRUE      TRUE
353   TRUE     TRUE      TRUE
354   TRUE     TRUE      TRUE
355   TRUE     TRUE      TRUE
356   TRUE     TRUE      TRUE
357   TRUE     TRUE      TRUE
358  FALSE     TRUE      TRUE
359  FALSE     TRUE      TRUE
360  FALSE     TRUE      TRUE
361   TRUE     TRUE      TRUE
362   TRUE     TRUE      TRUE
363  FALSE     TRUE      TRUE
364  FALSE     TRUE      TRUE
365  FALSE     TRUE      TRUE
366  FALSE     TRUE      TRUE
367  FALSE     TRUE      TRUE
368  FALSE     TRUE      TRUE
369  FALSE     TRUE      TRUE
370   TRUE     TRUE      TRUE
371  FALSE     TRUE      TRUE
372   TRUE     TRUE      TRUE
375  FALSE     TRUE      TRUE
376   TRUE     TRUE      TRUE
377   TRUE     TRUE      TRUE
378   TRUE     TRUE      TRUE
379   TRUE     TRUE      TRUE
380   TRUE     TRUE      TRUE
381   TRUE     TRUE      TRUE
382   TRUE     TRUE      TRUE
383   TRUE     TRUE      TRUE
384   TRUE     TRUE      TRUE
385   TRUE     TRUE      TRUE
386   TRUE     TRUE      TRUE
387   TRUE     TRUE      TRUE
388   TRUE     TRUE      TRUE
> modelLookup("gbm")
  model         parameter                   label forReg forClass probModel
1   gbm           n.trees   # Boosting Iterations   TRUE     TRUE      TRUE
2   gbm interaction.depth          Max Tree Depth   TRUE     TRUE      TRUE
3   gbm         shrinkage               Shrinkage   TRUE     TRUE      TRUE
4   gbm    n.minobsinnode Min. Terminal Node Size   TRUE     TRUE      TRUE
> 
> getModelInfo("pls")
$enpls.fs
$enpls.fs$label
[1] "Ensemble Partial Least Squares Regression with Feature Selection"

$enpls.fs$library
[1] "enpls"

$enpls.fs$type
[1] "Regression"

$enpls.fs$parameters
  parameter   class             label
1   maxcomp numeric  Max. #Components
2 threshold numeric Importance Cutoff

$enpls.fs$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(maxcomp = ncol(x), threshold = 2)
    }
    else {
        out <- data.frame(maxcomp = sample(1:ncol(x), size = len, 
            replace = TRUE), threshold = runif(len, min = 0, 
            max = 5))
    }
    out
}

$enpls.fs$loop
NULL

$enpls.fs$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    x <- if (is.matrix(x)) 
        x
    else as.matrix(x)
    vi <- enpls.fs(x = x, y = y, maxcomp = param$maxcomp, ...)[[1]]
    if (any(vi > param$threshold)) {
        keepers <- names(vi)[vi > param$threshold]
    }
    else keepers <- names(vi)[which.max(vi)]
    enpls.en(x = x[, keepers, drop = FALSE], y = y, maxcomp = min(param$maxcomp, 
        length(keepers)), ...)
}

$enpls.fs$predict
function (modelFit, newdata, submodels = NULL) 
{
    print(enpls.en)
    newdata <- if (is.matrix(newdata)) 
        newdata
    else as.matrix(newdata)
    keepers <- rownames(modelFit[[1]][[1]]$loadings)
    predict(modelFit, newdata[, keepers, drop = FALSE])
}

$enpls.fs$predictors
function (x, ...) 
rownames(x$projection)

$enpls.fs$tags
[1] "Partial Least Squares" "Ensemble Model"       

$enpls.fs$prob
NULL

$enpls.fs$sort
function (x) 
x[order(x[, 1]), ]


$enpls
$enpls$label
[1] "Ensemble Partial Least Squares Regression"

$enpls$library
[1] "enpls"

$enpls$type
[1] "Regression"

$enpls$parameters
  parameter   class            label
1   maxcomp numeric Max. #Components

$enpls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(maxcomp = ncol(x))
    }
    else {
        out <- data.frame(maxcomp = sample(1:ncol(x), size = len, 
            replace = TRUE))
    }
    out
}

$enpls$loop
NULL

$enpls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    x <- if (is.matrix(x)) 
        x
    else as.matrix(x)
    enpls.en(x = x, y = y, maxcomp = param$maxcomp, ...)
}

$enpls$predict
function (modelFit, newdata, submodels = NULL) 
{
    newdata <- if (is.matrix(newdata)) 
        newdata
    else as.matrix(newdata)
    predict(modelFit, newdata)
}

$enpls$predictors
function (x, ...) 
rownames(x$projection)

$enpls$tags
[1] "Partial Least Squares" "Ensemble Model"       

$enpls$prob
NULL

$enpls$sort
function (x) 
x[order(x[, 1]), ]


$gpls
$gpls$label
[1] "Generalized Partial Least Squares"

$gpls$library
[1] "gpls"

$gpls$loop
NULL

$gpls$type
[1] "Classification"

$gpls$parameters
  parameter   class       label
1    K.prov numeric #Components

$gpls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(K.prov = seq(1, len))
    }
    else {
        out <- data.frame(K.prov = unique(sample(1:ncol(x), size = len, 
            replace = TRUE)))
    }
    out
}

$gpls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
gpls(x, y, K.prov = param$K.prov, ...)

$gpls$predict
function (modelFit, newdata, submodels = NULL) 
predict(modelFit, newdata)$class

$gpls$prob
function (modelFit, newdata, submodels = NULL) 
{
    out <- predict(modelFit, newdata)$predicted
    out <- cbind(out, 1 - out)
    colnames(out) <- modelFit$obsLevels
    out
}

$gpls$predictors
function (x, ...) 
{
    out <- if (hasTerms(x)) 
        predictors(x$terms)
    else colnames(x$data$x.order)
    out[!(out %in% "Intercept")]
}

$gpls$tags
[1] "Logistic Regression"   "Partial Least Squares" "Linear Classifier"    

$gpls$sort
function (x) 
x[order(x[, 1]), ]

$gpls$levels
function (x) 
x$obsLevels


$kernelpls
$kernelpls$label
[1] "Partial Least Squares"

$kernelpls$library
[1] "pls"

$kernelpls$type
[1] "Regression"     "Classification"

$kernelpls$parameters
  parameter   class       label
1     ncomp numeric #Components

$kernelpls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(ncomp = seq(1, min(ncol(x) - 1, len), 
            by = 1))
    }
    else {
        out <- data.frame(ncomp = unique(sample(1:ncol(x), size = len, 
            replace = TRUE)))
    }
    out
}

$kernelpls$loop
function (grid) 
{
    grid <- grid[order(grid$ncomp, decreasing = TRUE), , drop = FALSE]
    loop <- grid[1, , drop = FALSE]
    submodels <- list(grid[-1, , drop = FALSE])
    list(loop = loop, submodels = submodels)
}

$kernelpls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    out <- if (is.factor(y)) {
        plsda(x, y, method = "oscorespls", ncomp = param$ncomp, 
            ...)
    }
    else {
        dat <- if (is.data.frame(x)) 
            x
        else as.data.frame(x)
        dat$.outcome <- y
        plsr(.outcome ~ ., data = dat, method = "kernelpls", 
            ncomp = param$ncomp, ...)
    }
    out
}

$kernelpls$predict
function (modelFit, newdata, submodels = NULL) 
{
    out <- if (modelFit$problemType == "Classification") {
        if (!is.matrix(newdata)) 
            newdata <- as.matrix(newdata)
        out <- predict(modelFit, newdata, type = "class")
    }
    else as.vector(pls:::predict.mvr(modelFit, newdata, ncomp = max(modelFit$ncomp)))
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels))
        if (modelFit$problemType == "Classification") {
            if (length(submodels$ncomp) > 1) {
                tmp <- as.list(predict(modelFit, newdata, ncomp = submodels$ncomp))
            }
            else tmp <- list(predict(modelFit, newdata, ncomp = submodels$ncomp))
        }
        else {
            tmp <- as.list(as.data.frame(apply(predict(modelFit, 
                newdata, ncomp = submodels$ncomp), 3, function(x) list(x))))
        }
        out <- c(list(out), tmp)
    }
    out
}

$kernelpls$prob
function (modelFit, newdata, submodels = NULL) 
{
    if (!is.matrix(newdata)) 
        newdata <- as.matrix(newdata)
    out <- predict(modelFit, newdata, type = "prob", ncomp = modelFit$tuneValue$ncomp)
    if (length(dim(out)) == 3) {
        if (dim(out)[1] > 1) {
            out <- out[, , 1]
        }
        else {
            out <- as.data.frame(t(out[, , 1]))
        }
    }
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels) + 
            1)
        tmp[[1]] <- out
        for (j in seq(along = submodels$ncomp)) {
            tmpProb <- predict(modelFit, newdata, type = "prob", 
                ncomp = submodels$ncomp[j])
            if (length(dim(tmpProb)) == 3) {
                if (dim(tmpProb)[1] > 1) {
                  tmpProb <- tmpProb[, , 1]
                }
                else {
                  tmpProb <- as.data.frame(t(tmpProb[, , 1]))
                }
            }
            tmp[[j + 1]] <- as.data.frame(tmpProb[, modelFit$obsLevels, 
                drop = FALSE])
        }
        out <- tmp
    }
    out
}

$kernelpls$varImp
function (object, estimate = NULL, ...) 
{
    modelCoef <- coef(object, intercept = FALSE, comps = 1:object$ncomp)
    perf <- MSEP(object)$val
    nms <- dimnames(perf)
    if (length(nms$estimate) > 1) {
        pIndex <- if (is.null(estimate)) 
            1
        else which(nms$estimate == estimate)
        perf <- perf[pIndex, , , drop = FALSE]
    }
    numResp <- dim(modelCoef)[2]
    if (numResp <= 2) {
        modelCoef <- modelCoef[, 1, , drop = FALSE]
        perf <- perf[, 1, ]
        delta <- -diff(perf)
        delta <- delta/sum(delta)
        out <- data.frame(Overall = apply(abs(modelCoef), 1, 
            weighted.mean, w = delta))
    }
    else {
        perf <- -t(apply(perf[1, , ], 1, diff))
        perf <- t(apply(perf, 1, function(u) u/sum(u)))
        out <- matrix(NA, ncol = numResp, nrow = dim(modelCoef)[1])
        for (i in 1:numResp) {
            tmp <- abs(modelCoef[, i, , drop = FALSE])
            out[, i] <- apply(tmp, 1, weighted.mean, w = perf[i, 
                ])
        }
        colnames(out) <- dimnames(modelCoef)[[2]]
        rownames(out) <- dimnames(modelCoef)[[1]]
    }
    as.data.frame(out)
}

$kernelpls$predictors
function (x, ...) 
rownames(x$projection)

$kernelpls$levels
function (x) 
x$obsLevels

$kernelpls$tags
[1] "Partial Least Squares" "Feature Extraction"    "Kernel Method"        
[4] "Linear Classifier"     "Linear Regression"    

$kernelpls$sort
function (x) 
x[order(x[, 1]), ]


$ORFpls
$ORFpls$label
[1] "Oblique Random Forest"

$ORFpls$library
[1] "obliqueRF"

$ORFpls$loop
NULL

$ORFpls$type
[1] "Classification"

$ORFpls$parameters
  parameter   class                         label
1      mtry numeric #Randomly Selected Predictors

$ORFpls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(mtry = caret::var_seq(p = ncol(x), 
            classification = is.factor(y), len = len))
    }
    else {
        out <- data.frame(mtry = unique(sample(1:ncol(x), size = len, 
            replace = TRUE)))
    }
    out
}

$ORFpls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
obliqueRF(as.matrix(x), y, training_method = "pls", ...)

$ORFpls$predict
function (modelFit, newdata, submodels = NULL) 
predict(modelFit, newdata)

$ORFpls$prob
function (modelFit, newdata, submodels = NULL) 
predict(modelFit, newdata, type = "prob")

$ORFpls$levels
function (x) 
x$obsLevels

$ORFpls$tags
[1] "Random Forest"              "Oblique Tree"              
[3] "Partial Least Squares"      "Implicit Feature Selection"
[5] "Ensemble Model"            

$ORFpls$sort
function (x) 
x[order(x[, 1]), ]


$pls
$pls$label
[1] "Partial Least Squares"

$pls$library
[1] "pls"

$pls$type
[1] "Regression"     "Classification"

$pls$parameters
  parameter   class       label
1     ncomp numeric #Components

$pls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(ncomp = seq(1, min(ncol(x) - 1, len), 
            by = 1))
    }
    else {
        out <- data.frame(ncomp = unique(sample(1:ncol(x), replace = TRUE)))
    }
    out
}

$pls$loop
function (grid) 
{
    grid <- grid[order(grid$ncomp, decreasing = TRUE), , drop = FALSE]
    loop <- grid[1, , drop = FALSE]
    submodels <- list(grid[-1, , drop = FALSE])
    list(loop = loop, submodels = submodels)
}

$pls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    out <- if (is.factor(y)) {
        plsda(x, y, method = "oscorespls", ncomp = param$ncomp, 
            ...)
    }
    else {
        dat <- if (is.data.frame(x)) 
            x
        else as.data.frame(x)
        dat$.outcome <- y
        plsr(.outcome ~ ., data = dat, method = "oscorespls", 
            ncomp = param$ncomp, ...)
    }
    out
}

$pls$predict
function (modelFit, newdata, submodels = NULL) 
{
    out <- if (modelFit$problemType == "Classification") {
        if (!is.matrix(newdata)) 
            newdata <- as.matrix(newdata)
        out <- predict(modelFit, newdata, type = "class")
    }
    else as.vector(pls:::predict.mvr(modelFit, newdata, ncomp = max(modelFit$ncomp)))
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels))
        if (modelFit$problemType == "Classification") {
            if (length(submodels$ncomp) > 1) {
                tmp <- as.list(predict(modelFit, newdata, ncomp = submodels$ncomp))
            }
            else tmp <- list(predict(modelFit, newdata, ncomp = submodels$ncomp))
        }
        else {
            tmp <- as.list(as.data.frame(apply(predict(modelFit, 
                newdata, ncomp = submodels$ncomp), 3, function(x) list(x))))
        }
        out <- c(list(out), tmp)
    }
    out
}

$pls$prob
function (modelFit, newdata, submodels = NULL) 
{
    if (!is.matrix(newdata)) 
        newdata <- as.matrix(newdata)
    out <- predict(modelFit, newdata, type = "prob", ncomp = modelFit$tuneValue$ncomp)
    if (length(dim(out)) == 3) {
        if (dim(out)[1] > 1) {
            out <- out[, , 1]
        }
        else {
            out <- as.data.frame(t(out[, , 1]))
        }
    }
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels) + 
            1)
        tmp[[1]] <- out
        for (j in seq(along = submodels$ncomp)) {
            tmpProb <- predict(modelFit, newdata, type = "prob", 
                ncomp = submodels$ncomp[j])
            if (length(dim(tmpProb)) == 3) {
                if (dim(tmpProb)[1] > 1) {
                  tmpProb <- tmpProb[, , 1]
                }
                else {
                  tmpProb <- as.data.frame(t(tmpProb[, , 1]))
                }
            }
            tmp[[j + 1]] <- as.data.frame(tmpProb[, modelFit$obsLevels])
        }
        out <- tmp
    }
    out
}

$pls$varImp
function (object, estimate = NULL, ...) 
{
    modelCoef <- coef(object, intercept = FALSE, comps = 1:object$ncomp)
    perf <- MSEP(object)$val
    nms <- dimnames(perf)
    if (length(nms$estimate) > 1) {
        pIndex <- if (is.null(estimate)) 
            1
        else which(nms$estimate == estimate)
        perf <- perf[pIndex, , , drop = FALSE]
    }
    numResp <- dim(modelCoef)[2]
    if (numResp <= 2) {
        modelCoef <- modelCoef[, 1, , drop = FALSE]
        perf <- perf[, 1, ]
        delta <- -diff(perf)
        delta <- delta/sum(delta)
        out <- data.frame(Overall = apply(abs(modelCoef), 1, 
            weighted.mean, w = delta))
    }
    else {
        perf <- -t(apply(perf[1, , ], 1, diff))
        perf <- t(apply(perf, 1, function(u) u/sum(u)))
        out <- matrix(NA, ncol = numResp, nrow = dim(modelCoef)[1])
        for (i in 1:numResp) {
            tmp <- abs(modelCoef[, i, , drop = FALSE])
            out[, i] <- apply(tmp, 1, weighted.mean, w = perf[i, 
                ])
        }
        colnames(out) <- dimnames(modelCoef)[[2]]
        rownames(out) <- dimnames(modelCoef)[[1]]
    }
    as.data.frame(out)
}

$pls$predictors
function (x, ...) 
rownames(x$projection)

$pls$levels
function (x) 
x$obsLevels

$pls$tags
[1] "Partial Least Squares" "Feature Extraction"    "Linear Classifier"    
[4] "Linear Regression"    

$pls$sort
function (x) 
x[order(x[, 1]), ]


$plsRglm
$plsRglm$label
[1] "Partial Least Squares Generalized Linear Models "

$plsRglm$library
[1] "plsRglm"

$plsRglm$loop
NULL

$plsRglm$type
[1] "Classification" "Regression"    

$plsRglm$parameters
          parameter   class             label
1                nt numeric   #PLS Components
2 alpha.pvals.expli numeric p-Value threshold

$plsRglm$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- expand.grid(nt = 1:len, alpha.pvals.expli = 10^(c(-2:(len - 
            3), 0)))
    }
    else {
        out <- data.frame(nt = sample(1:ncol(x), size = len, 
            replace = TRUE), alpha.pvals.expli = runif(len, min = 0, 
            0.2))
    }
    out
}

$plsRglm$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    if (is.factor(y)) {
        lv <- levels(y)
        y <- as.numeric(y) - 1
        dst <- "pls-glm-logistic"
    }
    else {
        lv <- NULL
        dst <- "pls-glm-gaussian"
    }
    theDots <- list(...)
    if (any(names(theDots) == "modele")) {
        mod <- plsRglm(y, x, nt = param$nt, pvals.expli = param$alpha.pvals.expli < 
            1, sparse = param$alpha.pvals.expli < 1, alpha.pvals.expli = param$alpha.pvals.expli, 
            ...)
    }
    else {
        mod <- plsRglm(y, x, nt = param$nt, modele = dst, pvals.expli = param$alpha.pvals.expli < 
            1, sparse = param$alpha.pvals.expli < 1, alpha.pvals.expli = param$alpha.pvals.expli, 
            ...)
    }
    mod
}

$plsRglm$predict
function (modelFit, newdata, submodels = NULL) 
{
    out <- predict(modelFit, newdata, type = "response")
    if (modelFit$problemType == "Classification") {
        out <- factor(ifelse(out >= 0.5, modelFit$obsLevels[2], 
            modelFit$obsLevels[1]))
    }
    out
}

$plsRglm$prob
function (modelFit, newdata, submodels = NULL) 
{
    out <- predict(modelFit, newdata, type = "response")
    out <- cbind(1 - out, out)
    dimnames(out)[[2]] <- rev(modelFit$obsLevels)
    out
}

$plsRglm$varImp
NULL

$plsRglm$predictors
function (x, ...) 
{
    vars <- names(which(coef(x)[[2]][, 1] != 0))
    vars[vars != "Intercept"]
}

$plsRglm$tags
[1] "Generalized Linear Models" "Partial Least Squares"    

$plsRglm$levels
function (x) 
x$lev

$plsRglm$sort
function (x) 
x[order(-x$alpha.pvals.expli, x$nt), ]


$simpls
$simpls$label
[1] "Partial Least Squares"

$simpls$library
[1] "pls"

$simpls$type
[1] "Regression"     "Classification"

$simpls$parameters
  parameter   class       label
1     ncomp numeric #Components

$simpls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(ncomp = seq(1, min(ncol(x) - 1, len), 
            by = 1))
    }
    else {
        out <- data.frame(ncomp = unique(sample(1:(ncol(x) - 
            1), size = len, replace = TRUE)))
    }
    out
}

$simpls$loop
function (grid) 
{
    grid <- grid[order(grid$ncomp, decreasing = TRUE), , drop = FALSE]
    loop <- grid[1, , drop = FALSE]
    submodels <- list(grid[-1, , drop = FALSE])
    list(loop = loop, submodels = submodels)
}

$simpls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    out <- if (is.factor(y)) {
        plsda(x, y, method = "oscorespls", ncomp = param$ncomp, 
            ...)
    }
    else {
        dat <- if (is.data.frame(x)) 
            x
        else as.data.frame(x)
        dat$.outcome <- y
        plsr(.outcome ~ ., data = dat, method = "simpls", ncomp = param$ncomp, 
            ...)
    }
    out
}

$simpls$predict
function (modelFit, newdata, submodels = NULL) 
{
    out <- if (modelFit$problemType == "Classification") {
        if (!is.matrix(newdata)) 
            newdata <- as.matrix(newdata)
        out <- predict(modelFit, newdata, type = "class")
    }
    else as.vector(pls:::predict.mvr(modelFit, newdata, ncomp = max(modelFit$ncomp)))
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels))
        if (modelFit$problemType == "Classification") {
            if (length(submodels$ncomp) > 1) {
                tmp <- as.list(predict(modelFit, newdata, ncomp = submodels$ncomp))
            }
            else tmp <- list(predict(modelFit, newdata, ncomp = submodels$ncomp))
        }
        else {
            tmp <- as.list(as.data.frame(apply(predict(modelFit, 
                newdata, ncomp = submodels$ncomp), 3, function(x) list(x))))
        }
        out <- c(list(out), tmp)
    }
    out
}

$simpls$prob
function (modelFit, newdata, submodels = NULL) 
{
    if (!is.matrix(newdata)) 
        newdata <- as.matrix(newdata)
    out <- predict(modelFit, newdata, type = "prob", ncomp = modelFit$tuneValue$ncomp)
    if (length(dim(out)) == 3) {
        if (dim(out)[1] > 1) {
            out <- out[, , 1]
        }
        else {
            out <- as.data.frame(t(out[, , 1]))
        }
    }
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels) + 
            1)
        tmp[[1]] <- out
        for (j in seq(along = submodels$ncomp)) {
            tmpProb <- predict(modelFit, newdata, type = "prob", 
                ncomp = submodels$ncomp[j])
            if (length(dim(tmpProb)) == 3) {
                if (dim(tmpProb)[1] > 1) {
                  tmpProb <- tmpProb[, , 1]
                }
                else {
                  tmpProb <- as.data.frame(t(tmpProb[, , 1]))
                }
            }
            tmp[[j + 1]] <- as.data.frame(tmpProb[, modelFit$obsLevels, 
                drop = FALSE])
        }
        out <- tmp
    }
    out
}

$simpls$varImp
function (object, estimate = NULL, ...) 
{
    modelCoef <- coef(object, intercept = FALSE, comps = 1:object$ncomp)
    perf <- MSEP(object)$val
    nms <- dimnames(perf)
    if (length(nms$estimate) > 1) {
        pIndex <- if (is.null(estimate)) 
            1
        else which(nms$estimate == estimate)
        perf <- perf[pIndex, , , drop = FALSE]
    }
    numResp <- dim(modelCoef)[2]
    if (numResp <= 2) {
        modelCoef <- modelCoef[, 1, , drop = FALSE]
        perf <- perf[, 1, ]
        delta <- -diff(perf)
        delta <- delta/sum(delta)
        out <- data.frame(Overall = apply(abs(modelCoef), 1, 
            weighted.mean, w = delta))
    }
    else {
        perf <- -t(apply(perf[1, , ], 1, diff))
        perf <- t(apply(perf, 1, function(u) u/sum(u)))
        out <- matrix(NA, ncol = numResp, nrow = dim(modelCoef)[1])
        for (i in 1:numResp) {
            tmp <- abs(modelCoef[, i, , drop = FALSE])
            out[, i] <- apply(tmp, 1, weighted.mean, w = perf[i, 
                ])
        }
        colnames(out) <- dimnames(modelCoef)[[2]]
        rownames(out) <- dimnames(modelCoef)[[1]]
    }
    as.data.frame(out)
}

$simpls$levels
function (x) 
x$obsLevels

$simpls$predictors
function (x, ...) 
rownames(x$projection)

$simpls$tags
[1] "Partial Least Squares" "Feature Extraction"    "Linear Classifier"    
[4] "Linear Regression"    

$simpls$sort
function (x) 
x[order(x[, 1]), ]


$spls
$spls$label
[1] "Sparse Partial Least Squares"

$spls$library
[1] "spls"

$spls$type
[1] "Regression"     "Classification"

$spls$parameters
  parameter   class       label
1         K numeric #Components
2       eta numeric   Threshold
3     kappa numeric       Kappa

$spls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- expand.grid(K = 1:min(nrow(x), ncol(x)), eta = seq(0.1, 
            0.9, length = len), kappa = 0.5)
    }
    else {
        out <- data.frame(kappa = runif(len, min = 0, max = 0.5), 
            eta = runif(len, min = 0, max = 1), K = sample(1:min(nrow(x), 
                ncol(x)), size = len, replace = TRUE))
    }
    out
}

$spls$loop
NULL

$spls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    if (is.factor(y)) {
        caret:::splsda(x, y, K = param$K, eta = param$eta, kappa = param$kappa, 
            ...)
    }
    else {
        spls(x, y, K = param$K, eta = param$eta, kappa = param$kappa, 
            ...)
    }
}

$spls$predict
function (modelFit, newdata, submodels = NULL) 
{
    if (length(modelFit$obsLevels) < 2) {
        predict(modelFit, newdata)
    }
    else {
        as.character(caret:::predict.splsda(modelFit, newdata, 
            type = "class"))
    }
}

$spls$prob
function (modelFit, newdata, submodels = NULL) 
{
    if (!is.matrix(newdata)) 
        newdata <- as.matrix(newdata)
    caret:::predict.splsda(modelFit, newdata, type = "prob")
}

$spls$predictors
function (x, ...) 
colnames(x$x)[x$A]

$spls$tags
[1] "Partial Least Squares" "Feature Extraction"    "Linear Classifier"    
[4] "Linear Regression"     "L1 Regularization"    

$spls$levels
function (x) 
x$obsLevels

$spls$sort
function (x) 
x[order(-x$eta, x$K), ]


$widekernelpls
$widekernelpls$label
[1] "Partial Least Squares"

$widekernelpls$library
[1] "pls"

$widekernelpls$type
[1] "Regression"     "Classification"

$widekernelpls$parameters
  parameter   class       label
1     ncomp numeric #Components

$widekernelpls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(ncomp = seq(1, min(ncol(x) - 1, len), 
            by = 1))
    }
    else {
        out <- data.frame(ncomp = unique(sample(1:(ncol(x) - 
            1), size = len, replace = TRUE)))
    }
    out
}

$widekernelpls$loop
function (grid) 
{
    grid <- grid[order(grid$ncomp, decreasing = TRUE), , drop = FALSE]
    loop <- grid[1, , drop = FALSE]
    submodels <- list(grid[-1, , drop = FALSE])
    list(loop = loop, submodels = submodels)
}

$widekernelpls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    out <- if (is.factor(y)) {
        plsda(x, y, method = "oscorespls", ncomp = param$ncomp, 
            ...)
    }
    else {
        dat <- if (is.data.frame(x)) 
            x
        else as.data.frame(x)
        dat$.outcome <- y
        plsr(.outcome ~ ., data = dat, method = "widekernelpls", 
            ncomp = param$ncomp, ...)
    }
    out
}

$widekernelpls$predict
function (modelFit, newdata, submodels = NULL) 
{
    out <- if (modelFit$problemType == "Classification") {
        if (!is.matrix(newdata)) 
            newdata <- as.matrix(newdata)
        out <- predict(modelFit, newdata, type = "class")
    }
    else as.vector(pls:::predict.mvr(modelFit, newdata, ncomp = max(modelFit$ncomp)))
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels))
        if (modelFit$problemType == "Classification") {
            if (length(submodels$ncomp) > 1) {
                tmp <- as.list(predict(modelFit, newdata, ncomp = submodels$ncomp))
            }
            else tmp <- list(predict(modelFit, newdata, ncomp = submodels$ncomp))
        }
        else {
            tmp <- as.list(as.data.frame(apply(predict(modelFit, 
                newdata, ncomp = submodels$ncomp), 3, function(x) list(x))))
        }
        out <- c(list(out), tmp)
    }
    out
}

$widekernelpls$prob
function (modelFit, newdata, submodels = NULL) 
{
    if (!is.matrix(newdata)) 
        newdata <- as.matrix(newdata)
    out <- predict(modelFit, newdata, type = "prob", ncomp = modelFit$tuneValue$ncomp)
    if (length(dim(out)) == 3) {
        if (dim(out)[1] > 1) {
            out <- out[, , 1]
        }
        else {
            out <- as.data.frame(t(out[, , 1]))
        }
    }
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels) + 
            1)
        tmp[[1]] <- out
        for (j in seq(along = submodels$ncomp)) {
            tmpProb <- predict(modelFit, newdata, type = "prob", 
                ncomp = submodels$ncomp[j])
            if (length(dim(tmpProb)) == 3) {
                if (dim(tmpProb)[1] > 1) {
                  tmpProb <- tmpProb[, , 1]
                }
                else {
                  tmpProb <- as.data.frame(t(tmpProb[, , 1]))
                }
            }
            tmp[[j + 1]] <- as.data.frame(tmpProb[, modelFit$obsLevels, 
                drop = FALSE])
        }
        out <- tmp
    }
    out
}

$widekernelpls$predictors
function (x, ...) 
rownames(x$projection)

$widekernelpls$varImp
function (object, estimate = NULL, ...) 
{
    modelCoef <- coef(object, intercept = FALSE, comps = 1:object$ncomp)
    perf <- MSEP(object)$val
    nms <- dimnames(perf)
    if (length(nms$estimate) > 1) {
        pIndex <- if (is.null(estimate)) 
            1
        else which(nms$estimate == estimate)
        perf <- perf[pIndex, , , drop = FALSE]
    }
    numResp <- dim(modelCoef)[2]
    if (numResp <= 2) {
        modelCoef <- modelCoef[, 1, , drop = FALSE]
        perf <- perf[, 1, ]
        delta <- -diff(perf)
        delta <- delta/sum(delta)
        out <- data.frame(Overall = apply(abs(modelCoef), 1, 
            weighted.mean, w = delta))
    }
    else {
        perf <- -t(apply(perf[1, , ], 1, diff))
        perf <- t(apply(perf, 1, function(u) u/sum(u)))
        out <- matrix(NA, ncol = numResp, nrow = dim(modelCoef)[1])
        for (i in 1:numResp) {
            tmp <- abs(modelCoef[, i, , drop = FALSE])
            out[, i] <- apply(tmp, 1, weighted.mean, w = perf[i, 
                ])
        }
        colnames(out) <- dimnames(modelCoef)[[2]]
        rownames(out) <- dimnames(modelCoef)[[1]]
    }
    as.data.frame(out)
}

$widekernelpls$levels
function (x) 
x$obsLevels

$widekernelpls$tags
[1] "Partial Least Squares" "Feature Extraction"    "Linear Classifier"    
[4] "Linear Regression"    

$widekernelpls$sort
function (x) 
x[order(x[, 1]), ]


> getModelInfo("^pls")
$pls
$pls$label
[1] "Partial Least Squares"

$pls$library
[1] "pls"

$pls$type
[1] "Regression"     "Classification"

$pls$parameters
  parameter   class       label
1     ncomp numeric #Components

$pls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(ncomp = seq(1, min(ncol(x) - 1, len), 
            by = 1))
    }
    else {
        out <- data.frame(ncomp = unique(sample(1:ncol(x), replace = TRUE)))
    }
    out
}

$pls$loop
function (grid) 
{
    grid <- grid[order(grid$ncomp, decreasing = TRUE), , drop = FALSE]
    loop <- grid[1, , drop = FALSE]
    submodels <- list(grid[-1, , drop = FALSE])
    list(loop = loop, submodels = submodels)
}

$pls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    out <- if (is.factor(y)) {
        plsda(x, y, method = "oscorespls", ncomp = param$ncomp, 
            ...)
    }
    else {
        dat <- if (is.data.frame(x)) 
            x
        else as.data.frame(x)
        dat$.outcome <- y
        plsr(.outcome ~ ., data = dat, method = "oscorespls", 
            ncomp = param$ncomp, ...)
    }
    out
}

$pls$predict
function (modelFit, newdata, submodels = NULL) 
{
    out <- if (modelFit$problemType == "Classification") {
        if (!is.matrix(newdata)) 
            newdata <- as.matrix(newdata)
        out <- predict(modelFit, newdata, type = "class")
    }
    else as.vector(pls:::predict.mvr(modelFit, newdata, ncomp = max(modelFit$ncomp)))
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels))
        if (modelFit$problemType == "Classification") {
            if (length(submodels$ncomp) > 1) {
                tmp <- as.list(predict(modelFit, newdata, ncomp = submodels$ncomp))
            }
            else tmp <- list(predict(modelFit, newdata, ncomp = submodels$ncomp))
        }
        else {
            tmp <- as.list(as.data.frame(apply(predict(modelFit, 
                newdata, ncomp = submodels$ncomp), 3, function(x) list(x))))
        }
        out <- c(list(out), tmp)
    }
    out
}

$pls$prob
function (modelFit, newdata, submodels = NULL) 
{
    if (!is.matrix(newdata)) 
        newdata <- as.matrix(newdata)
    out <- predict(modelFit, newdata, type = "prob", ncomp = modelFit$tuneValue$ncomp)
    if (length(dim(out)) == 3) {
        if (dim(out)[1] > 1) {
            out <- out[, , 1]
        }
        else {
            out <- as.data.frame(t(out[, , 1]))
        }
    }
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels) + 
            1)
        tmp[[1]] <- out
        for (j in seq(along = submodels$ncomp)) {
            tmpProb <- predict(modelFit, newdata, type = "prob", 
                ncomp = submodels$ncomp[j])
            if (length(dim(tmpProb)) == 3) {
                if (dim(tmpProb)[1] > 1) {
                  tmpProb <- tmpProb[, , 1]
                }
                else {
                  tmpProb <- as.data.frame(t(tmpProb[, , 1]))
                }
            }
            tmp[[j + 1]] <- as.data.frame(tmpProb[, modelFit$obsLevels])
        }
        out <- tmp
    }
    out
}

$pls$varImp
function (object, estimate = NULL, ...) 
{
    modelCoef <- coef(object, intercept = FALSE, comps = 1:object$ncomp)
    perf <- MSEP(object)$val
    nms <- dimnames(perf)
    if (length(nms$estimate) > 1) {
        pIndex <- if (is.null(estimate)) 
            1
        else which(nms$estimate == estimate)
        perf <- perf[pIndex, , , drop = FALSE]
    }
    numResp <- dim(modelCoef)[2]
    if (numResp <= 2) {
        modelCoef <- modelCoef[, 1, , drop = FALSE]
        perf <- perf[, 1, ]
        delta <- -diff(perf)
        delta <- delta/sum(delta)
        out <- data.frame(Overall = apply(abs(modelCoef), 1, 
            weighted.mean, w = delta))
    }
    else {
        perf <- -t(apply(perf[1, , ], 1, diff))
        perf <- t(apply(perf, 1, function(u) u/sum(u)))
        out <- matrix(NA, ncol = numResp, nrow = dim(modelCoef)[1])
        for (i in 1:numResp) {
            tmp <- abs(modelCoef[, i, , drop = FALSE])
            out[, i] <- apply(tmp, 1, weighted.mean, w = perf[i, 
                ])
        }
        colnames(out) <- dimnames(modelCoef)[[2]]
        rownames(out) <- dimnames(modelCoef)[[1]]
    }
    as.data.frame(out)
}

$pls$predictors
function (x, ...) 
rownames(x$projection)

$pls$levels
function (x) 
x$obsLevels

$pls$tags
[1] "Partial Least Squares" "Feature Extraction"    "Linear Classifier"    
[4] "Linear Regression"    

$pls$sort
function (x) 
x[order(x[, 1]), ]


$plsRglm
$plsRglm$label
[1] "Partial Least Squares Generalized Linear Models "

$plsRglm$library
[1] "plsRglm"

$plsRglm$loop
NULL

$plsRglm$type
[1] "Classification" "Regression"    

$plsRglm$parameters
          parameter   class             label
1                nt numeric   #PLS Components
2 alpha.pvals.expli numeric p-Value threshold

$plsRglm$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- expand.grid(nt = 1:len, alpha.pvals.expli = 10^(c(-2:(len - 
            3), 0)))
    }
    else {
        out <- data.frame(nt = sample(1:ncol(x), size = len, 
            replace = TRUE), alpha.pvals.expli = runif(len, min = 0, 
            0.2))
    }
    out
}

$plsRglm$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    if (is.factor(y)) {
        lv <- levels(y)
        y <- as.numeric(y) - 1
        dst <- "pls-glm-logistic"
    }
    else {
        lv <- NULL
        dst <- "pls-glm-gaussian"
    }
    theDots <- list(...)
    if (any(names(theDots) == "modele")) {
        mod <- plsRglm(y, x, nt = param$nt, pvals.expli = param$alpha.pvals.expli < 
            1, sparse = param$alpha.pvals.expli < 1, alpha.pvals.expli = param$alpha.pvals.expli, 
            ...)
    }
    else {
        mod <- plsRglm(y, x, nt = param$nt, modele = dst, pvals.expli = param$alpha.pvals.expli < 
            1, sparse = param$alpha.pvals.expli < 1, alpha.pvals.expli = param$alpha.pvals.expli, 
            ...)
    }
    mod
}

$plsRglm$predict
function (modelFit, newdata, submodels = NULL) 
{
    out <- predict(modelFit, newdata, type = "response")
    if (modelFit$problemType == "Classification") {
        out <- factor(ifelse(out >= 0.5, modelFit$obsLevels[2], 
            modelFit$obsLevels[1]))
    }
    out
}

$plsRglm$prob
function (modelFit, newdata, submodels = NULL) 
{
    out <- predict(modelFit, newdata, type = "response")
    out <- cbind(1 - out, out)
    dimnames(out)[[2]] <- rev(modelFit$obsLevels)
    out
}

$plsRglm$varImp
NULL

$plsRglm$predictors
function (x, ...) 
{
    vars <- names(which(coef(x)[[2]][, 1] != 0))
    vars[vars != "Intercept"]
}

$plsRglm$tags
[1] "Generalized Linear Models" "Partial Least Squares"    

$plsRglm$levels
function (x) 
x$lev

$plsRglm$sort
function (x) 
x[order(-x$alpha.pvals.expli, x$nt), ]


> getModelInfo("pls", regex = FALSE)
$pls
$pls$label
[1] "Partial Least Squares"

$pls$library
[1] "pls"

$pls$type
[1] "Regression"     "Classification"

$pls$parameters
  parameter   class       label
1     ncomp numeric #Components

$pls$grid
function (x, y, len = NULL, search = "grid") 
{
    if (search == "grid") {
        out <- data.frame(ncomp = seq(1, min(ncol(x) - 1, len), 
            by = 1))
    }
    else {
        out <- data.frame(ncomp = unique(sample(1:ncol(x), replace = TRUE)))
    }
    out
}

$pls$loop
function (grid) 
{
    grid <- grid[order(grid$ncomp, decreasing = TRUE), , drop = FALSE]
    loop <- grid[1, , drop = FALSE]
    submodels <- list(grid[-1, , drop = FALSE])
    list(loop = loop, submodels = submodels)
}

$pls$fit
function (x, y, wts, param, lev, last, classProbs, ...) 
{
    out <- if (is.factor(y)) {
        plsda(x, y, method = "oscorespls", ncomp = param$ncomp, 
            ...)
    }
    else {
        dat <- if (is.data.frame(x)) 
            x
        else as.data.frame(x)
        dat$.outcome <- y
        plsr(.outcome ~ ., data = dat, method = "oscorespls", 
            ncomp = param$ncomp, ...)
    }
    out
}

$pls$predict
function (modelFit, newdata, submodels = NULL) 
{
    out <- if (modelFit$problemType == "Classification") {
        if (!is.matrix(newdata)) 
            newdata <- as.matrix(newdata)
        out <- predict(modelFit, newdata, type = "class")
    }
    else as.vector(pls:::predict.mvr(modelFit, newdata, ncomp = max(modelFit$ncomp)))
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels))
        if (modelFit$problemType == "Classification") {
            if (length(submodels$ncomp) > 1) {
                tmp <- as.list(predict(modelFit, newdata, ncomp = submodels$ncomp))
            }
            else tmp <- list(predict(modelFit, newdata, ncomp = submodels$ncomp))
        }
        else {
            tmp <- as.list(as.data.frame(apply(predict(modelFit, 
                newdata, ncomp = submodels$ncomp), 3, function(x) list(x))))
        }
        out <- c(list(out), tmp)
    }
    out
}

$pls$prob
function (modelFit, newdata, submodels = NULL) 
{
    if (!is.matrix(newdata)) 
        newdata <- as.matrix(newdata)
    out <- predict(modelFit, newdata, type = "prob", ncomp = modelFit$tuneValue$ncomp)
    if (length(dim(out)) == 3) {
        if (dim(out)[1] > 1) {
            out <- out[, , 1]
        }
        else {
            out <- as.data.frame(t(out[, , 1]))
        }
    }
    if (!is.null(submodels)) {
        tmp <- vector(mode = "list", length = nrow(submodels) + 
            1)
        tmp[[1]] <- out
        for (j in seq(along = submodels$ncomp)) {
            tmpProb <- predict(modelFit, newdata, type = "prob", 
                ncomp = submodels$ncomp[j])
            if (length(dim(tmpProb)) == 3) {
                if (dim(tmpProb)[1] > 1) {
                  tmpProb <- tmpProb[, , 1]
                }
                else {
                  tmpProb <- as.data.frame(t(tmpProb[, , 1]))
                }
            }
            tmp[[j + 1]] <- as.data.frame(tmpProb[, modelFit$obsLevels])
        }
        out <- tmp
    }
    out
}

$pls$varImp
function (object, estimate = NULL, ...) 
{
    modelCoef <- coef(object, intercept = FALSE, comps = 1:object$ncomp)
    perf <- MSEP(object)$val
    nms <- dimnames(perf)
    if (length(nms$estimate) > 1) {
        pIndex <- if (is.null(estimate)) 
            1
        else which(nms$estimate == estimate)
        perf <- perf[pIndex, , , drop = FALSE]
    }
    numResp <- dim(modelCoef)[2]
    if (numResp <= 2) {
        modelCoef <- modelCoef[, 1, , drop = FALSE]
        perf <- perf[, 1, ]
        delta <- -diff(perf)
        delta <- delta/sum(delta)
        out <- data.frame(Overall = apply(abs(modelCoef), 1, 
            weighted.mean, w = delta))
    }
    else {
        perf <- -t(apply(perf[1, , ], 1, diff))
        perf <- t(apply(perf, 1, function(u) u/sum(u)))
        out <- matrix(NA, ncol = numResp, nrow = dim(modelCoef)[1])
        for (i in 1:numResp) {
            tmp <- abs(modelCoef[, i, , drop = FALSE])
            out[, i] <- apply(tmp, 1, weighted.mean, w = perf[i, 
                ])
        }
        colnames(out) <- dimnames(modelCoef)[[2]]
        rownames(out) <- dimnames(modelCoef)[[1]]
    }
    as.data.frame(out)
}

$pls$predictors
function (x, ...) 
rownames(x$projection)

$pls$levels
function (x) 
x$obsLevels

$pls$tags
[1] "Partial Least Squares" "Feature Extraction"    "Linear Classifier"    
[4] "Linear Regression"    

$pls$sort
function (x) 
x[order(x[, 1]), ]


> 
> ## Not run: 
> ##D checkInstall(getModelInfo("pls")$library)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("nearZeroVar")
> ### * nearZeroVar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nearZeroVar
> ### Title: Identification of near zero variance predictors
> ### Aliases: nearZeroVar nzv checkResamples checkConditionalX
> ### Keywords: utilities
> 
> ### ** Examples
> 
> nearZeroVar(iris[, -5], saveMetrics = TRUE)
             freqRatio percentUnique zeroVar   nzv
Sepal.Length  1.111111      23.33333   FALSE FALSE
Sepal.Width   1.857143      15.33333   FALSE FALSE
Petal.Length  1.000000      28.66667   FALSE FALSE
Petal.Width   2.230769      14.66667   FALSE FALSE
> 
> data(BloodBrain)
> nearZeroVar(bbbDescr)
[1]  3 16 17 22 25 50 60
> 
> 
> set.seed(1)
> classes <- factor(rep(letters[1:3], each = 30))
> x <- data.frame(x1 = rep(c(0, 1), 45),
+                 x2 = c(rep(0, 10), rep(1, 80)))
> 
> lapply(x, table, y = classes)
$x1
   y
     a  b  c
  0 15 15 15
  1 15 15 15

$x2
   y
     a  b  c
  0 10  0  0
  1 20 30 30

> checkConditionalX(x, classes)
[1] 2
> 
> folds <- createFolds(classes, k = 3, returnTrain = TRUE)
> x$x3 <- x$x1
> x$x3[folds[[1]]] <- 0
> 
> checkResamples(folds, x, classes)
[1] 3 2
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("nullModel")
> ### * nullModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nullModel
> ### Title: Fit a simple, non-informative model
> ### Aliases: nullModel nullModel.default predict.nullModel
> ### Keywords: models
> 
> ### ** Examples
> 
> outcome <- factor(sample(letters[1:2], 
+                          size = 100, 
+                          prob = c(.1, .9), 
+                          replace = TRUE))
> useless <- nullModel(y = outcome)
> useless
Null Regression Model

Call:
nullModel.default(y = outcome)

Predicted Value: b 
> predict(useless, matrix(NA, nrow = 10))
 [1] b b b b b b b b b b
Levels: a b
> 
> 
> 
> 
> cleanEx()
> nameEx("panel.lift")
> ### * panel.lift
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: panel.lift2
> ### Title: Lattice Panel Functions for Lift Plots
> ### Aliases: panel.lift panel.lift2
> ### Keywords: hplot
> 
> ### ** Examples
> 
> set.seed(1)
> simulated <- data.frame(obs = factor(rep(letters[1:2], each = 100)),
+                         perfect = sort(runif(200), decreasing = TRUE),
+                         random = runif(200))
> 
> regionInfo <- trellis.par.get("reference.line")
> regionInfo$col <- "lightblue"
> trellis.par.set("reference.line", regionInfo)
> 
> lift2 <- lift(obs ~ random + perfect, data = simulated)
> lift2

Call:
lift.formula(x = obs ~ random + perfect, data = simulated)

Models: random, perfect 
Event: a (50%)
> xyplot(lift2, auto.key = list(columns = 2))
> 
> ## use a different panel function
> xyplot(lift2, panel = panel.lift)
> 
> 
> 
> cleanEx()
> nameEx("pcaNNet")
> ### * pcaNNet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pcaNNet.default
> ### Title: Neural Networks with a Principal Component Step
> ### Aliases: pcaNNet.default predict.pcaNNet pcaNNet.formula pcaNNet
> ### Keywords: neural
> 
> ### ** Examples
> 
> data(BloodBrain)
> modelFit <- pcaNNet(bbbDescr[, 1:10], logBBB, size = 5, linout = TRUE, trace = FALSE)
> modelFit
Neural Network Model with PCA Pre-Processing

Created from 208 samples and 10 variables
PCA needed 9 components to capture 99 percent of the variance

a 9-5-1 network with 56 weights
options were - linear output units 

> 
> predict(modelFit, bbbDescr[, 1:10])
            [,1]
1    0.547635551
2   -1.094184949
3   -0.380244861
4    0.346560137
5    0.607887571
6    0.551055176
7   -0.433447188
8    0.594042888
9    0.616490892
10   0.469154553
11   0.749525286
12   0.852711353
13   0.561492455
14  -0.944257865
15  -0.450996949
16  -0.983423204
17  -0.187962649
18  -0.476810794
19  -0.217829625
20  -0.618672892
21   0.173723741
22   0.633440621
23   0.642971507
24   0.581013106
25  -0.826867414
26   0.469154553
27  -0.039493704
28  -0.898368909
29   0.585235207
30   0.411755523
31  -0.144048483
32  -1.104558708
33  -0.969352050
34  -0.535933518
35   0.643828798
36   0.511001266
37   0.356303498
38  -1.606647743
39   0.108864794
40   0.868402175
41   1.128439331
42   0.063423787
43  -0.270845573
44   0.682140238
45  -0.831488294
46   0.605162323
47   0.296607359
48   0.636413694
49   0.648471720
50   0.622410771
51   0.628500364
52   0.567225907
53   0.487124547
54   0.623844419
55   0.643869629
56  -0.613360471
57  -0.764306369
58  -0.362845264
59  -0.914188867
60  -0.195953922
61   0.081702927
62   0.303066228
63   0.167669394
64  -0.753202519
65  -0.140622507
66   0.637863215
67   0.618754308
68  -0.242993438
69  -0.561962051
70   0.371143124
71   0.466270852
72   0.643520684
73   0.458431692
74   0.546545459
75   0.611178531
76  -0.196063314
77  -0.767634438
78   0.402831727
79   1.268339145
80  -0.733125153
81  -0.648791508
82   0.643770158
83   0.342102471
84   0.414648476
85   0.592921234
86   0.176497228
87  -1.174280006
88  -0.303849699
89   0.553569892
90   0.590085699
91   0.466330417
92   1.359116844
93  -1.102057770
94  -0.819510050
95   0.631521799
96   0.415905790
97  -0.531081840
98   0.549179097
99   0.487706601
100  0.631220752
101 -0.091260960
102  0.628304985
103 -0.498544489
104  0.640737451
105  0.638464582
106 -0.676680446
107  0.409475326
108 -0.971598963
109 -0.474959434
110  0.611656624
111  0.266052084
112 -1.230283115
113  0.548489702
114 -0.869598913
115 -0.610564413
116  0.171336278
117  0.519662262
118 -0.616623000
119 -0.053148931
120  0.316995586
121 -0.974243022
122  0.592251018
123 -1.068149179
124 -0.876301591
125  0.111635993
126 -0.214974378
127 -0.185076780
128  0.581034198
129 -0.108647754
130  0.607908438
131  0.634837305
132  0.635841955
133  0.633667659
134 -0.576295857
135  0.643389419
136  0.185095630
137 -1.480812060
138 -0.833797860
139 -0.123688204
140  0.063305538
141 -0.091881638
142  0.505193645
143 -2.279126902
144  0.431158053
145 -0.153130775
146  0.627985488
147 -0.571007389
148  0.641501249
149  0.260968089
150 -0.283588166
151  0.035319362
152  0.181779516
153  0.467155682
154 -0.283588166
155  0.204385681
156  0.064452783
157  0.061651815
158 -0.238711824
159  0.045605200
160  0.635483914
161 -0.044492333
162  0.531599304
163 -0.221369578
164 -0.642739809
165 -0.218635333
166  0.642136267
167  0.636010492
168  0.628015927
169  0.186565332
170  0.173819525
171 -0.129427952
172  0.112856423
173  0.032336337
174  0.568006620
175  0.182538466
176 -0.611409511
177 -0.482640074
178  0.211197720
179  0.079393290
180 -1.102613063
181 -0.865034473
182 -1.129304188
183 -1.742665531
184 -1.184342047
185 -1.509050239
186 -1.439110190
187 -0.999880393
188 -0.966599678
189 -0.004726891
190  0.051141304
191 -0.933888311
192  0.592192493
193 -0.090437365
194  0.029210406
195  0.635512185
196 -0.129427952
197 -0.747776790
198 -1.044793754
199 -0.040354993
200  0.668522607
201  0.614172373
202 -1.688184882
203 -0.216292649
204  0.317288967
205  0.636033043
206  0.043079720
207 -0.534499563
208 -0.112124260
> 
> 
> 
> cleanEx()
> nameEx("plot.gafs")
> ### * plot.gafs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.gafs
> ### Title: Plot Method for the gafs and safs Classes
> ### Aliases: plot.safs plot.gafs
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1)
> ##D train_data <- twoClassSim(100, noiseVars = 10)
> ##D test_data  <- twoClassSim(10,  noiseVars = 10)
> ##D 
> ##D ## A short example 
> ##D ctrl <- safsControl(functions = rfSA, 
> ##D                     method = "cv",
> ##D                     number = 3)
> ##D 
> ##D rf_search <- safs(x = train_data[, -ncol(train_data)],
> ##D                   y = train_data$Class,
> ##D                   iters = 50,
> ##D                   safsControl = ctrl)
> ##D 
> ##D plot(rf_search)
> ##D plot(rf_search, 
> ##D 	 output = "lattice", 
> ##D 	 auto.key = list(columns = 2))
> ##D 
> ##D plot_data <- plot(rf_search, output = "data")   
> ##D summary(plot_data)                 
> ##D     
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plot.rfe")
> ### * plot.rfe
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.rfe
> ### Title: Plot RFE Performance Profiles
> ### Aliases: plot.rfe ggplot.rfe
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(BloodBrain)
> ##D 
> ##D x <- scale(bbbDescr[,-nearZeroVar(bbbDescr)])
> ##D x <- x[, -findCorrelation(cor(x), .8)]
> ##D x <- as.data.frame(x)
> ##D 
> ##D set.seed(1)
> ##D lmProfile <- rfe(x, logBBB,
> ##D                  sizes = c(2:25, 30, 35, 40, 45, 50, 55, 60, 65),
> ##D                  rfeControl = rfeControl(functions = lmFuncs, 
> ##D                                          number = 200))
> ##D plot(lmProfile)
> ##D plot(lmProfile, metric = "Rsquared")
> ##D ggplot(lmProfile)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plot.train")
> ### * plot.train
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.train
> ### Title: Plot Method for the train Class
> ### Aliases: plot.train ggplot.train
> ### Keywords: hplot
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D library(klaR)
> ##D rdaFit <- train(Species ~ .,
> ##D                 data = iris, 
> ##D                 method = "rda", 
> ##D                 control = trainControl(method = "cv"))
> ##D plot(rdaFit)
> ##D plot(rdaFit, plotType = "level")
> ##D 
> ##D ggplot(rdaFit) + theme_bw()
> ##D 
> ## End(Not run)
>  
> 
> 
> cleanEx()
> nameEx("plotClassProbs")
> ### * plotClassProbs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotClassProbs
> ### Title: Plot Predicted Probabilities in Classification Models
> ### Aliases: plotClassProbs
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(mdrr)
> ##D set.seed(90)
> ##D inTrain <- createDataPartition(mdrrClass, p = .5)[[1]]
> ##D 
> ##D trainData <- mdrrDescr[inTrain,1:20]
> ##D testData <- mdrrDescr[-inTrain,1:20]
> ##D 
> ##D trainY <- mdrrClass[inTrain]
> ##D testY <- mdrrClass[-inTrain]
> ##D 
> ##D ctrl <- trainControl(method = "cv")
> ##D 
> ##D nbFit1 <- train(trainData, trainY, "nb",
> ##D                 trControl = ctrl,
> ##D                 tuneGrid = data.frame(usekernel = TRUE, fL = 0))
> ##D 
> ##D nbFit2 <- train(trainData, trainY, "nb",
> ##D                 trControl = ctrl,
> ##D                 tuneGrid = data.frame(usekernel = FALSE, fL = 0))
> ##D 
> ##D 
> ##D models <- list(para = nbFit2, nonpara = nbFit1)
> ##D 
> ##D predProbs <- extractProb(models, testX = testData,  testY = testY)
> ##D 
> ##D plotClassProbs(predProbs, useObjects = TRUE)
> ##D plotClassProbs(predProbs,
> ##D                subset = object == "para" & dataType == "Test")
> ##D plotClassProbs(predProbs,
> ##D                useObjects = TRUE,
> ##D                plotType = "densityplot",
> ##D                auto.key = list(columns = 2))
> ## End(Not run)
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("plotObsVsPred")
> ### * plotObsVsPred
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotObsVsPred
> ### Title: Plot Observed versus Predicted Results in Regression and
> ###   Classification Models
> ### Aliases: plotObsVsPred
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # regression example
> ##D data(BostonHousing)
> ##D rpartFit <- train(BostonHousing[1:100, -c(4, 14)], 
> ##D                   BostonHousing$medv[1:100], 
> ##D                   "rpart", tuneLength = 9)
> ##D plsFit <- train(BostonHousing[1:100, -c(4, 14)], 
> ##D                 BostonHousing$medv[1:100], 
> ##D                 "pls")
> ##D 
> ##D predVals <- extractPrediction(list(rpartFit, plsFit), 
> ##D                               testX = BostonHousing[101:200, -c(4, 14)], 
> ##D                               testY = BostonHousing$medv[101:200], 
> ##D                               unkX = BostonHousing[201:300, -c(4, 14)])
> ##D 
> ##D plotObsVsPred(predVals)
> ##D 
> ##D 
> ##D #classification example
> ##D data(Satellite)
> ##D numSamples <- dim(Satellite)[1]
> ##D set.seed(716)
> ##D 
> ##D varIndex <- 1:numSamples
> ##D 
> ##D trainSamples <- sample(varIndex, 150)
> ##D 
> ##D varIndex <- (1:numSamples)[-trainSamples]
> ##D testSamples <- sample(varIndex, 100)
> ##D 
> ##D varIndex <- (1:numSamples)[-c(testSamples, trainSamples)]
> ##D unkSamples <- sample(varIndex, 50)
> ##D 
> ##D trainX <- Satellite[trainSamples, -37]
> ##D trainY <- Satellite[trainSamples, 37]
> ##D 
> ##D testX <- Satellite[testSamples, -37]
> ##D testY <- Satellite[testSamples, 37]
> ##D 
> ##D unkX <- Satellite[unkSamples, -37]
> ##D 
> ##D knnFit  <- train(trainX, trainY, "knn")
> ##D rpartFit <- train(trainX, trainY, "rpart")
> ##D 
> ##D predTargets <- extractPrediction(list(knnFit, rpartFit), 
> ##D                                  testX = testX, 
> ##D                                  testY = testY, 
> ##D                                  unkX = unkX)
> ##D 
> ##D plotObsVsPred(predTargets)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plsda")
> ### * plsda
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plsda
> ### Title: Partial Least Squares and Sparse Partial Least Squares
> ###   Discriminant Analysis
> ### Aliases: plsda.default predict.plsda plsda splsda.default
> ###   predict.splsda splsda
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(mdrr)
> ##D set.seed(1)
> ##D inTrain <- sample(seq(along = mdrrClass), 450)
> ##D  
> ##D nzv <- nearZeroVar(mdrrDescr)
> ##D filteredDescr <- mdrrDescr[, -nzv]
> ##D 
> ##D training <- filteredDescr[inTrain,]
> ##D test <- filteredDescr[-inTrain,]
> ##D trainMDRR <- mdrrClass[inTrain]
> ##D testMDRR <- mdrrClass[-inTrain]
> ##D  
> ##D preProcValues <- preProcess(training)
> ##D 
> ##D trainDescr <- predict(preProcValues, training)
> ##D testDescr <- predict(preProcValues, test)
> ##D 
> ##D useBayes   <- plsda(trainDescr, trainMDRR, ncomp = 5,
> ##D                     probMethod = "Bayes")
> ##D useSoftmax <- plsda(trainDescr, trainMDRR, ncomp = 5)
> ##D 
> ##D confusionMatrix(predict(useBayes, testDescr),
> ##D                 testMDRR)
> ##D 
> ##D confusionMatrix(predict(useSoftmax, testDescr),
> ##D                 testMDRR)
> ##D 
> ##D histogram(~predict(useBayes, testDescr, type = "prob")[,"Active",]
> ##D           | testMDRR, xlab = "Active Prob", xlim = c(-.1,1.1))
> ##D histogram(~predict(useSoftmax, testDescr, type = "prob")[,"Active",]
> ##D           | testMDRR, xlab = "Active Prob", xlim = c(-.1,1.1))
> ##D 
> ##D 
> ##D ## different sized objects are returned
> ##D length(predict(useBayes, testDescr))
> ##D dim(predict(useBayes, testDescr, ncomp = 1:3))
> ##D dim(predict(useBayes, testDescr, type = "prob"))
> ##D dim(predict(useBayes, testDescr, type = "prob", ncomp = 1:3))
> ##D 
> ##D ## Using spls:
> ##D ## (As of 11/09, the spls package now has a similar function with
> ##D ## the same mane. To avoid conflicts, use caret:::splsda to 
> ##D ## get this version)
> ##D 
> ##D splsFit <- caret:::splsda(trainDescr, trainMDRR, 
> ##D                           K = 5, eta = .9,
> ##D                           probMethod = "Bayes")
> ##D 
> ##D confusionMatrix(caret:::predict.splsda(splsFit, testDescr),
> ##D                 testMDRR)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("postResample")
> ### * postResample
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: postResample
> ### Title: Calculates performance across resamples
> ### Aliases: postResample defaultSummary twoClassSummary getTrainPerf
> ###   mnLogLoss R2 RMSE multiClassSummary
> ### Keywords: utilities
> 
> ### ** Examples
> 
> predicted <-  matrix(rnorm(50), ncol = 5)
> observed <- rnorm(10)
> apply(predicted, 2, postResample, obs = observed)
              [,1]      [,2]      [,3]       [,4]      [,5]
RMSE     1.5326605 1.1495602 0.9728776 1.34291281 1.4303961
Rsquared 0.3762138 0.1091892 0.2534096 0.03811267 0.5410893
> 
> classes <- c("class1", "class2")
> set.seed(1)
> dat <- data.frame(obs =  factor(sample(classes, 50, replace = TRUE)),
+                   pred = factor(sample(classes, 50, replace = TRUE)),
+                   class1 = runif(50), class2 = runif(50))
> 
> defaultSummary(dat, lev = classes)
 Accuracy     Kappa 
0.6400000 0.2890995 
> twoClassSummary(dat, lev = classes)
      ROC      Sens      Spec 
0.4847021 0.7391304 0.5555556 
> mnLogLoss(dat, lev = classes)
  logLoss 
0.9689131 
> 
> 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("prcomp.resamples")
> ### * prcomp.resamples
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prcomp.resamples
> ### Title: Principal Components Analysis of Resampling Results
> ### Aliases: prcomp.resamples cluster.resamples cluster
> ###   plot.prcomp.resamples
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D #load(url("http://topepo.github.io/caret/exampleModels.RData"))
> ##D 
> ##D resamps <- resamples(list(CART = rpartFit,
> ##D                           CondInfTree = ctreeFit,
> ##D                           MARS = earthFit))
> ##D resampPCA <- prcomp(resamps)
> ##D 
> ##D resampPCA
> ##D 
> ##D plot(resampPCA, what = "scree")
> ##D 
> ##D plot(resampPCA, what = "components")
> ##D 
> ##D plot(resampPCA, what = "components", dims = 2, auto.key = list(columns = 3))
> ##D 
> ##D clustered <- cluster(resamps)
> ##D plot(clustered)
> ##D 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("preProcess")
> ### * preProcess
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: preProcess
> ### Title: Pre-Processing of Predictors
> ### Aliases: preProcess preProcess.default predict.preProcess
> ### Keywords: utilities
> 
> ### ** Examples
> 
> data(BloodBrain)
> # one variable has one unique value
> ## Not run: 
> ##D preProc <- preProcess(bbbDescr)
> ##D 
> ##D preProc  <- preProcess(bbbDescr[1:100,-3])
> ##D training <- predict(preProc, bbbDescr[1:100,-3])
> ##D test     <- predict(preProc, bbbDescr[101:208,-3])
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("predict.bagEarth")
> ### * predict.bagEarth
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.bagEarth
> ### Title: Predicted values based on bagged Earth and FDA models
> ### Aliases: predict.bagEarth predict.bagFDA
> ### Keywords: regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(trees)
> ##D ## out of bag predictions vs just re-predicting the training set
> ##D fit1 <- bagEarth(Volume ~ ., data = trees, keepX = TRUE)
> ##D fit2 <- bagEarth(Volume ~ ., data = trees, keepX = FALSE)
> ##D hist(predict(fit1) - predict(fit2))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("predict.gafs")
> ### * predict.gafs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.gafs
> ### Title: Predict new samples
> ### Aliases: predict.gafs predict.safs
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> ## Not run: 
> ##D 
> ##D set.seed(1)
> ##D train_data <- twoClassSim(100, noiseVars = 10)
> ##D test_data  <- twoClassSim(10,  noiseVars = 10)
> ##D 
> ##D ## A short example 
> ##D ctrl <- safsControl(functions = rfSA, 
> ##D                     method = "cv",
> ##D                     number = 3)
> ##D 
> ##D rf_search <- safs(x = train_data[, -ncol(train_data)],
> ##D                   y = train_data$Class,
> ##D                   iters = 3,
> ##D                   safsControl = ctrl)
> ##D 
> ##D rf_search
> ##D 
> ##D predict(rf_search, train_data)  
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("print.train")
> ### * print.train
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: print.train
> ### Title: Print Method for the train Class
> ### Aliases: print.train
> ### Keywords: print
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(iris)
> ##D TrainData <- iris[,1:4]
> ##D TrainClasses <- iris[,5]
> ##D 
> ##D library(klaR)
> ##D rdaFit <- train(TrainData, TrainClasses, method = "rda",
> ##D                 control = trainControl(method = "cv"))
> ##D print(rdaFit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("resampleHist")
> ### * resampleHist
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: resampleHist
> ### Title: Plot the resampling distribution of the model statistics
> ### Aliases: resampleHist
> ### Keywords: hplot
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D data(iris)
> ##D TrainData <- iris[,1:4]
> ##D TrainClasses <- iris[,5]
> ##D 
> ##D knnFit <- train(TrainData, TrainClasses, "knn")
> ##D 
> ##D resampleHist(knnFit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("resampleSummary")
> ### * resampleSummary
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: resampleSummary
> ### Title: Summary of resampled performance estimates
> ### Aliases: resampleSummary
> ### Keywords: utilities
> 
> ### ** Examples
> 
> resampleSummary(rnorm(10), matrix(rnorm(50), ncol = 5))
$metrics
     RMSE  Rsquared      RMSE  Rsquared 
1.2856814 0.2636029 0.2244948 0.2027765 

$data
          obs        pred      group
1   0.3981059 -0.62645381 Resample 1
2  -0.6120264  0.18364332 Resample 1
3   0.3411197 -0.83562861 Resample 1
4  -1.1293631  1.59528080 Resample 1
5   1.4330237  0.32950777 Resample 1
6   1.9803999 -0.82046838 Resample 1
7  -0.3672215  0.48742905 Resample 1
8  -1.0441346  0.73832471 Resample 1
9   0.5697196  0.57578135 Resample 1
10 -0.1350546 -0.30538839 Resample 1
11  0.3981059  1.51178117 Resample 2
12 -0.6120264  0.38984324 Resample 2
13  0.3411197 -0.62124058 Resample 2
14 -1.1293631 -2.21469989 Resample 2
15  1.4330237  1.12493092 Resample 2
16  1.9803999 -0.04493361 Resample 2
17 -0.3672215 -0.01619026 Resample 2
18 -1.0441346  0.94383621 Resample 2
19  0.5697196  0.82122120 Resample 2
20 -0.1350546  0.59390132 Resample 2
21  0.3981059  0.91897737 Resample 3
22 -0.6120264  0.78213630 Resample 3
23  0.3411197  0.07456498 Resample 3
24 -1.1293631 -1.98935170 Resample 3
25  1.4330237  0.61982575 Resample 3
26  1.9803999 -0.05612874 Resample 3
27 -0.3672215 -0.15579551 Resample 3
28 -1.0441346 -1.47075238 Resample 3
29  0.5697196 -0.47815006 Resample 3
30 -0.1350546  0.41794156 Resample 3
31  0.3981059  1.35867955 Resample 4
32 -0.6120264 -0.10278773 Resample 4
33  0.3411197  0.38767161 Resample 4
34 -1.1293631 -0.05380504 Resample 4
35  1.4330237 -1.37705956 Resample 4
36  1.9803999 -0.41499456 Resample 4
37 -0.3672215 -0.39428995 Resample 4
38 -1.0441346 -0.05931340 Resample 4
39  0.5697196  1.10002537 Resample 4
40 -0.1350546  0.76317575 Resample 4
41  0.3981059 -0.16452360 Resample 5
42 -0.6120264 -0.25336168 Resample 5
43  0.3411197  0.69696338 Resample 5
44 -1.1293631  0.55666320 Resample 5
45  1.4330237 -0.68875569 Resample 5
46  1.9803999 -0.70749516 Resample 5
47 -0.3672215  0.36458196 Resample 5
48 -1.0441346  0.76853292 Resample 5
49  0.5697196 -0.11234621 Resample 5
50 -0.1350546  0.88110773 Resample 5

> 
> 
> 
> cleanEx()
> nameEx("resamples")
> ### * resamples
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: resamples
> ### Title: Collation and Visualization of Resampling Results
> ### Aliases: resamples.default resamples summary.resamples sort.resamples
> ###   as.matrix.resamples as.data.frame.resamples modelCor
> ### Keywords: models
> 
> ### ** Examples
> 
> 
> data(BloodBrain)
> set.seed(1)
> 
> ## tmp <- createDataPartition(logBBB,
> ##                            p = .8,
> ##                            times = 100)
> 
> ## rpartFit <- train(bbbDescr, logBBB,
> ##                   "rpart", 
> ##                   tuneLength = 16,
> ##                   trControl = trainControl(
> ##                     method = "LGOCV", index = tmp))
> 
> ## ctreeFit <- train(bbbDescr, logBBB,
> ##                   "ctree", 
> ##                   trControl = trainControl(
> ##                     method = "LGOCV", index = tmp))
> 
> ## earthFit <- train(bbbDescr, logBBB,
> ##                   "earth",
> ##                   tuneLength = 20,
> ##                   trControl = trainControl(
> ##                     method = "LGOCV", index = tmp))
> 
> ## or load pre-calculated results using:
> ## load(url("http://caret.r-forge.r-project.org/exampleModels.RData"))
> 
> ## resamps <- resamples(list(CART = rpartFit,
> ##                           CondInfTree = ctreeFit,
> ##                           MARS = earthFit))
> 
> ## resamps
> ## summary(resamps)
> 
> 
> 
> cleanEx()
> nameEx("rf_seq")
> ### * rf_seq
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: var_seq
> ### Title: Sequences of Variables for Tuning
> ### Aliases: var_seq
> ### Keywords: models
> 
> ### ** Examples
> 
> var_seq(p = 100, len = 10)
 [1]   2  12  23  34  45  56  67  78  89 100
> var_seq(p = 600, len = 10)
 [1]   2   3   7  13  25  47  89 168 318 600
> 
> 
> 
> cleanEx()
> nameEx("rfe")
> ### * rfe
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rfe
> ### Title: Backwards Feature Selection
> ### Aliases: rfe rfe.default rfeIter predict.rfe update.rfe
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(BloodBrain)
> ##D 
> ##D x <- scale(bbbDescr[,-nearZeroVar(bbbDescr)])
> ##D x <- x[, -findCorrelation(cor(x), .8)]
> ##D x <- as.data.frame(x)
> ##D 
> ##D set.seed(1)
> ##D lmProfile <- rfe(x, logBBB,
> ##D                  sizes = c(2:25, 30, 35, 40, 45, 50, 55, 60, 65),
> ##D                  rfeControl = rfeControl(functions = lmFuncs, 
> ##D                                          number = 200))
> ##D set.seed(1)
> ##D lmProfile2 <- rfe(x, logBBB,
> ##D                  sizes = c(2:25, 30, 35, 40, 45, 50, 55, 60, 65),
> ##D                  rfeControl = rfeControl(functions = lmFuncs, 
> ##D                                          rerank = TRUE, 
> ##D                                          number = 200))
> ##D 
> ##D xyplot(lmProfile$results$RMSE + lmProfile2$results$RMSE  ~ 
> ##D        lmProfile$results$Variables, 
> ##D        type = c("g", "p", "l"), 
> ##D        auto.key = TRUE)
> ##D 
> ##D rfProfile <- rfe(x, logBBB,
> ##D                  sizes = c(2, 5, 10, 20),
> ##D                  rfeControl = rfeControl(functions = rfFuncs))
> ##D 
> ##D bagProfile <- rfe(x, logBBB,
> ##D                   sizes = c(2, 5, 10, 20),
> ##D                   rfeControl = rfeControl(functions = treebagFuncs))
> ##D 
> ##D set.seed(1)
> ##D svmProfile <- rfe(x, logBBB,
> ##D                   sizes = c(2, 5, 10, 20),
> ##D                   rfeControl = rfeControl(functions = caretFuncs, 
> ##D                                           number = 200),
> ##D                   ## pass options to train()
> ##D                   method = "svmRadial")
> ##D 
> ##D ## classification 
> ##D 
> ##D data(mdrr)
> ##D mdrrDescr <- mdrrDescr[,-nearZeroVar(mdrrDescr)]
> ##D mdrrDescr <- mdrrDescr[, -findCorrelation(cor(mdrrDescr), .8)]
> ##D 
> ##D set.seed(1)
> ##D inTrain <- createDataPartition(mdrrClass, p = .75, list = FALSE)[,1]
> ##D 
> ##D train <- mdrrDescr[ inTrain, ]
> ##D test  <- mdrrDescr[-inTrain, ]
> ##D trainClass <- mdrrClass[ inTrain]
> ##D testClass  <- mdrrClass[-inTrain]
> ##D 
> ##D set.seed(2)
> ##D ldaProfile <- rfe(train, trainClass,
> ##D                   sizes = c(1:10, 15, 30),
> ##D                   rfeControl = rfeControl(functions = ldaFuncs, method = "cv"))
> ##D plot(ldaProfile, type = c("o", "g"))
> ##D 
> ##D postResample(predict(ldaProfile, test), testClass)
> ##D 
> ## End(Not run)
> 
> #######################################
> ## Parallel Processing Example via multicore
> 
> ## Not run: 
> ##D library(doMC)
> ##D 
> ##D ## Note: if the underlying model also uses foreach, the
> ##D ## number of cores specified above will double (along with
> ##D ## the memory requirements)
> ##D registerDoMC(cores = 2)
> ##D 
> ##D set.seed(1)
> ##D lmProfile <- rfe(x, logBBB,
> ##D                  sizes = c(2:25, 30, 35, 40, 45, 50, 55, 60, 65),
> ##D                  rfeControl = rfeControl(functions = lmFuncs, 
> ##D                                          number = 200))
> ##D 
> ##D 
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("rfeControl")
> ### * rfeControl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rfeControl
> ### Title: Controlling the Feature Selection Algorithms
> ### Aliases: rfeControl
> ### Keywords: utilities
> 
> ### ** Examples
> 
>   ## Not run: 
> ##D subsetSizes <- c(2, 4, 6, 8)
> ##D set.seed(123)
> ##D seeds <- vector(mode = "list", length = 51)
> ##D for(i in 1:50) seeds[[i]] <- sample.int(1000, length(subsetSizes) + 1)
> ##D seeds[[51]] <- sample.int(1000, 1)
> ##D 
> ##D set.seed(1)
> ##D rfMod <- rfe(bbbDescr, logBBB,
> ##D              sizes = subsetSizes,
> ##D              rfeControl = rfeControl(functions = rfFuncs, 
> ##D                                      seeds = seeds,
> ##D                                      number = 50))
> ##D   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rfeFunctions")
> ### * rfeFunctions
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: caretFuncs
> ### Title: Backwards Feature Selection Helper Functions
> ### Aliases: caretFuncs lmFuncs rfFuncs gamFuncs treebagFuncs ldaFuncs
> ###   nbFuncs lrFuncs pickSizeBest pickSizeTolerance pickVars
> ### Keywords: models
> 
> ### ** Examples
> 
> ## For picking subset sizes:
> ## Minimize the RMSE
> example <- data.frame(RMSE = c(1.2, 1.1, 1.05, 1.01, 1.01, 1.03, 1.00),
+                       Variables = 1:7)
> ## Percent Loss in performance (positive)
> example$PctLoss <- (example$RMSE - min(example$RMSE))/min(example$RMSE)*100
> 
> xyplot(RMSE ~ Variables, data= example)
> xyplot(PctLoss ~ Variables, data= example)
> 
> absoluteBest <- pickSizeBest(example, metric = "RMSE", maximize = FALSE)
> within5Pct <- pickSizeTolerance(example, metric = "RMSE", maximize = FALSE)
> 
> cat("numerically optimal:",
+     example$RMSE[absoluteBest],
+     "RMSE in position",
+     absoluteBest, "\n")
numerically optimal: 1 RMSE in position 7 
> cat("Accepting a 1.5 pct loss:",
+     example$RMSE[within5Pct],
+     "RMSE in position",
+     within5Pct, "\n")
Accepting a 1.5 pct loss: 1.01 RMSE in position 4 
> 
> ## Example where we would like to maximize
> example2 <- data.frame(Rsquared = c(0.4, 0.6, 0.94, 0.95, 0.95, 0.95, 0.95),
+                       Variables = 1:7)
> ## Percent Loss in performance (positive)
> example2$PctLoss <- (max(example2$Rsquared) - example2$Rsquared)/max(example2$Rsquared)*100
> 
> xyplot(Rsquared ~ Variables, data= example2)
> xyplot(PctLoss ~ Variables, data= example2)
> 
> absoluteBest2 <- pickSizeBest(example2, metric = "Rsquared", maximize = TRUE)
> within5Pct2 <- pickSizeTolerance(example2, metric = "Rsquared", maximize = TRUE)
> 
> cat("numerically optimal:",
+     example2$Rsquared[absoluteBest2],
+     "R^2 in position",
+     absoluteBest2, "\n")
numerically optimal: 0.95 R^2 in position 4 
> cat("Accepting a 1.5 pct loss:",
+     example2$Rsquared[within5Pct2],
+     "R^2 in position",
+     within5Pct2, "\n")
Accepting a 1.5 pct loss: 0.94 R^2 in position 3 
> 
> 
> 
> cleanEx()
> nameEx("sa_functions")
> ### * sa_functions
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: safs_initial
> ### Title: Ancillary simulated annealing functions
> ### Aliases: safs_initial safs_perturb safs_prob caretSA rfSA treebagSA
> 
> ### ** Examples
> 
> selected_vars <- safs_initial(vars = 10 , prob = 0.2)
> selected_vars
[1] 3 4 5
> 
> ###
> 
> safs_perturb(selected_vars, vars = 10, number = 1)
[1]  3  4  5 10
> 
> ###
> 
> safs_prob(old = .8, new = .9, iteration = 1)
[1] 0.8824969
> safs_prob(old = .5, new = .6, iteration = 1)
[1] 0.8187308
> 
> grid <- expand.grid(old = c(4, 3.5),
+                     new = c(4.5, 4, 3.5) + 1,
+                     iter = 1:40)
> grid <- subset(grid, old < new)
> 
> grid$prob <- apply(grid, 1, 
+                    function(x) 
+                      safs_prob(new = x["new"], 
+                                old= x["old"], 
+                                iteration = x["iter"]))
> 
> grid$Difference <- factor(grid$new - grid$old)
> grid$Group <- factor(paste("Current Value", grid$old))
> 
> ggplot(grid, aes(x = iter, y = prob, color = Difference)) + 
+   geom_line() + facet_wrap(~Group) + theme_bw() +
+   ylab("Probability") + xlab("Iteration")
> 
> ## Not run: 
> ##D ###
> ##D ## Hypothetical examples
> ##D lda_sa <- safs(x = predictors,
> ##D                y = classes,
> ##D                safsControl = safsControl(functions = caretSA),
> ##D                ## now pass arguments to `train`
> ##D                method = "lda",
> ##D                metric = "Accuracy"
> ##D                trControl = trainControl(method = "cv", classProbs = TRUE))
> ##D 
> ##D rf_sa <- safs(x = predictors,
> ##D               y = classes,
> ##D               safsControl = safsControl(functions = rfSA),
> ##D               ## these are arguments to `randomForest`
> ##D               ntree = 1000,
> ##D               importance = TRUE)
> ##D 	
> ## End(Not run)
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("safs.default")
> ### * safs.default
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: safs.default
> ### Title: Simulated annealing feature selection
> ### Aliases: safs.default safs
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D 
> ##D set.seed(1)
> ##D train_data <- twoClassSim(100, noiseVars = 10)
> ##D test_data  <- twoClassSim(10,  noiseVars = 10)
> ##D 
> ##D ## A short example 
> ##D ctrl <- safsControl(functions = rfSA, 
> ##D                     method = "cv",
> ##D                     number = 3)
> ##D 
> ##D rf_search <- safs(x = train_data[, -ncol(train_data)],
> ##D                   y = train_data$Class,
> ##D                   iters = 3,
> ##D                   safsControl = ctrl)
> ##D 
> ##D rf_search 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("sbf")
> ### * sbf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sbf
> ### Title: Selection By Filtering (SBF)
> ### Aliases: sbf sbf.default sbf.formula predict.sbf
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(BloodBrain)
> ##D 
> ##D ## Use a GAM is the filter, then fit a random forest model
> ##D RFwithGAM <- sbf(bbbDescr, logBBB,
> ##D                  sbfControl = sbfControl(functions = rfSBF,
> ##D                                          verbose = FALSE, 
> ##D                                          method = "cv"))
> ##D RFwithGAM
> ##D 
> ##D predict(RFwithGAM, bbbDescr[1:10,])
> ##D 
> ##D ## classification example with parallel processing
> ##D 
> ##D ## library(doMC)
> ##D 
> ##D ## Note: if the underlying model also uses foreach, the
> ##D ## number of cores specified above will double (along with
> ##D ## the memory requirements)
> ##D ## registerDoMC(cores = 2)
> ##D 
> ##D data(mdrr)
> ##D mdrrDescr <- mdrrDescr[,-nearZeroVar(mdrrDescr)]
> ##D mdrrDescr <- mdrrDescr[, -findCorrelation(cor(mdrrDescr), .8)]
> ##D 
> ##D set.seed(1)
> ##D filteredNB <- sbf(mdrrDescr, mdrrClass,
> ##D                  sbfControl = sbfControl(functions = nbSBF,
> ##D                                          verbose = FALSE, 
> ##D                                          method = "repeatedcv",
> ##D                                          repeats = 5))
> ##D confusionMatrix(filteredNB)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("sbfControl")
> ### * sbfControl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sbfControl
> ### Title: Control Object for Selection By Filtering (SBF)
> ### Aliases: sbfControl
> ### Keywords: utilities
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(BloodBrain)
> ##D 
> ##D ## Use a GAM is the filter, then fit a random forest model
> ##D set.seed(1)
> ##D RFwithGAM <- sbf(bbbDescr, logBBB,
> ##D                  sbfControl = sbfControl(functions = rfSBF,
> ##D                                          verbose = FALSE, 
> ##D                                          seeds = sample.int(100000, 11),
> ##D                                          method = "cv"))
> ##D RFwithGAM
> ##D 
> ##D 
> ##D ## A simple example for multivariate scoring
> ##D rfSBF2 <- rfSBF
> ##D rfSBF2$score <- function(x, y) apply(x, 2, rfSBF$score, y = y)
> ##D 
> ##D set.seed(1)
> ##D RFwithGAM2 <- sbf(bbbDescr, logBBB,
> ##D                   sbfControl = sbfControl(functions = rfSBF2,
> ##D                                           verbose = FALSE, 
> ##D                                           seeds = sample.int(100000, 11),
> ##D                                           method = "cv",
> ##D                                           multivariate = TRUE))
> ##D RFwithGAM2
> ##D 
> ##D 
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("selection")
> ### * selection
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: oneSE
> ### Title: Selecting tuning Parameters
> ### Aliases: oneSE best tolerance
> ### Keywords: manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # simulate a PLS regression model
> ##D test <- data.frame(ncomp = 1:5,
> ##D                    RMSE = c(3, 1.1, 1.02, 1, 2),
> ##D                    RMSESD = .4)
> ##D 
> ##D best(test, "RMSE", maximize = FALSE)
> ##D oneSE(test, "RMSE", maximize = FALSE, num = 10)
> ##D tolerance(test, "RMSE", tol = 3, maximize = FALSE)
> ##D 
> ##D ### usage example
> ##D 
> ##D data(BloodBrain)
> ##D 
> ##D marsGrid <- data.frame(degree = 1, nprune = (1:10) * 3)
> ##D 
> ##D set.seed(1)
> ##D marsFit <- train(bbbDescr, logBBB,
> ##D                  method = "earth",
> ##D                  tuneGrid = marsGrid,
> ##D                  trControl = trainControl(method = "cv",
> ##D                                           number = 10,
> ##D                                           selectionFunction = "tolerance"))
> ##D 
> ##D # around 18 terms should yield the smallest CV RMSE     
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("sensitivity")
> ### * sensitivity
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sensitivity
> ### Title: Calculate sensitivity, specificity and predictive values
> ### Aliases: sensitivity sensitivity.default sensitivity.table
> ###   sensitivity.matrix specificity specificity.default specificity.table
> ###   specificity.matrix posPredValue posPredValue.default
> ###   posPredValue.table posPredValue.matrix negPredValue
> ###   negPredValue.default negPredValue.table negPredValue.matrix
> ### Keywords: manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ###################
> ##D ## 2 class example
> ##D 
> ##D lvs <- c("normal", "abnormal")
> ##D truth <- factor(rep(lvs, times = c(86, 258)),
> ##D                 levels = rev(lvs))
> ##D pred <- factor(
> ##D                c(
> ##D                  rep(lvs, times = c(54, 32)),
> ##D                  rep(lvs, times = c(27, 231))),               
> ##D                levels = rev(lvs))
> ##D 
> ##D xtab <- table(pred, truth)
> ##D 
> ##D sensitivity(pred, truth)
> ##D sensitivity(xtab)
> ##D posPredValue(pred, truth)
> ##D posPredValue(pred, truth, prevalence = 0.25)
> ##D 
> ##D specificity(pred, truth)
> ##D negPredValue(pred, truth)
> ##D negPredValue(xtab)
> ##D negPredValue(pred, truth, prevalence = 0.25)
> ##D 
> ##D 
> ##D prev <- seq(0.001, .99, length = 20)
> ##D npvVals <- ppvVals <- prev  * NA
> ##D for(i in seq(along = prev))
> ##D   {
> ##D     ppvVals[i] <- posPredValue(pred, truth, prevalence = prev[i])
> ##D     npvVals[i] <- negPredValue(pred, truth, prevalence = prev[i])
> ##D   }
> ##D 
> ##D plot(prev, ppvVals,
> ##D      ylim = c(0, 1),
> ##D      type = "l",
> ##D      ylab = "",
> ##D      xlab = "Prevalence (i.e. prior)")
> ##D points(prev, npvVals, type = "l", col = "red")
> ##D abline(h=sensitivity(pred, truth), lty = 2)
> ##D abline(h=specificity(pred, truth), lty = 2, col = "red")
> ##D legend(.5, .5,
> ##D        c("ppv", "npv", "sens", "spec"),
> ##D        col = c("black", "red", "black", "red"),
> ##D        lty = c(1, 1, 2, 2))
> ##D 
> ##D ###################
> ##D ## 3 class example
> ##D 
> ##D library(MASS)
> ##D 
> ##D fit <- lda(Species ~ ., data = iris)
> ##D model <- predict(fit)$class
> ##D 
> ##D irisTabs <- table(model, iris$Species)
> ##D 
> ##D ## When passing factors, an error occurs with more
> ##D ## than two levels
> ##D sensitivity(model, iris$Species)
> ##D 
> ##D ## When passing a table, more than two levels can
> ##D ## be used
> ##D sensitivity(irisTabs, "versicolor")
> ##D specificity(irisTabs, c("setosa", "virginica"))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("spatialSign")
> ### * spatialSign
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: spatialSign
> ### Title: Compute the multivariate spatial sign
> ### Aliases: spatialSign spatialSign.default spatialSign.matrix
> ###   spatialSign.data.frame
> ### Keywords: manip
> 
> ### ** Examples
> 
> spatialSign(rnorm(5))
[1] -0.32230088  0.09448167 -0.42991811  0.82074751  0.16952670
> 
> spatialSign(matrix(rnorm(12), ncol = 3))
           [,1]       [,2]        [,3]
[1,] -0.3445240 -0.1282361 -0.92997779
[2,]  0.2504238  0.7766995  0.57794961
[3,]  0.8830219  0.4662449 -0.05373972
[4,]  0.6796389 -0.7332979 -0.01911061
> 
> # should fail since the fifth column is a factor
> try(spatialSign(iris), silent = TRUE)
> 
> spatialSign(iris[,-5])
    Sepal.Length Sepal.Width Petal.Length Petal.Width
1      0.8037728   0.5516088    0.2206435  0.03152050
2      0.8281329   0.5070201    0.2366094  0.03380134
3      0.8053331   0.5483119    0.2227517  0.03426949
4      0.8000302   0.5391508    0.2608794  0.03478392
5      0.7909650   0.5694948    0.2214702  0.03163860
6      0.7841750   0.5663486    0.2468699  0.05808704
7      0.7801094   0.5766026    0.2374246  0.05087670
8      0.8021849   0.5454857    0.2406555  0.03208740
9      0.8064237   0.5315065    0.2565893  0.03665562
10     0.8180312   0.5175299    0.2504177  0.01669451
11     0.8037352   0.5507074    0.2232598  0.02976797
12     0.7869910   0.5574520    0.2623303  0.03279129
13     0.8230722   0.5144201    0.2400627  0.01714734
14     0.8025126   0.5598925    0.2052939  0.01866308
15     0.8112086   0.5594542    0.1678363  0.02797271
16     0.7738111   0.5973279    0.2036345  0.05430253
17     0.7942894   0.5736535    0.1912178  0.05883625
18     0.8032741   0.5512666    0.2205066  0.04725142
19     0.8068282   0.5378855    0.2406330  0.04246464
20     0.7796488   0.5809148    0.2293085  0.04586170
21     0.8173379   0.5146202    0.2573101  0.03027177
22     0.7859186   0.5701762    0.2311525  0.06164067
23     0.7757707   0.6071249    0.1686458  0.03372916
24     0.8059779   0.5215151    0.2686593  0.07901744
25     0.7761140   0.5497474    0.3072118  0.03233808
26     0.8264745   0.4958847    0.2644718  0.03305898
27     0.7977821   0.5424918    0.2552903  0.06382256
28     0.8064196   0.5427825    0.2326211  0.03101614
29     0.8160943   0.5336001    0.2197177  0.03138824
30     0.7952406   0.5414404    0.2707202  0.03384003
31     0.8084658   0.5221342    0.2694886  0.03368608
32     0.8222503   0.5177131    0.2284029  0.06090743
33     0.7657831   0.6037905    0.2208990  0.01472660
34     0.7786745   0.5946241    0.1982080  0.02831544
35     0.8176894   0.5173137    0.2503131  0.03337508
36     0.8251230   0.5280787    0.1980295  0.03300492
37     0.8269975   0.5262712    0.1954721  0.03007264
38     0.7852322   0.5769053    0.2243521  0.01602515
39     0.8021241   0.5469028    0.2369912  0.03646019
40     0.8077957   0.5385305    0.2375870  0.03167826
41     0.8003330   0.5602331    0.2080866  0.04801998
42     0.8609386   0.4400353    0.2487156  0.05739590
43     0.7860904   0.5717021    0.2322540  0.03573138
44     0.7888948   0.5522264    0.2524463  0.09466737
45     0.7669390   0.5714447    0.2857224  0.06015208
46     0.8221058   0.5138162    0.2397809  0.05138162
47     0.7772909   0.5791579    0.2438560  0.03048200
48     0.7959478   0.5537028    0.2422450  0.03460643
49     0.7983702   0.5573528    0.2259538  0.03012718
50     0.8122836   0.5361072    0.2274394  0.03249135
51     0.7670110   0.3506336    0.5149931  0.15340221
52     0.7454976   0.3727488    0.5241780  0.17472599
53     0.7551929   0.3392895    0.5362964  0.16417236
54     0.7538492   0.3152460    0.5482539  0.17818253
55     0.7581754   0.3265986    0.5365549  0.17496355
56     0.7223296   0.3548286    0.5702602  0.16474184
57     0.7263485   0.3804682    0.5418790  0.18446945
58     0.7591655   0.3718361    0.5112747  0.15493173
59     0.7630185   0.3352657    0.5318008  0.15029153
60     0.7246023   0.3762358    0.5434518  0.19508524
61     0.7692308   0.3076923    0.5384615  0.15384615
62     0.7392346   0.3758820    0.5262348  0.18794100
63     0.7889275   0.2892734    0.5259517  0.13148792
64     0.7308141   0.3474362    0.5630863  0.16772783
65     0.7591171   0.3931142    0.4880038  0.17622361
66     0.7694544   0.3560162    0.5053134  0.16078153
67     0.7063189   0.3783851    0.5675777  0.18919257
68     0.7567650   0.3522871    0.5349545  0.13047672
69     0.7644424   0.2712537    0.5548372  0.18494574
70     0.7618519   0.3401124    0.5305754  0.14964948
71     0.6985796   0.3788906    0.5683359  0.21312598
72     0.7701185   0.3534970    0.5049958  0.16412362
73     0.7414331   0.2942195    0.5766702  0.17653168
74     0.7365989   0.3381110    0.5675435  0.14490471
75     0.7674170   0.3477358    0.5156083  0.15588157
76     0.7678573   0.3490260    0.5119048  0.16287881
77     0.7646727   0.3148652    0.5397690  0.15743261
78     0.7408858   0.3317399    0.5528998  0.18798594
79     0.7335095   0.3545296    0.5501321  0.18337737
80     0.7866747   0.3588341    0.4830459  0.13801311
81     0.7652185   0.3339135    0.5286965  0.15304371
82     0.7724292   0.3370600    0.5196342  0.14044168
83     0.7643498   0.3558180    0.5139594  0.15814134
84     0.7077953   0.3185079    0.6016260  0.18874540
85     0.6933341   0.3851856    0.5777784  0.19259280
86     0.7152494   0.4053080    0.5364370  0.19073316
87     0.7545734   0.3491310    0.5293276  0.16893434
88     0.7753002   0.2830461    0.5414795  0.15998258
89     0.7299244   0.3910309    0.5344090  0.16944674
90     0.7471419   0.3396100    0.5433760  0.17659719
91     0.7233712   0.3419573    0.5786969  0.15782644
92     0.7326039   0.3602970    0.5524554  0.16813860
93     0.7626299   0.3418686    0.5259517  0.15778550
94     0.7698688   0.3541396    0.5081134  0.15397376
95     0.7354428   0.3545885    0.5515821  0.17072780
96     0.7323962   0.3854717    0.5396603  0.15418867
97     0.7344605   0.3736729    0.5411814  0.16750853
98     0.7572810   0.3542121    0.5252110  0.15878473
99     0.7825805   0.3836179    0.4603415  0.16879188
100    0.7431482   0.3650553    0.5345452  0.16948994
101    0.6538775   0.3425072    0.6227404  0.25947519
102    0.6905251   0.3214514    0.6071859  0.22620651
103    0.7149140   0.3020764    0.5940835  0.21145345
104    0.6927680   0.3188932    0.6157937  0.19793370
105    0.6861902   0.3167032    0.6122928  0.23224900
106    0.7095371   0.2800804    0.6161769  0.19605630
107    0.6705412   0.3421128    0.6158031  0.23263673
108    0.7136656   0.2835110    0.6159032  0.17597233
109    0.7141413   0.2664706    0.6182118  0.19185884
110    0.6919879   0.3459939    0.5862675  0.24027357
111    0.7156264   0.3523084    0.5614915  0.22019275
112    0.7157655   0.3019636    0.5927433  0.21249287
113    0.7171815   0.3164036    0.5800733  0.22148252
114    0.6925518   0.3037508    0.6075016  0.24300063
115    0.6776792   0.3271555    0.5958904  0.28041899
116    0.6958989   0.3479494    0.5762913  0.25008866
117    0.7061047   0.3258945    0.5974732  0.19553670
118    0.6929910   0.3419956    0.6029922  0.19799743
119    0.7060062   0.2383917    0.6326549  0.21088496
120    0.7271258   0.2666128    0.6059382  0.18178146
121    0.7055893   0.3272298    0.5828782  0.23519645
122    0.6830792   0.3415396    0.5976943  0.24395687
123    0.7148654   0.2599511    0.6220258  0.18567933
124    0.7312246   0.3133820    0.5687303  0.20892133
125    0.6959560   0.3427843    0.5920820  0.21813547
126    0.7152945   0.3179087    0.5960788  0.17882363
127    0.7278520   0.3287073    0.5634983  0.21131186
128    0.7117121   0.3500224    0.5717032  0.21001342
129    0.6959400   0.3044738    0.6089475  0.22835532
130    0.7308986   0.3045411    0.5887794  0.16242190
131    0.7276616   0.2753314    0.5998292  0.18683203
132    0.7157900   0.3443040    0.5798805  0.18121266
133    0.6941775   0.3037026    0.6074053  0.23862350
134    0.7236600   0.3216267    0.5858200  0.17230001
135    0.6938541   0.2957411    0.6369808  0.15924521
136    0.7315440   0.2850171    0.5795348  0.21851314
137    0.6701748   0.3616817    0.5957110  0.25530470
138    0.6980480   0.3381170    0.5998850  0.19632600
139    0.7106691   0.3553345    0.5685352  0.21320072
140    0.7241526   0.3253439    0.5667281  0.22039426
141    0.6999704   0.3238669    0.5850499  0.25073566
142    0.7333789   0.3294891    0.5420626  0.24445962
143    0.6905251   0.3214514    0.6071859  0.22620651
144    0.6919350   0.3256165    0.6003554  0.23403685
145    0.6891487   0.3394315    0.5862907  0.25714504
146    0.7215572   0.3230853    0.5600146  0.24769876
147    0.7296536   0.2895451    0.5790902  0.22005426
148    0.7165390   0.3307103    0.5732312  0.22047353
149    0.6746707   0.3699807    0.5876164  0.25028107
150    0.6902592   0.3509792    0.5966647  0.21058754
> 
> trellis.par.set(caretTheme())
> featurePlot(iris[,-5], iris[,5], "pairs")
> featurePlot(spatialSign(scale(iris[,-5])), iris[,5], "pairs")
> 
> 
> 
> cleanEx()
> nameEx("summary.bagEarth")
> ### * summary.bagEarth
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.bagEarth
> ### Title: Summarize a bagged earth or FDA fit
> ### Aliases: summary.bagEarth summary.bagFDA
> ### Keywords: manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(trees)
> ##D fit <- bagEarth(trees[,-3], trees[3])
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("tecator")
> ### * tecator
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tecator
> ### Title: Fat, Water and Protein Content of Meat Samples
> ### Aliases: tecator absorp endpoints
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(tecator)
> 
> splom(~endpoints)
> 
> # plot 10 random spectra
> set.seed(1)
> inSubset <- sample(1:dim(endpoints)[1], 10)
> 
> absorpSubset <- absorp[inSubset,]
> endpointSubset <- endpoints[inSubset, 3]
> 
> newOrder <- order(absorpSubset[,1])
> absorpSubset <- absorpSubset[newOrder,]
> endpointSubset <- endpointSubset[newOrder]
> 
> plotColors <- rainbow(10)
> 
> plot(absorpSubset[1,], 
+      type = "n", 
+      ylim = range(absorpSubset), 
+      xlim = c(0, 105),
+      xlab = "Wavelength Index", 
+      ylab = "Absorption")
>    
> for(i in 1:10)
+ {
+    points(absorpSubset[i,], type = "l", col = plotColors[i], lwd = 2)
+    text(105, absorpSubset[i,100], endpointSubset[i], col = plotColors[i])
+ }
> title("Predictor Profiles for 10 Random Samples")
> 
> 
> 
> cleanEx()
> nameEx("train")
> ### * train
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: train
> ### Title: Fit Predictive Models over Different Tuning Parameters
> ### Aliases: train train.default train.formula
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D 
> ##D #######################################
> ##D ## Classification Example
> ##D 
> ##D data(iris)
> ##D TrainData <- iris[,1:4]
> ##D TrainClasses <- iris[,5]
> ##D 
> ##D knnFit1 <- train(TrainData, TrainClasses,
> ##D                  method = "knn",
> ##D                  preProcess = c("center", "scale"),
> ##D                  tuneLength = 10,
> ##D                  trControl = trainControl(method = "cv"))
> ##D 
> ##D knnFit2 <- train(TrainData, TrainClasses,
> ##D                  method = "knn",
> ##D                  preProcess = c("center", "scale"),
> ##D                  tuneLength = 10, 
> ##D                  trControl = trainControl(method = "boot"))
> ##D 
> ##D 
> ##D library(MASS)
> ##D nnetFit <- train(TrainData, TrainClasses,
> ##D                  method = "nnet",
> ##D                  preProcess = "range", 
> ##D                  tuneLength = 2,
> ##D                  trace = FALSE,
> ##D                  maxit = 100)
> ##D 
> ##D #######################################
> ##D ## Regression Example
> ##D 
> ##D library(mlbench)
> ##D data(BostonHousing)
> ##D 
> ##D lmFit <- train(medv ~ . + rm:lstat,
> ##D                data = BostonHousing, 
> ##D                method = "lm")
> ##D 
> ##D library(rpart)
> ##D rpartFit <- train(medv ~ .,
> ##D                   data = BostonHousing,
> ##D                   method = "rpart",
> ##D                   tuneLength = 9)
> ##D 
> ##D #######################################
> ##D ## Example with a custom metric
> ##D 
> ##D madSummary <- function (data,
> ##D                         lev = NULL,
> ##D                         model = NULL) {
> ##D   out <- mad(data$obs - data$pred, 
> ##D              na.rm = TRUE)  
> ##D   names(out) <- "MAD"
> ##D   out
> ##D }
> ##D 
> ##D robustControl <- trainControl(summaryFunction = madSummary)
> ##D marsGrid <- expand.grid(degree = 1, nprune = (1:10) * 2)
> ##D 
> ##D earthFit <- train(medv ~ .,
> ##D                   data = BostonHousing, 
> ##D                   method = "earth",
> ##D                   tuneGrid = marsGrid,
> ##D                   metric = "MAD",
> ##D                   maximize = FALSE,
> ##D                   trControl = robustControl)
> ##D 
> ##D #######################################
> ##D ## Parallel Processing Example via multicore package
> ##D 
> ##D ## library(doMC)
> ##D ## registerDoMC(2)
> ##D 
> ##D ## NOTE: don't run models form RWeka when using
> ##D ### multicore. The session will crash.
> ##D 
> ##D ## The code for train() does not change:
> ##D set.seed(1)
> ##D usingMC <-  train(medv ~ .,
> ##D                   data = BostonHousing, 
> ##D                   method = "glmboost")
> ##D 
> ##D ## or use:
> ##D ## library(doMPI) or 
> ##D ## library(doParallel) or 
> ##D ## library(doSMP) and so on
> ##D 
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("trainControl")
> ### * trainControl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: trainControl
> ### Title: Control parameters for train
> ### Aliases: trainControl
> ### Keywords: utilities
> 
> ### ** Examples
> 
> ## Not run: 
> ##D 
> ##D ## Do 5 repeats of 10-Fold CV for the iris data. We will fit
> ##D ## a KNN model that evaluates 12 values of k and set the seed
> ##D ## at each iteration.
> ##D 
> ##D set.seed(123)
> ##D seeds <- vector(mode = "list", length = 51)
> ##D for(i in 1:50) seeds[[i]] <- sample.int(1000, 22)
> ##D 
> ##D ## For the last model:
> ##D seeds[[51]] <- sample.int(1000, 1)
> ##D 
> ##D ctrl <- trainControl(method = "repeatedcv",
> ##D                      repeats = 5,
> ##D                      seeds = seeds)
> ##D 
> ##D set.seed(1)
> ##D mod <- train(Species ~ ., data = iris,
> ##D              method = "knn",
> ##D              tuneLength = 12,
> ##D              trControl = ctrl)
> ##D 
> ##D 
> ##D ctrl2 <- trainControl(method = "adaptive_cv",
> ##D                       repeats = 5,
> ##D                       verboseIter = TRUE,
> ##D                       seeds = seeds)
> ##D 
> ##D set.seed(1)
> ##D mod2 <- train(Species ~ ., data = iris,
> ##D               method = "knn",
> ##D               tuneLength = 12,
> ##D               trControl = ctrl2)
> ##D 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("twoClassSim")
> ### * twoClassSim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: twoClassSim
> ### Title: Simulation Functions
> ### Aliases: twoClassSim SLC14_1 SLC14_2 LPH07_1 LPH07_2
> ### Keywords: models
> 
> ### ** Examples
> 
> example <- twoClassSim(100, linearVars = 1)
> splom(~example[, 1:6], groups = example$Class)
> 
> 
> 
> cleanEx()
> nameEx("update.safs")
> ### * update.safs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: update.safs
> ### Title: Update or Re-fit a SA or GA Model
> ### Aliases: update.safs update.gafs
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1)
> ##D train_data <- twoClassSim(100, noiseVars = 10)
> ##D test_data  <- twoClassSim(10,  noiseVars = 10)
> ##D 
> ##D ## A short example 
> ##D ctrl <- safsControl(functions = rfSA, 
> ##D                     method = "cv",
> ##D                     number = 3)
> ##D 
> ##D rf_search <- safs(x = train_data[, -ncol(train_data)],
> ##D                   y = train_data$Class,
> ##D                   iters = 3,
> ##D                   safsControl = ctrl)
> ##D 
> ##D rf_search2 <- update(rf_search, 
> ##D 	                 iter = 1,
> ##D 	                 x = train_data[, -ncol(train_data)],
> ##D                      y = train_data$Class)
> ##D rf_search2
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("update.train")
> ### * update.train
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: update.train
> ### Title: Update or Re-fit a Model
> ### Aliases: update.train
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(iris)
> ##D TrainData <- iris[,1:4]
> ##D TrainClasses <- iris[,5]
> ##D 
> ##D knnFit1 <- train(TrainData, TrainClasses,
> ##D                  method = "knn",
> ##D                  preProcess = c("center", "scale"),
> ##D                  tuneLength = 10,
> ##D                  trControl = trainControl(method = "cv"))
> ##D 
> ##D update(knnFit1, list(.k = 3))
> ## End(Not run)
> 
> 
> 
> ### * <FOOTER>
> ###
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  10.545 0.324 10.898 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
