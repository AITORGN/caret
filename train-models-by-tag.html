<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>The caret Package</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Documentation for the <code>caret</code> package">
  <meta name="generator" content="bookdown 0.1.7 and GitBook 2.6.7">

  <meta property="og:title" content="The caret Package" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Documentation for the <code>caret</code> package" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="The caret Package" />
  
  <meta name="twitter:description" content="Documentation for the <code>caret</code> package" />
  

<meta name="author" content="Max Kuhn">

<meta name="date" content="2016-11-10">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="available-models.html">
<link rel="next" href="models-clustered-by-tag-similarity.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.6/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.1/datatables.js"></script>
<script src="libs/datatables-1.10.7/jquery.dataTables.min.js"></script>
<link href="libs/datatables-bootstrap-1.10.7/dataTables.bootstrap.css" rel="stylesheet" />
<link href="libs/datatables-bootstrap-1.10.7/dataTables.extra.css" rel="stylesheet" />
<script src="libs/datatables-bootstrap-1.10.7/dataTables.bootstrap.min.js"></script>
<script src="libs/d3-3.5.2/d3.min.js"></script>
<script src="libs/forceNetwork-binding-0.2.13/forceNetwork.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="visualizations.html"><a href="visualizations.html"><i class="fa fa-check"></i><b>2</b> Visualizations</a></li>
<li class="chapter" data-level="3" data-path="pre-processing.html"><a href="pre-processing.html"><i class="fa fa-check"></i><b>3</b> Pre-Processing</a><ul>
<li class="chapter" data-level="3.1" data-path="pre-processing.html"><a href="pre-processing.html#creating-dummy-variables"><i class="fa fa-check"></i><b>3.1</b> Creating Dummy Variables</a></li>
<li class="chapter" data-level="3.2" data-path="pre-processing.html"><a href="pre-processing.html#zero--and-near-zero-variance-predictors"><i class="fa fa-check"></i><b>3.2</b> Zero- and Near Zero-Variance Predictors</a></li>
<li class="chapter" data-level="3.3" data-path="pre-processing.html"><a href="pre-processing.html#identifying-correlated-predictors"><i class="fa fa-check"></i><b>3.3</b> Identifying Correlated Predictors</a></li>
<li class="chapter" data-level="3.4" data-path="pre-processing.html"><a href="pre-processing.html#linear-dependencies"><i class="fa fa-check"></i><b>3.4</b> Linear Dependencies</a></li>
<li class="chapter" data-level="3.5" data-path="pre-processing.html"><a href="pre-processing.html#the-preprocess-function"><i class="fa fa-check"></i><b>3.5</b> The <code>preProcess</code> Function</a></li>
<li class="chapter" data-level="3.6" data-path="pre-processing.html"><a href="pre-processing.html#centering-and-scaling"><i class="fa fa-check"></i><b>3.6</b> Centering and Scaling</a></li>
<li class="chapter" data-level="3.7" data-path="pre-processing.html"><a href="pre-processing.html#imputation"><i class="fa fa-check"></i><b>3.7</b> Imputation</a></li>
<li class="chapter" data-level="3.8" data-path="pre-processing.html"><a href="pre-processing.html#transforming-predictors"><i class="fa fa-check"></i><b>3.8</b> Transforming Predictors</a></li>
<li class="chapter" data-level="3.9" data-path="pre-processing.html"><a href="pre-processing.html#putting-it-all-together"><i class="fa fa-check"></i><b>3.9</b> Putting It All Together</a></li>
<li class="chapter" data-level="3.10" data-path="pre-processing.html"><a href="pre-processing.html#class-distance-calculations"><i class="fa fa-check"></i><b>3.10</b> Class Distance Calculations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-splitting.html"><a href="data-splitting.html"><i class="fa fa-check"></i><b>4</b> Data Splitting</a><ul>
<li class="chapter" data-level="4.1" data-path="data-splitting.html"><a href="data-splitting.html#simple-splitting-based-on-the-outcome"><i class="fa fa-check"></i><b>4.1</b> Simple Splitting Based on the Outcome</a></li>
<li class="chapter" data-level="4.2" data-path="data-splitting.html"><a href="data-splitting.html#splitting-based-on-the-predictors"><i class="fa fa-check"></i><b>4.2</b> Splitting Based on the Predictors</a></li>
<li class="chapter" data-level="4.3" data-path="data-splitting.html"><a href="data-splitting.html#data-splitting-for-time-series"><i class="fa fa-check"></i><b>4.3</b> Data Splitting for Time Series</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html"><i class="fa fa-check"></i><b>5</b> Model Training and Tuning</a><ul>
<li class="chapter" data-level="5.1" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#model-training-and-parameter-tuning"><i class="fa fa-check"></i><b>5.1</b> Model Training and Parameter Tuning</a></li>
<li class="chapter" data-level="5.2" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#an-example"><i class="fa fa-check"></i><b>5.2</b> An Example</a></li>
<li class="chapter" data-level="5.3" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#basic-parameter-tuning"><i class="fa fa-check"></i><b>5.3</b> Basic Parameter Tuning</a></li>
<li class="chapter" data-level="5.4" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#notes-on-reproducibility"><i class="fa fa-check"></i><b>5.4</b> Notes on Reproducibility</a></li>
<li class="chapter" data-level="5.5" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#customizing-the-tuning-process"><i class="fa fa-check"></i><b>5.5</b> Customizing the Tuning Process</a><ul>
<li class="chapter" data-level="5.5.1" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#pre-processing-options"><i class="fa fa-check"></i><b>5.5.1</b> Pre-Processing Options</a></li>
<li class="chapter" data-level="5.5.2" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#alternate-tuning-grids"><i class="fa fa-check"></i><b>5.5.2</b> Alternate Tuning Grids</a></li>
<li class="chapter" data-level="5.5.3" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#plotting-the-resampling-profile"><i class="fa fa-check"></i><b>5.5.3</b> Plotting the Resampling Profile</a></li>
<li class="chapter" data-level="5.5.4" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#the-traincontrol-function"><i class="fa fa-check"></i><b>5.5.4</b> The <code>trainControl</code> Function</a></li>
<li class="chapter" data-level="5.5.5" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#alternate-performance-metrics"><i class="fa fa-check"></i><b>5.5.5</b> Alternate Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#choosing-the-final-model"><i class="fa fa-check"></i><b>5.6</b> Choosing the Final Model</a></li>
<li class="chapter" data-level="5.7" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#extracting-predictions-and-class-probabilities"><i class="fa fa-check"></i><b>5.7</b> Extracting Predictions and Class Probabilities</a></li>
<li class="chapter" data-level="5.8" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#exploring-and-comparing-resampling-distributions"><i class="fa fa-check"></i><b>5.8</b> Exploring and Comparing Resampling Distributions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#within-model"><i class="fa fa-check"></i><b>5.8.1</b> Within-Model</a></li>
<li class="chapter" data-level="5.8.2" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#between-models"><i class="fa fa-check"></i><b>5.8.2</b> Between-Models</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#fitting-models-without-parameter-tuning"><i class="fa fa-check"></i><b>5.9</b> Fitting Models Without Parameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="available-models.html"><a href="available-models.html"><i class="fa fa-check"></i><b>6</b> Available Models</a></li>
<li class="chapter" data-level="7" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html"><i class="fa fa-check"></i><b>7</b> <code>train</code> Models By Tag</a><ul>
<li class="chapter" data-level="7.0.1" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#accepts-case-weights"><i class="fa fa-check"></i><b>7.0.1</b> Accepts Case Weights</a></li>
<li class="chapter" data-level="7.0.2" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#bagging"><i class="fa fa-check"></i><b>7.0.2</b> Bagging</a></li>
<li class="chapter" data-level="7.0.3" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#bayesian-model"><i class="fa fa-check"></i><b>7.0.3</b> Bayesian Model</a></li>
<li class="chapter" data-level="7.0.4" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#binary-predictors-only"><i class="fa fa-check"></i><b>7.0.4</b> Binary Predictors Only</a></li>
<li class="chapter" data-level="7.0.5" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#boosting"><i class="fa fa-check"></i><b>7.0.5</b> Boosting</a></li>
<li class="chapter" data-level="7.0.6" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#categorical-predictors-only"><i class="fa fa-check"></i><b>7.0.6</b> Categorical Predictors Only</a></li>
<li class="chapter" data-level="7.0.7" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#cost-sensitive-learning"><i class="fa fa-check"></i><b>7.0.7</b> Cost Sensitive Learning</a></li>
<li class="chapter" data-level="7.0.8" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#discriminant-analysis"><i class="fa fa-check"></i><b>7.0.8</b> Discriminant Analysis</a></li>
<li class="chapter" data-level="7.0.9" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#distance-weighted-discrimination"><i class="fa fa-check"></i><b>7.0.9</b> Distance Weighted Discrimination</a></li>
<li class="chapter" data-level="7.0.10" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#ensemble-model"><i class="fa fa-check"></i><b>7.0.10</b> Ensemble Model</a></li>
<li class="chapter" data-level="7.0.11" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#feature-extraction"><i class="fa fa-check"></i><b>7.0.11</b> Feature Extraction</a></li>
<li class="chapter" data-level="7.0.12" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#feature-selection-wrapper"><i class="fa fa-check"></i><b>7.0.12</b> Feature Selection Wrapper</a></li>
<li class="chapter" data-level="7.0.13" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#gaussian-process"><i class="fa fa-check"></i><b>7.0.13</b> Gaussian Process</a></li>
<li class="chapter" data-level="7.0.14" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#generalized-additive-model"><i class="fa fa-check"></i><b>7.0.14</b> Generalized Additive Model</a></li>
<li class="chapter" data-level="7.0.15" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#generalized-linear-model"><i class="fa fa-check"></i><b>7.0.15</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="7.0.16" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#handle-missing-predictor-data"><i class="fa fa-check"></i><b>7.0.16</b> Handle Missing Predictor Data</a></li>
<li class="chapter" data-level="7.0.17" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#implicit-feature-selection"><i class="fa fa-check"></i><b>7.0.17</b> Implicit Feature Selection</a></li>
<li class="chapter" data-level="7.0.18" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#kernel-method"><i class="fa fa-check"></i><b>7.0.18</b> Kernel Method</a></li>
<li class="chapter" data-level="7.0.19" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#l1-regularization"><i class="fa fa-check"></i><b>7.0.19</b> L1 Regularization</a></li>
<li class="chapter" data-level="7.0.20" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#l2-regularization"><i class="fa fa-check"></i><b>7.0.20</b> L2 Regularization</a></li>
<li class="chapter" data-level="7.0.21" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#linear-classifier"><i class="fa fa-check"></i><b>7.0.21</b> Linear Classifier</a></li>
<li class="chapter" data-level="7.0.22" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#linear-regression"><i class="fa fa-check"></i><b>7.0.22</b> Linear Regression</a></li>
<li class="chapter" data-level="7.0.23" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#logic-regression"><i class="fa fa-check"></i><b>7.0.23</b> Logic Regression</a></li>
<li class="chapter" data-level="7.0.24" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#logistic-regression"><i class="fa fa-check"></i><b>7.0.24</b> Logistic Regression</a></li>
<li class="chapter" data-level="7.0.25" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#mixture-model"><i class="fa fa-check"></i><b>7.0.25</b> Mixture Model</a></li>
<li class="chapter" data-level="7.0.26" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#model-tree"><i class="fa fa-check"></i><b>7.0.26</b> Model Tree</a></li>
<li class="chapter" data-level="7.0.27" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>7.0.27</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="7.0.28" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#neural-network"><i class="fa fa-check"></i><b>7.0.28</b> Neural Network</a></li>
<li class="chapter" data-level="7.0.29" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#oblique-tree"><i class="fa fa-check"></i><b>7.0.29</b> Oblique Tree</a></li>
<li class="chapter" data-level="7.0.30" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#ordinal-outcomes"><i class="fa fa-check"></i><b>7.0.30</b> Ordinal Outcomes</a></li>
<li class="chapter" data-level="7.0.31" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#partial-least-squares"><i class="fa fa-check"></i><b>7.0.31</b> Partial Least Squares</a></li>
<li class="chapter" data-level="7.0.32" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#polynomial-model"><i class="fa fa-check"></i><b>7.0.32</b> Polynomial Model</a></li>
<li class="chapter" data-level="7.0.33" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#prototype-models"><i class="fa fa-check"></i><b>7.0.33</b> Prototype Models</a></li>
<li class="chapter" data-level="7.0.34" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#quantile-regression"><i class="fa fa-check"></i><b>7.0.34</b> Quantile Regression</a></li>
<li class="chapter" data-level="7.0.35" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#radial-basis-function"><i class="fa fa-check"></i><b>7.0.35</b> Radial Basis Function</a></li>
<li class="chapter" data-level="7.0.36" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#random-forest"><i class="fa fa-check"></i><b>7.0.36</b> Random Forest</a></li>
<li class="chapter" data-level="7.0.37" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#regularization"><i class="fa fa-check"></i><b>7.0.37</b> Regularization</a></li>
<li class="chapter" data-level="7.0.38" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#relevance-vector-machines"><i class="fa fa-check"></i><b>7.0.38</b> Relevance Vector Machines</a></li>
<li class="chapter" data-level="7.0.39" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#ridge-regression"><i class="fa fa-check"></i><b>7.0.39</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.0.40" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#robust-methods"><i class="fa fa-check"></i><b>7.0.40</b> Robust Methods</a></li>
<li class="chapter" data-level="7.0.41" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#robust-model"><i class="fa fa-check"></i><b>7.0.41</b> Robust Model</a></li>
<li class="chapter" data-level="7.0.42" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#roc-curves"><i class="fa fa-check"></i><b>7.0.42</b> ROC Curves</a></li>
<li class="chapter" data-level="7.0.43" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#rule-based-model"><i class="fa fa-check"></i><b>7.0.43</b> Rule-Based Model</a></li>
<li class="chapter" data-level="7.0.44" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#self-organising-maps"><i class="fa fa-check"></i><b>7.0.44</b> Self-Organising Maps</a></li>
<li class="chapter" data-level="7.0.45" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#string-kernel"><i class="fa fa-check"></i><b>7.0.45</b> String Kernel</a></li>
<li class="chapter" data-level="7.0.46" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#support-vector-machines"><i class="fa fa-check"></i><b>7.0.46</b> Support Vector Machines</a></li>
<li class="chapter" data-level="7.0.47" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#text-mining"><i class="fa fa-check"></i><b>7.0.47</b> Text Mining</a></li>
<li class="chapter" data-level="7.0.48" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#tree-based-model"><i class="fa fa-check"></i><b>7.0.48</b> Tree-Based Model</a></li>
<li class="chapter" data-level="7.0.49" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#two-class-only"><i class="fa fa-check"></i><b>7.0.49</b> Two Class Only</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="models-clustered-by-tag-similarity.html"><a href="models-clustered-by-tag-similarity.html"><i class="fa fa-check"></i><b>8</b> Models Clustered by Tag Similarity</a></li>
<li class="chapter" data-level="9" data-path="parallel-processing.html"><a href="parallel-processing.html"><i class="fa fa-check"></i><b>9</b> Parallel Processing</a></li>
<li class="chapter" data-level="10" data-path="random-hyperparameter-search.html"><a href="random-hyperparameter-search.html"><i class="fa fa-check"></i><b>10</b> Random Hyperparameter Search</a></li>
<li class="chapter" data-level="11" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html"><i class="fa fa-check"></i><b>11</b> Subsampling For Class Imbalances</a><ul>
<li class="chapter" data-level="11.1" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#subsampling-techniques"><i class="fa fa-check"></i><b>11.1</b> Subsampling Techniques</a></li>
<li class="chapter" data-level="11.2" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#subsampling-during-resampling"><i class="fa fa-check"></i><b>11.2</b> Subsampling During Resampling</a></li>
<li class="chapter" data-level="11.3" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#complications"><i class="fa fa-check"></i><b>11.3</b> Complications</a></li>
<li class="chapter" data-level="11.4" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#using-custom-subsampling-techniques"><i class="fa fa-check"></i><b>11.4</b> Using Custom Subsampling Techniques</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html"><i class="fa fa-check"></i><b>12</b> Using Your Own Model in <code>train</code></a><ul>
<li class="chapter" data-level="12.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#introduction-1"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-1-svms-with-laplacian-kernels"><i class="fa fa-check"></i><b>12.2</b> Illustrative Example 1: SVMs with Laplacian Kernels</a></li>
<li class="chapter" data-level="12.3" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#model-components"><i class="fa fa-check"></i><b>12.3</b> Model Components</a><ul>
<li class="chapter" data-level="12.3.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-parameters-element"><i class="fa fa-check"></i><b>12.3.1</b> The parameters Element</a></li>
<li class="chapter" data-level="12.3.2" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-grid-element"><i class="fa fa-check"></i><b>12.3.2</b> The <code>grid</code> Element</a></li>
<li class="chapter" data-level="12.3.3" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-fit-element"><i class="fa fa-check"></i><b>12.3.3</b> The <code>fit</code> Element</a></li>
<li class="chapter" data-level="12.3.4" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-predict-element"><i class="fa fa-check"></i><b>12.3.4</b> The <code>predict</code> Element</a></li>
<li class="chapter" data-level="12.3.5" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-prob-element"><i class="fa fa-check"></i><b>12.3.5</b> The <code>prob</code> Element</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-sort-element"><i class="fa fa-check"></i><b>12.4</b> The sort Element</a><ul>
<li class="chapter" data-level="12.4.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-levels-element"><i class="fa fa-check"></i><b>12.4.1</b> The <code>levels</code> Element</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-2-something-more-complicated---logitboost"><i class="fa fa-check"></i><b>12.5</b> Illustrative Example 2: Something More Complicated - <code>LogitBoost</code></a><ul>
<li class="chapter" data-level="12.5.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-loop-element"><i class="fa fa-check"></i><b>12.5.1</b> The loop Element</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-3-nonstandard-formulas"><i class="fa fa-check"></i><b>12.6</b> Illustrative Example 3: Nonstandard Formulas</a></li>
<li class="chapter" data-level="12.7" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-4-pls-feature-extraction-pre-processing"><i class="fa fa-check"></i><b>12.7</b> Illustrative Example 4: PLS Feature Extraction Pre-Processing</a></li>
<li class="chapter" data-level="12.8" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-5-optimizing-probability-thresholds-for-class-imbalances"><i class="fa fa-check"></i><b>12.8</b> Illustrative Example 5: Optimizing probability thresholds for class imbalances</a></li>
<li class="chapter" data-level="12.9" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-6-offsets-in-generalized-linear-models"><i class="fa fa-check"></i><b>12.9</b> Illustrative Example 6: Offsets in Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="adaptive-resampling.html"><a href="adaptive-resampling.html"><i class="fa fa-check"></i><b>13</b> Adaptive Resampling</a></li>
<li class="chapter" data-level="14" data-path="variable-importance.html"><a href="variable-importance.html"><i class="fa fa-check"></i><b>14</b> Variable Importance</a><ul>
<li class="chapter" data-level="14.1" data-path="variable-importance.html"><a href="variable-importance.html#model-specific-metrics"><i class="fa fa-check"></i><b>14.1</b> Model Specific Metrics</a></li>
<li class="chapter" data-level="14.2" data-path="variable-importance.html"><a href="variable-importance.html#model-independent-metrics"><i class="fa fa-check"></i><b>14.2</b> Model Independent Metrics</a></li>
<li class="chapter" data-level="14.3" data-path="variable-importance.html"><a href="variable-importance.html#an-example-1"><i class="fa fa-check"></i><b>14.3</b> An Example</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html"><i class="fa fa-check"></i><b>15</b> Miscellaneous Model Functions</a><ul>
<li class="chapter" data-level="15.1" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#yet-another-k-nearest-neighbor-function"><i class="fa fa-check"></i><b>15.1</b> Yet Another <em>k</em>-Nearest Neighbor Function</a></li>
<li class="chapter" data-level="15.2" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#partial-least-squares-discriminant-analysis"><i class="fa fa-check"></i><b>15.2</b> Partial Least Squares Discriminant Analysis</a></li>
<li class="chapter" data-level="15.3" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#bagged-mars-and-fda"><i class="fa fa-check"></i><b>15.3</b> Bagged MARS and FDA</a></li>
<li class="chapter" data-level="15.4" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#bagging-1"><i class="fa fa-check"></i><b>15.4</b> Bagging</a><ul>
<li class="chapter" data-level="15.4.1" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#the-fit-function"><i class="fa fa-check"></i><b>15.4.1</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="15.4.2" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#the-pred-function"><i class="fa fa-check"></i><b>15.4.2</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="15.4.3" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#the-aggregate-function"><i class="fa fa-check"></i><b>15.4.3</b> The <code>aggregate</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#model-averaged-neural-networks"><i class="fa fa-check"></i><b>15.5</b> Model Averaged Neural Networks</a></li>
<li class="chapter" data-level="15.6" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#neural-networks-with-a-principal-component-step"><i class="fa fa-check"></i><b>15.6</b> Neural Networks with a Principal Component Step</a></li>
<li class="chapter" data-level="15.7" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#independent-component-regression"><i class="fa fa-check"></i><b>15.7</b> Independent Component Regression</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>16</b> Measuring Performance</a><ul>
<li class="chapter" data-level="16.1" data-path="measuring-performance.html"><a href="measuring-performance.html#measures-for-regression"><i class="fa fa-check"></i><b>16.1</b> Measures for Regression</a></li>
<li class="chapter" data-level="16.2" data-path="measuring-performance.html"><a href="measuring-performance.html#measures-for-predicted-classes"><i class="fa fa-check"></i><b>16.2</b> Measures for Predicted Classes</a></li>
<li class="chapter" data-level="16.3" data-path="measuring-performance.html"><a href="measuring-performance.html#measures-for-class-probabilities"><i class="fa fa-check"></i><b>16.3</b> Measures for Class Probabilities</a></li>
<li class="chapter" data-level="16.4" data-path="measuring-performance.html"><a href="measuring-performance.html#lift-curves"><i class="fa fa-check"></i><b>16.4</b> Lift Curves</a></li>
<li class="chapter" data-level="16.5" data-path="measuring-performance.html"><a href="measuring-performance.html#calibration-curves"><i class="fa fa-check"></i><b>16.5</b> Calibration Curves</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html"><i class="fa fa-check"></i><b>17</b> Feature Selection Overview</a><ul>
<li class="chapter" data-level="17.1" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html#models-with-built-in-feature-selection"><i class="fa fa-check"></i><b>17.1</b> Models with Built-In Feature Selection</a></li>
<li class="chapter" data-level="17.2" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html#feature-selection-methods"><i class="fa fa-check"></i><b>17.2</b> Feature Selection Methods</a></li>
<li class="chapter" data-level="17.3" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html#external-validation"><i class="fa fa-check"></i><b>17.3</b> External Validation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html"><i class="fa fa-check"></i><b>18</b> Recursive Feature Elimination</a><ul>
<li class="chapter" data-level="18.1" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#backwards-selection"><i class="fa fa-check"></i><b>18.1</b> Backwards Selection</a></li>
<li class="chapter" data-level="18.2" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#resampling-and-external-validation"><i class="fa fa-check"></i><b>18.2</b> Resampling and External Validation</a></li>
<li class="chapter" data-level="18.3" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#recursive-feature-elimination-via-caret"><i class="fa fa-check"></i><b>18.3</b> Recursive Feature Elimination via <a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a></a></li>
<li class="chapter" data-level="18.4" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#an-example-2"><i class="fa fa-check"></i><b>18.4</b> An Example</a></li>
<li class="chapter" data-level="18.5" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#helper-functions"><i class="fa fa-check"></i><b>18.5</b> Helper Functions</a><ul>
<li class="chapter" data-level="18.5.1" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-summary-function"><i class="fa fa-check"></i><b>18.5.1</b> The <code>summary</code> Function</a></li>
<li class="chapter" data-level="18.5.2" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-fit-function-1"><i class="fa fa-check"></i><b>18.5.2</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="18.5.3" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-pred-function-1"><i class="fa fa-check"></i><b>18.5.3</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="18.5.4" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-rank-function"><i class="fa fa-check"></i><b>18.5.4</b> The <code>rank</code> Function</a></li>
<li class="chapter" data-level="18.5.5" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-selectsize-function"><i class="fa fa-check"></i><b>18.5.5</b> The <code>selectSize</code> Function</a></li>
<li class="chapter" data-level="18.5.6" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-selectvar-function"><i class="fa fa-check"></i><b>18.5.6</b> The <code>selectVar</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-example"><i class="fa fa-check"></i><b>18.6</b> The Example</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html"><i class="fa fa-check"></i><b>19</b> Feature Selection using Univariate Filters</a><ul>
<li class="chapter" data-level="19.1" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#univariate-filters"><i class="fa fa-check"></i><b>19.1</b> Univariate Filters</a></li>
<li class="chapter" data-level="19.2" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#basic-syntax"><i class="fa fa-check"></i><b>19.2</b> Basic Syntax</a><ul>
<li class="chapter" data-level="19.2.1" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-score-function"><i class="fa fa-check"></i><b>19.2.1</b> The <code>score</code> Function</a></li>
<li class="chapter" data-level="19.2.2" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-filter-function"><i class="fa fa-check"></i><b>19.2.2</b> The <code>filter</code> Function</a></li>
<li class="chapter" data-level="19.2.3" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-fit-function-2"><i class="fa fa-check"></i><b>19.2.3</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="19.2.4" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-summary-and-pred-functions"><i class="fa fa-check"></i><b>19.2.4</b> The <code>summary</code> and <code>pred</code> Functions</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-example-1"><i class="fa fa-check"></i><b>19.3</b> The Example</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html"><i class="fa fa-check"></i><b>20</b> Feature Selection using Genetic Algorithms</a><ul>
<li class="chapter" data-level="20.1" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>20.1</b> Genetic Algorithms</a></li>
<li class="chapter" data-level="20.2" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#internal-and-external-performance-estimates"><i class="fa fa-check"></i><b>20.2</b> Internal and External Performance Estimates</a></li>
<li class="chapter" data-level="20.3" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#basic-syntax-1"><i class="fa fa-check"></i><b>20.3</b> Basic Syntax</a></li>
<li class="chapter" data-level="20.4" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#example"><i class="fa fa-check"></i><b>20.4</b> Example</a></li>
<li class="chapter" data-level="20.5" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#customizing-the-search"><i class="fa fa-check"></i><b>20.5</b> Customizing the Search</a><ul>
<li class="chapter" data-level="20.5.1" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-fit-function-3"><i class="fa fa-check"></i><b>20.5.1</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="20.5.2" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-pred-function-2"><i class="fa fa-check"></i><b>20.5.2</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="20.5.3" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-fitness_intern-function"><i class="fa fa-check"></i><b>20.5.3</b> The <code>fitness_intern</code> Function</a></li>
<li class="chapter" data-level="20.5.4" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-fitness_extern-function"><i class="fa fa-check"></i><b>20.5.4</b> The <code>fitness_extern</code> Function</a></li>
<li class="chapter" data-level="20.5.5" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-initial-function"><i class="fa fa-check"></i><b>20.5.5</b> The <code>initial</code> Function</a></li>
<li class="chapter" data-level="20.5.6" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-selection-function"><i class="fa fa-check"></i><b>20.5.6</b> The <code>selection</code> Function</a></li>
<li class="chapter" data-level="20.5.7" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-crossover-function"><i class="fa fa-check"></i><b>20.5.7</b> The <code>crossover</code> Function</a></li>
<li class="chapter" data-level="20.5.8" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-mutation-function"><i class="fa fa-check"></i><b>20.5.8</b> The <code>mutation</code> Function</a></li>
<li class="chapter" data-level="20.5.9" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-selectiter-function"><i class="fa fa-check"></i><b>20.5.9</b> The <code>selectIter</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="20.6" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-example-revisited"><i class="fa fa-check"></i><b>20.6</b> The Example Revisited</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html"><i class="fa fa-check"></i><b>21</b> Feature Selection using Simulated Annealing</a><ul>
<li class="chapter" data-level="21.1" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#simulated-annealing"><i class="fa fa-check"></i><b>21.1</b> Simulated Annealing</a></li>
<li class="chapter" data-level="21.2" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#internal-and-external-performance-estimates-1"><i class="fa fa-check"></i><b>21.2</b> Internal and External Performance Estimates</a></li>
<li class="chapter" data-level="21.3" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#basic-syntax-2"><i class="fa fa-check"></i><b>21.3</b> Basic Syntax</a></li>
<li class="chapter" data-level="21.4" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#example-1"><i class="fa fa-check"></i><b>21.4</b> Example</a></li>
<li class="chapter" data-level="21.5" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#customizing-the-search-1"><i class="fa fa-check"></i><b>21.5</b> Customizing the Search</a><ul>
<li class="chapter" data-level="21.5.1" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-fit-function-4"><i class="fa fa-check"></i><b>21.5.1</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="21.5.2" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-pred-function-3"><i class="fa fa-check"></i><b>21.5.2</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="21.5.3" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-fitness_intern-function-1"><i class="fa fa-check"></i><b>21.5.3</b> The <code>fitness_intern</code> Function</a></li>
<li class="chapter" data-level="21.5.4" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-fitness_extern-function-1"><i class="fa fa-check"></i><b>21.5.4</b> The <code>fitness_extern</code> Function</a></li>
<li class="chapter" data-level="21.5.5" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-initial-function-1"><i class="fa fa-check"></i><b>21.5.5</b> The <code>initial</code> Function</a></li>
<li class="chapter" data-level="21.5.6" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-perturb-function"><i class="fa fa-check"></i><b>21.5.6</b> The <code>perturb</code> Function</a></li>
<li class="chapter" data-level="21.5.7" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-prob-function"><i class="fa fa-check"></i><b>21.5.7</b> The <code>prob</code> Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="data-sets.html"><a href="data-sets.html"><i class="fa fa-check"></i><b>22</b> Data Sets</a><ul>
<li class="chapter" data-level="22.1" data-path="data-sets.html"><a href="data-sets.html#blood-brain-barrier-data"><i class="fa fa-check"></i><b>22.1</b> Blood-Brain Barrier Data</a></li>
<li class="chapter" data-level="22.2" data-path="data-sets.html"><a href="data-sets.html#cox-2-activity-data"><i class="fa fa-check"></i><b>22.2</b> COX-2 Activity Data</a></li>
<li class="chapter" data-level="22.3" data-path="data-sets.html"><a href="data-sets.html#dhfr-inhibition"><i class="fa fa-check"></i><b>22.3</b> DHFR Inhibition</a></li>
<li class="chapter" data-level="22.4" data-path="data-sets.html"><a href="data-sets.html#tecator-nir-data"><i class="fa fa-check"></i><b>22.4</b> Tecator NIR Data</a></li>
<li class="chapter" data-level="22.5" data-path="data-sets.html"><a href="data-sets.html#fatty-acid-composition-data"><i class="fa fa-check"></i><b>22.5</b> Fatty Acid Composition Data</a></li>
<li class="chapter" data-level="22.6" data-path="data-sets.html"><a href="data-sets.html#german-credit-data"><i class="fa fa-check"></i><b>22.6</b> German Credit Data</a></li>
<li class="chapter" data-level="22.7" data-path="data-sets.html"><a href="data-sets.html#kelly-blue-book"><i class="fa fa-check"></i><b>22.7</b> Kelly Blue Book</a></li>
<li class="chapter" data-level="22.8" data-path="data-sets.html"><a href="data-sets.html#cell-body-segmentation-data"><i class="fa fa-check"></i><b>22.8</b> Cell Body Segmentation Data</a></li>
<li class="chapter" data-level="22.9" data-path="data-sets.html"><a href="data-sets.html#sacramento-house-price-data"><i class="fa fa-check"></i><b>22.9</b> Sacramento House Price Data</a></li>
<li class="chapter" data-level="22.10" data-path="data-sets.html"><a href="data-sets.html#animal-scat-data"><i class="fa fa-check"></i><b>22.10</b> Animal Scat Data</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="session-information.html"><a href="session-information.html"><i class="fa fa-check"></i><b>23</b> Session Information</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The <code>caret</code> Package</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="train-models-by-tag" class="section level1">
<h1><span class="header-section-number">7</span> <code>train</code> Models By Tag</h1>
<p>The following is a basic list of model types or relevant characteristics. There entires in these lists are arguable. For example: random forests theoretically use feature selection but effectively may not, support vector machines use L2 regularization etc.</p>
<div id="top">

</div>
<p>Contents</p>
<ul>
<li><a href="train-models-by-tag.html#Accepts_Case_Weights">Accepts Case Weights</a></li>
<li><a href="train-models-by-tag.html#Bagging">Bagging</a></li>
<li><a href="train-models-by-tag.html#Bayesian_Model">Bayesian Model</a></li>
<li><a href="train-models-by-tag.html#Binary_Predictors_Only">Binary Predictors Only</a></li>
<li><a href="train-models-by-tag.html#Boosting">Boosting</a></li>
<li><a href="train-models-by-tag.html#Categorical_Predictors_Only">Categorical Predictors Only</a></li>
<li><a href="train-models-by-tag.html#Cost_Sensitive_Learning">Cost Sensitive Learning</a></li>
<li><a href="train-models-by-tag.html#Discriminant_Analysis">Discriminant Analysis</a></li>
<li><a href="train-models-by-tag.html#Distance_Weighted_Discrimination">Distance Weighted Discrimination</a></li>
<li><a href="train-models-by-tag.html#Ensemble_Model">Ensemble Model</a></li>
<li><a href="train-models-by-tag.html#Feature_Extraction">Feature Extraction</a></li>
<li><a href="train-models-by-tag.html#Feature_Selection_Wrapper">Feature Selection Wrapper</a></li>
<li><a href="train-models-by-tag.html#Gaussian_Process">Gaussian Process</a></li>
<li><a href="train-models-by-tag.html#Generalized_Additive_Model">Generalized Additive Model</a></li>
<li><a href="train-models-by-tag.html#Generalized_Linear_Model">Generalized Linear Model</a></li>
<li><a href="train-models-by-tag.html#Handle_Missing_Predictor_Data">Handle Missing Predictor Data</a></li>
<li><a href="train-models-by-tag.html#Implicit_Feature_Selection">Implicit Feature Selection</a></li>
<li><a href="train-models-by-tag.html#Kernel_Method">Kernel Method</a></li>
<li><a href="train-models-by-tag.html#L1_Regularization">L1 Regularization</a></li>
<li><a href="train-models-by-tag.html#L2_Regularization">L2 Regularization</a></li>
<li><a href="train-models-by-tag.html#Linear_Classifier">Linear Classifier</a></li>
<li><a href="train-models-by-tag.html#Linear_Regression">Linear Regression</a></li>
<li><a href="train-models-by-tag.html#Logic_Regression">Logic Regression</a></li>
<li><a href="train-models-by-tag.html#Logistic_Regression">Logistic Regression</a></li>
<li><a href="train-models-by-tag.html#Mixture_Model">Mixture Model</a></li>
<li><a href="train-models-by-tag.html#Model_Tree">Model Tree</a></li>
<li><a href="train-models-by-tag.html#Multivariate_Adaptive_Regression_Splines">Multivariate Adaptive Regression Splines</a></li>
<li><a href="train-models-by-tag.html#Neural_Network">Neural Network</a></li>
<li><a href="train-models-by-tag.html#Oblique_Tree">Oblique Tree</a></li>
<li><a href="train-models-by-tag.html#Ordinal_Outcomes">Ordinal Outcomes</a></li>
<li><a href="train-models-by-tag.html#Partial_Least_Squares">Partial Least Squares</a></li>
<li><a href="train-models-by-tag.html#Polynomial_Model">Polynomial Model</a></li>
<li><a href="train-models-by-tag.html#Prototype_Models">Prototype Models</a></li>
<li><a href="train-models-by-tag.html#Quantile_Regression">Quantile Regression</a></li>
<li><a href="train-models-by-tag.html#Radial_Basis_Function">Radial Basis Function</a></li>
<li><a href="train-models-by-tag.html#Random_Forest">Random Forest</a></li>
<li><a href="train-models-by-tag.html#Regularization">Regularization</a></li>
<li><a href="train-models-by-tag.html#Relevance_Vector_Machines">Relevance Vector Machines</a></li>
<li><a href="train-models-by-tag.html#Ridge_Regression">Ridge Regression</a></li>
<li><a href="train-models-by-tag.html#Robust_Methods">Robust Methods</a></li>
<li><a href="train-models-by-tag.html#Robust_Model">Robust Model</a></li>
<li><a href="train-models-by-tag.html#ROC_Curves">ROC Curves</a></li>
<li><a href="train-models-by-tag.html#Rule_Based_Model">Rule-Based Model</a></li>
<li><a href="train-models-by-tag.html#Self_Organising_Maps">Self-Organising Maps</a></li>
<li><a href="train-models-by-tag.html#String_Kernel">String Kernel</a></li>
<li><a href="train-models-by-tag.html#Support_Vector_Machines">Support Vector Machines</a></li>
<li><a href="train-models-by-tag.html#Text_Mining">Text Mining</a></li>
<li><a href="train-models-by-tag.html#Tree_Based_Model">Tree-Based Model</a></li>
<li><a href="train-models-by-tag.html#Two_Class_Only">Two Class Only</a></li>
</ul>
<div id="Accepts_Case_Weights">

</div>
<div id="accepts-case-weights" class="section level3">
<h3><span class="header-section-number">7.0.1</span> Accepts Case Weights</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmAdjCat&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Bagged CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;treebag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code>, <code>plyr</code>, <code>e1071</code></p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p><strong>Bagged MARS</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blackboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>party</code>, <code>mboost</code>, <code>plyr</code></p>
<p><strong>C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart1SE&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rpart</code></p>
<p>Notes: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>CART or Ordinal Responses</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p><strong>CHi-squared Automated Interaction Detection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;chaid&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha2</code> (Merging Threshold)</li>
<li><code>alpha3</code> (Splitting former Merged Threshold)</li>
<li><code>alpha4</code> ( Splitting former Merged Threshold)</li>
</ul>
<p>Required packages: <code>CHAID</code></p>
<p><strong>Conditional Inference Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Continuation Ratio Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmContRatio&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cost-Sensitive CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>Cumulative Probability Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmCumulative&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>DeepBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>Flexible Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;fda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p><strong>Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lm&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
</ul>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lmStepAIC&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Model Averaged Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Multivariate Adaptive Regression Spline</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;earth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Multivariate Adaptive Regression Splines</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gcvEarth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Negative Binomial Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm.nb&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>link</code> (Link Function)</li>
</ul>
<p><strong>Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nnet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Neural Networks with Feature Extraction</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcaNNet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Ordered Logistic or Probit Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;polr&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>method</code> (parameter)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Shrinkage Penalty Coefficient)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Projection Pursuit Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ppr&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nterms</code> (# Terms)</li>
</ul>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code></p>
<p><strong>Robust Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlm&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
<li><code>psi</code> (psi)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Single C5.0 Ruleset</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Rules&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p><strong>Single C5.0 Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Tree&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p><strong>Tree Models from Genetic Algorithms</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;evtree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>evtree</code></p>
<div id="Bagging">

</div>
</div>
<div id="bagging" class="section level3">
<h3><span class="header-section-number">7.0.2</span> Bagging</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged AdaBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Bagged CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;treebag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code>, <code>plyr</code>, <code>e1071</code></p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p><strong>Bagged Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p><strong>Bagged MARS</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Bagged Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>caret</code></p>
<p><strong>Conditional Inference Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Ensembles of Generalized Lienar Models</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;randomGLM&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxInteractionOrder</code> (Interaction Order)</li>
</ul>
<p>Required packages: <code>randomGLM</code></p>
<p><strong>Model Averaged Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Parallel Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;parRF&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>randomForest</code>, <code>foreach</code></p>
<p><strong>Quantile Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Random Ferns</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rFerns&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>depth</code> (Fern Depth)</li>
</ul>
<p>Required packages: <code>rFerns</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Rborist&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>predFixed</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>Rborist</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rf&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>randomForest</code></p>
<p><strong>Random Forest by Randomization</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;extraTrees&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (# Randomly Selected Predictors)</li>
<li><code>numRandomCuts</code> (# Random Cuts)</li>
</ul>
<p>Required packages: <code>extraTrees</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p><strong>Weighted Subspace Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;wsrf&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>wsrf</code></p>
<div id="Bayesian_Model">

</div>
</div>
<div id="bayesian-model" class="section level3">
<h3><span class="header-section-number">7.0.3</span> Bayesian Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Additive Regression Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bartMachine&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_trees</code> (#Trees)</li>
<li><code>k</code> (Prior Boundary)</li>
<li><code>alpha</code> (Base Terminal Node Hyperparameter)</li>
<li><code>beta</code> (Power Terminal Node Hyperparameter)</li>
<li><code>nu</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>bartMachine</code></p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Bayesian Regularized Neural Networks</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;brnn&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>neurons</code> (# Neurons)</li>
</ul>
<p>Required packages: <code>brnn</code></p>
<p><strong>Bayesian Ridge Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bridge&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p><strong>Bayesian Ridge Regression (Model Averaged)</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blassoAveraged&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors.</p>
<p><strong>Model Averaged Naive Bayes Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;manb&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>prior</code> (Prior Probability)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Naive Bayes</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nb&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fL</code> (Laplace Correction)</li>
<li><code>usekernel</code> (Distribution Type)</li>
<li><code>adjust</code> (Bandwidth Adjustment)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Naive Bayes Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbDiscrete&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Naive Bayes Classifier with Attribute Weighting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awnb&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Semi-Naive Structure Learner Wrapper</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbSearch&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Spike and Slab Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spikeslab&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (Variables Retained)</li>
</ul>
<p>Required packages: <code>spikeslab</code>, <code>plyr</code></p>
<p><strong>The Bayesian lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sparsity</code> (Sparsity Threshold)</li>
</ul>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter <code>sparsity</code>. For example, when <code>sparsity = .5</code>, only coefficients where at least half the posterior estimates are nonzero are used.</p>
<p><strong>Tree Augmented Naive Bayes Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tan&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier Structure Learner Wrapper</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tanSearch&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>sp</code> (Super-Parent)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier with Attribute Weighting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awtan&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Variational Bayesian Multinomial Probit Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vbmpRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimateTheta</code> (Theta Estimated)</li>
</ul>
<p>Required packages: <code>vbmp</code></p>
<div id="Binary_Predictors_Only">

</div>
</div>
<div id="binary-predictors-only" class="section level3">
<h3><span class="header-section-number">7.0.4</span> Binary Predictors Only</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p><strong>Binary Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;binda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda.freqs</code> (Shrinkage Intensity)</li>
</ul>
<p>Required packages: <code>binda</code></p>
<p><strong>Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<div id="Boosting">

</div>
</div>
<div id="boosting" class="section level3">
<h3><span class="header-section-number">7.0.5</span> Boosting</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>AdaBoost.M1</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Bagged AdaBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Boosted Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;BstLm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Boosted Smoothing Spline</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstSm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blackboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>party</code>, <code>mboost</code>, <code>plyr</code></p>
<p><strong>Boosted Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstTree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cubist</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p><strong>DeepBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>alpha</code> (L1 Regularization)</li>
<li><code>eta</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>xgboost</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ntrees</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>min_rows</code> (Min. Terminal Node Size)</li>
<li><code>learn_rate</code> (Shrinkage)</li>
<li><code>col_sample_rate</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<div id="Categorical_Predictors_Only">

</div>
</div>
<div id="categorical-predictors-only" class="section level3">
<h3><span class="header-section-number">7.0.6</span> Categorical Predictors Only</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Model Averaged Naive Bayes Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;manb&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>prior</code> (Prior Probability)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Naive Bayes Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbDiscrete&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Naive Bayes Classifier with Attribute Weighting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awnb&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Semi-Naive Structure Learner Wrapper</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbSearch&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tan&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier Structure Learner Wrapper</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tanSearch&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>sp</code> (Super-Parent)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier with Attribute Weighting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awtan&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<div id="Cost_Sensitive_Learning">

</div>
</div>
<div id="cost-sensitive-learning" class="section level3">
<h3><span class="header-section-number">7.0.7</span> Cost Sensitive Learning</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cost-Sensitive CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Discriminant_Analysis">

</div>
</div>
<div id="discriminant-analysis" class="section level3">
<h3><span class="header-section-number">7.0.8</span> Discriminant Analysis</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adaptive Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;amdai&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>adaptDA</code></p>
<p><strong>Binary Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;binda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda.freqs</code> (Shrinkage Intensity)</li>
</ul>
<p>Required packages: <code>binda</code></p>
<p><strong>Diagonal Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model)</li>
<li><code>shrinkage</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Factor-Based Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RFlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>q</code> (# Factors)</li>
</ul>
<p>Required packages: <code>HiDimDA</code></p>
<p><strong>Heteroscedastic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>newdim</code> (Dimension of the Discriminative Subspace)</li>
</ul>
<p>Required packages: <code>hda</code></p>
<p><strong>High Dimensional Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Threshold)</li>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>HDclassif</code></p>
<p><strong>High-Dimensional Regularized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdrda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>shrinkage_type</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>dimen</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Localized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;loclda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Nearest Neighbors)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Maximum Uncertainty Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Mlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>HiDimDA</code></p>
<p><strong>Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>subclasses</code> (#Subclasses Per Class)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Shrinkage Penalty Coefficient)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PenalizedLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>K</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>penalizedLDA</code>, <code>plyr</code></p>
<p><strong>Quadratic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Quadratic Discriminant Analysis with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepQDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Regularized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Regularized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimator</code> (Regularization Method)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Robust Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Linda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rmda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Subclasses Per Class)</li>
<li><code>model</code> (Model)</li>
</ul>
<p>Required packages: <code>robustDA</code></p>
<p><strong>Robust Quadratic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;QdaCov&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Regularized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rrlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>hp</code> (Robustness Parameter)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rrlda</code></p>
<p><strong>Shrinkage Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>diagonal</code> (Diagonalize)</li>
<li><code>lambda</code> (shrinkage)</li>
</ul>
<p>Required packages: <code>sda</code></p>
<p><strong>Sparse Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sparseLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;smda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>R</code> (# Subclasses)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Stabilized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;slda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code></p>
<p><strong>Stepwise Diagonal Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sddaLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>SDDA</code></p>
<p><strong>Stepwise Diagonal Quadratic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sddaQDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>SDDA</code></p>
<div id="Distance_Weighted_Discrimination">

</div>
</div>
<div id="distance-weighted-discrimination" class="section level3">
<h3><span class="header-section-number">7.0.9</span> Distance Weighted Discrimination</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<div id="Ensemble_Model">

</div>
</div>
<div id="ensemble-model" class="section level3">
<h3><span class="header-section-number">7.0.10</span> Ensemble Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>AdaBoost.M1</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Bagged AdaBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Bagged CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;treebag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code>, <code>plyr</code>, <code>e1071</code></p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p><strong>Bagged Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p><strong>Bagged MARS</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Bagged Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>caret</code></p>
<p><strong>Boosted Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;BstLm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Boosted Smoothing Spline</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstSm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blackboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>party</code>, <code>mboost</code>, <code>plyr</code></p>
<p><strong>Boosted Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstTree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Conditional Inference Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cubist</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p><strong>DeepBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>Ensemble Partial Least Squares Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enpls&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxcomp</code> (Max. #Components)</li>
</ul>
<p>Required packages: <code>enpls</code></p>
<p><strong>Ensemble Partial Least Squares Regression with Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enpls.fs&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxcomp</code> (Max. #Components)</li>
<li><code>threshold</code> (Importance Cutoff)</li>
</ul>
<p>Required packages: <code>enpls</code></p>
<p><strong>Ensembles of Generalized Lienar Models</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;randomGLM&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxInteractionOrder</code> (Interaction Order)</li>
</ul>
<p>Required packages: <code>randomGLM</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>alpha</code> (L1 Regularization)</li>
<li><code>eta</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>xgboost</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ntrees</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>min_rows</code> (Min. Terminal Node Size)</li>
<li><code>learn_rate</code> (Shrinkage)</li>
<li><code>col_sample_rate</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>Model Averaged Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Parallel Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;parRF&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>randomForest</code>, <code>foreach</code></p>
<p><strong>Quantile Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Random Ferns</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rFerns&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>depth</code> (Fern Depth)</li>
</ul>
<p>Required packages: <code>rFerns</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Rborist&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>predFixed</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>Rborist</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rf&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>randomForest</code></p>
<p><strong>Random Forest by Randomization</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;extraTrees&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (# Randomly Selected Predictors)</li>
<li><code>numRandomCuts</code> (# Random Cuts)</li>
</ul>
<p>Required packages: <code>extraTrees</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p><strong>Random Forest with Additional Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Boruta&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>Boruta</code>, <code>randomForest</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p><strong>Rotation Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForest&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
</ul>
<p>Required packages: <code>rotationForest</code></p>
<p><strong>Rotation Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForestCp&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code>, <code>rotationForest</code></p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p><strong>Tree-Based Ensembles</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nodeHarvest&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxinter</code> (Maximum Interaction Depth)</li>
<li><code>mode</code> (Prediction Mode)</li>
</ul>
<p>Required packages: <code>nodeHarvest</code></p>
<p><strong>Weighted Subspace Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;wsrf&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>wsrf</code></p>
<div id="Feature_Extraction">

</div>
</div>
<div id="feature-extraction" class="section level3">
<h3><span class="header-section-number">7.0.11</span> Feature Extraction</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Independent Component Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;icr&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.comp</code> (#Components)</li>
</ul>
<p>Required packages: <code>fastICA</code></p>
<p><strong>Neural Networks with Feature Extraction</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcaNNet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;simpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;widekernelpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Principal Component Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcr&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Projection Pursuit Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ppr&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nterms</code> (# Terms)</li>
</ul>
<p><strong>Sparse Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<p><strong>Supervised Principal Component Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;superpc&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Threshold)</li>
<li><code>n.components</code> (#Components)</li>
</ul>
<p>Required packages: <code>superpc</code></p>
<div id="Feature_Selection_Wrapper">

</div>
</div>
<div id="feature-selection-wrapper" class="section level3">
<h3><span class="header-section-number">7.0.12</span> Feature Selection Wrapper</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Linear Regression with Backwards Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapBackward&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Forward Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapForward&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapSeq&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lmStepAIC&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Quadratic Discriminant Analysis with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepQDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Random Forest with Additional Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Boruta&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>Boruta</code>, <code>randomForest</code></p>
<p><strong>Ridge Regression with Variable Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;foba&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Variables Retained)</li>
<li><code>lambda</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>foba</code></p>
<p><strong>Stepwise Diagonal Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sddaLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>SDDA</code></p>
<p><strong>Stepwise Diagonal Quadratic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sddaQDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>SDDA</code></p>
<div id="Gaussian_Process">

</div>
</div>
<div id="gaussian-process" class="section level3">
<h3><span class="header-section-number">7.0.13</span> Gaussian Process</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Gaussian Process</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprPoly&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprRadial&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Variational Bayesian Multinomial Probit Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vbmpRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimateTheta</code> (Theta Estimated)</li>
</ul>
<p>Required packages: <code>vbmp</code></p>
<div id="Generalized_Additive_Model">

</div>
</div>
<div id="generalized-additive-model" class="section level3">
<h3><span class="header-section-number">7.0.14</span> Generalized Additive Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Generalized Additive Model using LOESS</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamLoess&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>span</code> (Span)</li>
<li><code>degree</code> (Degree)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bam&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gam&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamSpline&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion.</p>
<div id="Generalized_Linear_Model">

</div>
</div>
<div id="generalized-linear-model" class="section level3">
<h3><span class="header-section-number">7.0.15</span> Generalized Linear Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Ensembles of Generalized Lienar Models</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;randomGLM&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxInteractionOrder</code> (Interaction Order)</li>
</ul>
<p>Required packages: <code>randomGLM</code></p>
<p><strong>Generalized Additive Model using LOESS</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamLoess&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>span</code> (Span)</li>
<li><code>degree</code> (Degree)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bam&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gam&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamSpline&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion.</p>
<p><strong>Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p><strong>Negative Binomial Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm.nb&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>link</code> (Link Function)</li>
</ul>
<p><strong>Penalized Ordinal Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<div id="Handle_Missing_Predictor_Data">

</div>
</div>
<div id="handle-missing-predictor-data" class="section level3">
<h3><span class="header-section-number">7.0.16</span> Handle Missing Predictor Data</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost.M1</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Bagged AdaBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Boosted Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart1SE&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rpart</code></p>
<p>Notes: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>CART or Ordinal Responses</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cost-Sensitive CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>Single C5.0 Ruleset</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Rules&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p><strong>Single C5.0 Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Tree&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<div id="Implicit_Feature_Selection">

</div>
</div>
<div id="implicit-feature-selection" class="section level3">
<h3><span class="header-section-number">7.0.17</span> Implicit Feature Selection</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>AdaBoost.M1</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Bagged AdaBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p><strong>Bagged MARS</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Bayesian Additive Regression Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bartMachine&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_trees</code> (#Trees)</li>
<li><code>k</code> (Prior Boundary)</li>
<li><code>alpha</code> (Base Terminal Node Hyperparameter)</li>
<li><code>beta</code> (Power Terminal Node Hyperparameter)</li>
<li><code>nu</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>bartMachine</code></p>
<p><strong>Boosted Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;BstLm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Boosted Smoothing Spline</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstSm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>C4.5-like Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;J48&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Confidence Threshold)</li>
<li><code>M</code> (Minimum Instances Per Leaf)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart1SE&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rpart</code></p>
<p>Notes: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>CART or Ordinal Responses</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p><strong>CHi-squared Automated Interaction Detection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;chaid&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha2</code> (Merging Threshold)</li>
<li><code>alpha3</code> (Splitting former Merged Threshold)</li>
<li><code>alpha4</code> ( Splitting former Merged Threshold)</li>
</ul>
<p>Required packages: <code>CHAID</code></p>
<p><strong>Conditional Inference Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cost-Sensitive CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>Cubist</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p><strong>DeepBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>Elasticnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enet&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>alpha</code> (L1 Regularization)</li>
<li><code>eta</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>xgboost</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p><strong>Flexible Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;fda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ntrees</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>min_rows</code> (Min. Terminal Node Size)</li>
<li><code>learn_rate</code> (Shrinkage)</li>
<li><code>col_sample_rate</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p><strong>Least Angle Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Least Angle Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars2&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>step</code> (#Steps)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Logistic Model Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LMT&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (# Iteratons)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Rules</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5Rules&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Multivariate Adaptive Regression Spline</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;earth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Multivariate Adaptive Regression Splines</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gcvEarth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Nearest Shrunken Centroids</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pam&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Shrinkage Threshold)</li>
</ul>
<p>Required packages: <code>pamr</code></p>
<p><strong>Non-Convex Penalized Quantile Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqnc&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;oblique.tree&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>oblique.splits</code> (Oblique Splits)</li>
<li><code>variable.selection</code> (Variable Selection Method)</li>
</ul>
<p>Required packages: <code>oblique.tree</code></p>
<p><strong>Parallel Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;parRF&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>randomForest</code>, <code>foreach</code></p>
<p><strong>Penalized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PenalizedLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>K</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>penalizedLDA</code>, <code>plyr</code></p>
<p><strong>Penalized Linear Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;penalized&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda1</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>penalized</code></p>
<p><strong>Penalized Ordinal Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p><strong>Quantile Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression with LASSO penalty</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqlasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Random Ferns</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rFerns&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>depth</code> (Fern Depth)</li>
</ul>
<p>Required packages: <code>rFerns</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Rborist&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>predFixed</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>Rborist</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rf&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>randomForest</code></p>
<p><strong>Random Forest by Randomization</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;extraTrees&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (# Randomly Selected Predictors)</li>
<li><code>numRandomCuts</code> (# Random Cuts)</li>
</ul>
<p>Required packages: <code>extraTrees</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p><strong>Relaxed Lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;relaxo&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>phi</code> (Relaxation Parameter)</li>
</ul>
<p>Required packages: <code>relaxo</code>, <code>plyr</code></p>
<p><strong>Rotation Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForest&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
</ul>
<p>Required packages: <code>rotationForest</code></p>
<p><strong>Rotation Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForestCp&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code>, <code>rotationForest</code></p>
<p><strong>Rule-Based Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;JRip&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumOpt</code> (# Optimizations)</li>
<li><code>NumFolds</code> (# Folds)</li>
<li><code>MinWeights</code> (Min Weights)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Rule-Based Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PART&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Confidence Threshold)</li>
<li><code>pruned</code> (Pruning)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Single C5.0 Ruleset</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Rules&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p><strong>Single C5.0 Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Tree&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p><strong>Single Rule Classification</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;OneR&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<p><strong>Sparse Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sparseLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;smda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>R</code> (# Subclasses)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Spike and Slab Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spikeslab&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (Variables Retained)</li>
</ul>
<p>Required packages: <code>spikeslab</code>, <code>plyr</code></p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p><strong>The Bayesian lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sparsity</code> (Sparsity Threshold)</li>
</ul>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter <code>sparsity</code>. For example, when <code>sparsity = .5</code>, only coefficients where at least half the posterior estimates are nonzero are used.</p>
<p><strong>The lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>Tree Models from Genetic Algorithms</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;evtree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>evtree</code></p>
<p><strong>Tree-Based Ensembles</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nodeHarvest&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxinter</code> (Maximum Interaction Depth)</li>
<li><code>mode</code> (Prediction Mode)</li>
</ul>
<p>Required packages: <code>nodeHarvest</code></p>
<p><strong>Weighted Subspace Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;wsrf&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>wsrf</code></p>
<div id="Kernel_Method">

</div>
</div>
<div id="kernel-method" class="section level3">
<h3><span class="header-section-number">7.0.18</span> Kernel Method</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Gaussian Process</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprPoly&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprRadial&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Least Squares Support Vector Machine</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmLinear&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmPoly&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Polynomial Kernel Regularized Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsPoly&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>KRLS</code></p>
<p><strong>Radial Basis Function Kernel Regularized Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsRadial&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>KRLS</code>, <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmLinear&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmPoly&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>scale</code> (Scale)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmRadial&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmPoly&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadial&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialCost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialSigma&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p>Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="L1_Regularization">

</div>
</div>
<div id="l1-regularization" class="section level3">
<h3><span class="header-section-number">7.0.19</span> L1 Regularization</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Ridge Regression (Model Averaged)</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blassoAveraged&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors.</p>
<p><strong>DeepBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>Elasticnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enet&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p><strong>Least Angle Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Least Angle Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars2&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>step</code> (#Steps)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Non-Convex Penalized Quantile Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqnc&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Penalized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PenalizedLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>K</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>penalizedLDA</code>, <code>plyr</code></p>
<p><strong>Penalized Linear Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;penalized&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda1</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>penalized</code></p>
<p><strong>Penalized Ordinal Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p><strong>Quantile Regression with LASSO penalty</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqlasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Relaxed Lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;relaxo&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>phi</code> (Relaxation Parameter)</li>
</ul>
<p>Required packages: <code>relaxo</code>, <code>plyr</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<p><strong>Sparse Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sparseLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;smda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>R</code> (# Subclasses)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<p><strong>The Bayesian lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sparsity</code> (Sparsity Threshold)</li>
</ul>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter <code>sparsity</code>. For example, when <code>sparsity = .5</code>, only coefficients where at least half the posterior estimates are nonzero are used.</p>
<p><strong>The lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<div id="L2_Regularization">

</div>
</div>
<div id="l2-regularization" class="section level3">
<h3><span class="header-section-number">7.0.20</span> L2 Regularization</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Ridge Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bridge&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Model Averaged Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Multi-Layer Perceptron</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecay&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron, multiple layers</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecayML&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units layer1)</li>
<li><code>layer2</code> (#Hidden Units layer2)</li>
<li><code>layer3</code> (#Hidden Units layer3)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multilayer Perceptron Network by Stochastic Gradient Descent</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpSGD&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>l2reg</code> (L2 Regularization)</li>
<li><code>lambda</code> (RMSE Gradient Scaling)</li>
<li><code>learn_rate</code> (Learning Rate)</li>
<li><code>momentum</code> (Momentum)</li>
<li><code>gamma</code> (Decay)</li>
<li><code>minibatchsz</code> (Batch Size)</li>
<li><code>repeats</code> (#Models)</li>
</ul>
<p>Required packages: <code>FCNN4R</code>, <code>plyr</code></p>
<p><strong>Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nnet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Neural Networks with Feature Extraction</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcaNNet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Penalized Linear Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;penalized&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda1</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>penalized</code></p>
<p><strong>Penalized Logistic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plr&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L2 Penalty)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>stepPlr</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Penalized Ordinal Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p><strong>Polynomial Kernel Regularized Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsPoly&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>KRLS</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Radial Basis Function Kernel Regularized Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsRadial&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>KRLS</code>, <code>kernlab</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbf&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbfDDA&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>negativeThreshold</code> (Activation Limit for Conflicting Classes)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Relaxed Lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;relaxo&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>phi</code> (Relaxation Parameter)</li>
</ul>
<p>Required packages: <code>relaxo</code>, <code>plyr</code></p>
<p><strong>Ridge Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ridge&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>Ridge Regression with Variable Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;foba&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Variables Retained)</li>
<li><code>lambda</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>foba</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<div id="Linear_Classifier">

</div>
</div>
<div id="linear-classifier" class="section level3">
<h3><span class="header-section-number">7.0.21</span> Linear Classifier</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmAdjCat&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Bagged Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Continuation Ratio Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmContRatio&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Cumulative Probability Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmCumulative&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Diagonal Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model)</li>
<li><code>shrinkage</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Ensembles of Generalized Lienar Models</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;randomGLM&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxInteractionOrder</code> (Interaction Order)</li>
</ul>
<p>Required packages: <code>randomGLM</code></p>
<p><strong>Factor-Based Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RFlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>q</code> (# Factors)</li>
</ul>
<p>Required packages: <code>HiDimDA</code></p>
<p><strong>Gaussian Process</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Generalized Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gpls&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K.prov</code> (#Components)</li>
</ul>
<p>Required packages: <code>gpls</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p><strong>Heteroscedastic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>newdim</code> (Dimension of the Discriminative Subspace)</li>
</ul>
<p>Required packages: <code>hda</code></p>
<p><strong>High Dimensional Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Threshold)</li>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>HDclassif</code></p>
<p><strong>High-Dimensional Regularized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdrda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>shrinkage_type</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Least Squares Support Vector Machine</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmLinear&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>dimen</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Localized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;loclda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Nearest Neighbors)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<p><strong>Logistic Model Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LMT&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (# Iteratons)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Maximum Uncertainty Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Mlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>HiDimDA</code></p>
<p><strong>Nearest Shrunken Centroids</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pam&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Shrinkage Threshold)</li>
</ul>
<p>Required packages: <code>pamr</code></p>
<p><strong>Ordered Logistic or Probit Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;polr&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>method</code> (parameter)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;simpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;widekernelpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Penalized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PenalizedLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>K</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>penalizedLDA</code>, <code>plyr</code></p>
<p><strong>Penalized Logistic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plr&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L2 Penalty)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>stepPlr</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Penalized Ordinal Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p><strong>Regularized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Regularized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimator</code> (Regularization Method)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Robust Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Linda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Regularized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rrlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>hp</code> (Robustness Parameter)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rrlda</code></p>
<p><strong>Robust SIMCA</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RSimca&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcovHD</code></p>
<p><strong>Shrinkage Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>diagonal</code> (Diagonalize)</li>
<li><code>lambda</code> (shrinkage)</li>
</ul>
<p>Required packages: <code>sda</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<p><strong>Sparse Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sparseLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<p><strong>Stabilized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;slda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code></p>
<p><strong>Stepwise Diagonal Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sddaLDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>SDDA</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<div id="Linear_Regression">

</div>
</div>
<div id="linear-regression" class="section level3">
<h3><span class="header-section-number">7.0.22</span> Linear Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p><strong>Bayesian Ridge Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bridge&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p><strong>Bayesian Ridge Regression (Model Averaged)</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blassoAveraged&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors.</p>
<p><strong>Boosted Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;BstLm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Cubist</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p><strong>Elasticnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enet&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p><strong>Independent Component Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;icr&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.comp</code> (#Components)</li>
</ul>
<p>Required packages: <code>fastICA</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Least Angle Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Least Angle Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars2&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>step</code> (#Steps)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Linear Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lm&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
</ul>
<p><strong>Linear Regression with Backwards Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapBackward&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Forward Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapForward&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapSeq&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lmStepAIC&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<p><strong>Model Rules</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5Rules&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Non-Convex Penalized Quantile Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqnc&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Non-Negative Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nnls&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>nnls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;simpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;widekernelpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Penalized Linear Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;penalized&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda1</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>penalized</code></p>
<p><strong>Penalized Ordinal Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p><strong>Principal Component Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcr&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Quantile Regression with LASSO penalty</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqlasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Relaxed Lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;relaxo&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>phi</code> (Relaxation Parameter)</li>
</ul>
<p>Required packages: <code>relaxo</code>, <code>plyr</code></p>
<p><strong>Relevance Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmLinear&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Ridge Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ridge&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>Ridge Regression with Variable Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;foba&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Variables Retained)</li>
<li><code>lambda</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>foba</code></p>
<p><strong>Robust Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlm&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
<li><code>psi</code> (psi)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Sparse Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<p><strong>Spike and Slab Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spikeslab&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (Variables Retained)</li>
</ul>
<p>Required packages: <code>spikeslab</code>, <code>plyr</code></p>
<p><strong>Supervised Principal Component Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;superpc&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Threshold)</li>
<li><code>n.components</code> (#Components)</li>
</ul>
<p>Required packages: <code>superpc</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>The Bayesian lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sparsity</code> (Sparsity Threshold)</li>
</ul>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter <code>sparsity</code>. For example, when <code>sparsity = .5</code>, only coefficients where at least half the posterior estimates are nonzero are used.</p>
<p><strong>The lasso</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<div id="Logic_Regression">

</div>
</div>
<div id="logic-regression" class="section level3">
<h3><span class="header-section-number">7.0.23</span> Logic Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p><strong>Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<div id="Logistic_Regression">

</div>
</div>
<div id="logistic-regression" class="section level3">
<h3><span class="header-section-number">7.0.24</span> Logistic Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmAdjCat&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Bagged Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Continuation Ratio Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmContRatio&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Cumulative Probability Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmCumulative&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Generalized Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gpls&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K.prov</code> (#Components)</li>
</ul>
<p>Required packages: <code>gpls</code></p>
<p><strong>Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<p><strong>Logistic Model Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LMT&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (# Iteratons)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Ordered Logistic or Probit Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;polr&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>method</code> (parameter)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Penalized Logistic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plr&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L2 Penalty)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>stepPlr</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<div id="Mixture_Model">

</div>
</div>
<div id="mixture-model" class="section level3">
<h3><span class="header-section-number">7.0.25</span> Mixture Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adaptive Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;amdai&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>adaptDA</code></p>
<p><strong>Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>subclasses</code> (#Subclasses Per Class)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Robust Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rmda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Subclasses Per Class)</li>
<li><code>model</code> (Model)</li>
</ul>
<p>Required packages: <code>robustDA</code></p>
<p><strong>Sparse Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;smda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>R</code> (# Subclasses)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<div id="Model_Tree">

</div>
</div>
<div id="model-tree" class="section level3">
<h3><span class="header-section-number">7.0.26</span> Model Tree</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Cubist</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p><strong>Logistic Model Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LMT&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (# Iteratons)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Rules</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5Rules&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<div id="Multivariate_Adaptive_Regression_Splines">

</div>
</div>
<div id="multivariate-adaptive-regression-splines" class="section level3">
<h3><span class="header-section-number">7.0.27</span> Multivariate Adaptive Regression Splines</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p><strong>Bagged MARS</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Flexible Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;fda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p><strong>Multivariate Adaptive Regression Spline</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;earth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p><strong>Multivariate Adaptive Regression Splines</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gcvEarth&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<div id="Neural_Network">

</div>
</div>
<div id="neural-network" class="section level3">
<h3><span class="header-section-number">7.0.28</span> Neural Network</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Regularized Neural Networks</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;brnn&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>neurons</code> (# Neurons)</li>
</ul>
<p>Required packages: <code>brnn</code></p>
<p><strong>Extreme Learning Machine</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;elm&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nhid</code> (#Hidden Units)</li>
<li><code>actfun</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>elmNN</code></p>
<p><strong>Model Averaged Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Multi-Layer Perceptron</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlp&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecay&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron, multiple layers</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecayML&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units layer1)</li>
<li><code>layer2</code> (#Hidden Units layer2)</li>
<li><code>layer3</code> (#Hidden Units layer3)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron, with multiple layers</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpML&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units layer1)</li>
<li><code>layer2</code> (#Hidden Units layer2)</li>
<li><code>layer3</code> (#Hidden Units layer3)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multilayer Perceptron Network by Stochastic Gradient Descent</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpSGD&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>l2reg</code> (L2 Regularization)</li>
<li><code>lambda</code> (RMSE Gradient Scaling)</li>
<li><code>learn_rate</code> (Learning Rate)</li>
<li><code>momentum</code> (Momentum)</li>
<li><code>gamma</code> (Decay)</li>
<li><code>minibatchsz</code> (Batch Size)</li>
<li><code>repeats</code> (#Models)</li>
</ul>
<p>Required packages: <code>FCNN4R</code>, <code>plyr</code></p>
<p><strong>Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;neuralnet&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units in Layer 1)</li>
<li><code>layer2</code> (#Hidden Units in Layer 2)</li>
<li><code>layer3</code> (#Hidden Units in Layer 3)</li>
</ul>
<p>Required packages: <code>neuralnet</code></p>
<p><strong>Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nnet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Neural Networks with Feature Extraction</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcaNNet&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbf&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbfDDA&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>negativeThreshold</code> (Activation Limit for Conflicting Classes)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Stacked AutoEncoder Deep Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dnn&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (Hidden Layer 1)</li>
<li><code>layer2</code> (Hidden Layer 2)</li>
<li><code>layer3</code> (Hidden Layer 3)</li>
<li><code>hidden_dropout</code> (Hidden Dropouts)</li>
<li><code>visible_dropout</code> (Visible Dropout)</li>
</ul>
<p>Required packages: <code>deepnet</code></p>
<div id="Oblique_Tree">

</div>
</div>
<div id="oblique-tree" class="section level3">
<h3><span class="header-section-number">7.0.29</span> Oblique Tree</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;oblique.tree&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>oblique.splits</code> (Oblique Splits)</li>
<li><code>variable.selection</code> (Variable Selection Method)</li>
</ul>
<p>Required packages: <code>oblique.tree</code></p>
<div id="Ordinal_Outcomes">

</div>
</div>
<div id="ordinal-outcomes" class="section level3">
<h3><span class="header-section-number">7.0.30</span> Ordinal Outcomes</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmAdjCat&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>CART or Ordinal Responses</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p><strong>Continuation Ratio Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmContRatio&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Cumulative Probability Model for Ordinal Data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmCumulative&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Ordered Logistic or Probit Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;polr&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>method</code> (parameter)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<div id="Partial_Least_Squares">

</div>
</div>
<div id="partial-least-squares" class="section level3">
<h3><span class="header-section-number">7.0.31</span> Partial Least Squares</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Ensemble Partial Least Squares Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enpls&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxcomp</code> (Max. #Components)</li>
</ul>
<p>Required packages: <code>enpls</code></p>
<p><strong>Ensemble Partial Least Squares Regression with Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enpls.fs&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxcomp</code> (Max. #Components)</li>
<li><code>threshold</code> (Importance Cutoff)</li>
</ul>
<p>Required packages: <code>enpls</code></p>
<p><strong>Generalized Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gpls&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K.prov</code> (#Components)</li>
</ul>
<p>Required packages: <code>gpls</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;simpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;widekernelpls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Partial Least Squares Generalized Linear Models </strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plsRglm&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nt</code> (#PLS Components)</li>
<li><code>alpha.pvals.expli</code> (p-Value threshold)</li>
</ul>
<p>Required packages: <code>plsRglm</code></p>
<p><strong>Sparse Partial Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<div id="Polynomial_Model">

</div>
</div>
<div id="polynomial-model" class="section level3">
<h3><span class="header-section-number">7.0.32</span> Polynomial Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Diagonal Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model)</li>
<li><code>shrinkage</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Gaussian Process with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprPoly&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>High-Dimensional Regularized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdrda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>shrinkage_type</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Least Squares Support Vector Machine with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmPoly&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Shrinkage Penalty Coefficient)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Polynomial Kernel Regularized Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsPoly&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>KRLS</code></p>
<p><strong>Quadratic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Quadratic Discriminant Analysis with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepQDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Regularized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Regularized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimator</code> (Regularization Method)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Relevance Vector Machines with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmPoly&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>scale</code> (Scale)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Robust Quadratic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;QdaCov&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Stepwise Diagonal Quadratic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sddaQDA&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>SDDA</code></p>
<p><strong>Support Vector Machines with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmPoly&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Prototype_Models">

</div>
</div>
<div id="prototype-models" class="section level3">
<h3><span class="header-section-number">7.0.33</span> Prototype Models</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Cubist</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p><strong>Greedy Prototype Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;protoclass&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>eps</code> (Ball Size)</li>
<li><code>Minkowski</code> (Distance Order)</li>
</ul>
<p>Required packages: <code>proxy</code>, <code>protoclass</code></p>
<p><strong>k-Nearest Neighbors</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kknn&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>kmax</code> (Max. #Neighbors)</li>
<li><code>distance</code> (Distance)</li>
<li><code>kernel</code> (Kernel)</li>
</ul>
<p>Required packages: <code>kknn</code></p>
<p><strong>k-Nearest Neighbors</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;knn&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Neighbors)</li>
</ul>
<p><strong>Knn regression via sklearn.neighbors.KNeighborsRegressor</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pythonKnnReg&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n_neighbors</code> (#Neighbors)</li>
<li><code>weights</code> (Weight Function)</li>
<li><code>algorithm</code> (Algorithm)</li>
<li><code>leaf_size</code> (Leaf Size)</li>
<li><code>metric</code> (Distance Metric)</li>
<li><code>p</code> (p)</li>
</ul>
<p>Required packages: <code>rPython</code></p>
<p><strong>Learning Vector Quantization</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lvq&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (Codebook Size)</li>
<li><code>k</code> (#Prototypes)</li>
</ul>
<p>Required packages: <code>class</code></p>
<p><strong>Nearest Shrunken Centroids</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pam&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Shrinkage Threshold)</li>
</ul>
<p>Required packages: <code>pamr</code></p>
<p><strong>Optimal Weighted Nearest Neighbor Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ownn&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Neighbors)</li>
</ul>
<p>Required packages: <code>snn</code></p>
<p><strong>Stabilized Nearest Neighbor Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;snn&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Stabilization Parameter)</li>
</ul>
<p>Required packages: <code>snn</code></p>
<div id="Quantile_Regression">

</div>
</div>
<div id="quantile-regression" class="section level3">
<h3><span class="header-section-number">7.0.34</span> Quantile Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Non-Convex Penalized Quantile Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqnc&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Quantile Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Quantile Regression with LASSO penalty</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqlasso&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<div id="Radial_Basis_Function">

</div>
</div>
<div id="radial-basis-function" class="section level3">
<h3><span class="header-section-number">7.0.35</span> Radial Basis Function</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Gaussian Process with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprRadial&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Radial Basis Function Kernel Regularized Least Squares</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsRadial&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>KRLS</code>, <code>kernlab</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbf&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbfDDA&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>negativeThreshold</code> (Activation Limit for Conflicting Classes)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Relevance Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmRadial&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadial&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialCost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialSigma&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p>Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Variational Bayesian Multinomial Probit Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vbmpRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimateTheta</code> (Theta Estimated)</li>
</ul>
<p>Required packages: <code>vbmp</code></p>
<div id="Random_Forest">

</div>
</div>
<div id="random-forest" class="section level3">
<h3><span class="header-section-number">7.0.36</span> Random Forest</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Conditional Inference Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Parallel Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;parRF&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>randomForest</code>, <code>foreach</code></p>
<p><strong>Quantile Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Random Ferns</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rFerns&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>depth</code> (Fern Depth)</li>
</ul>
<p>Required packages: <code>rFerns</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Rborist&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>predFixed</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>Rborist</code></p>
<p><strong>Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rf&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>randomForest</code></p>
<p><strong>Random Forest by Randomization</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;extraTrees&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (# Randomly Selected Predictors)</li>
<li><code>numRandomCuts</code> (# Random Cuts)</li>
</ul>
<p>Required packages: <code>extraTrees</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p><strong>Random Forest with Additional Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Boruta&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>Boruta</code>, <code>randomForest</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p><strong>Weighted Subspace Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;wsrf&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>wsrf</code></p>
<div id="Regularization">

</div>
</div>
<div id="regularization" class="section level3">
<h3><span class="header-section-number">7.0.37</span> Regularization</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Regularized Neural Networks</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;brnn&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>neurons</code> (# Neurons)</li>
</ul>
<p>Required packages: <code>brnn</code></p>
<p><strong>Diagonal Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model)</li>
<li><code>shrinkage</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Heteroscedastic Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>newdim</code> (Dimension of the Discriminative Subspace)</li>
</ul>
<p>Required packages: <code>hda</code></p>
<p><strong>High-Dimensional Regularized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdrda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>shrinkage_type</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Regularized Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Regularized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimator</code> (Regularization Method)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p><strong>Regularized Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p><strong>Robust Regularized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rrlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>hp</code> (Robustness Parameter)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rrlda</code></p>
<p><strong>Shrinkage Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>diagonal</code> (Diagonalize)</li>
<li><code>lambda</code> (shrinkage)</li>
</ul>
<p>Required packages: <code>sda</code></p>
<div id="Relevance_Vector_Machines">

</div>
</div>
<div id="relevance-vector-machines" class="section level3">
<h3><span class="header-section-number">7.0.38</span> Relevance Vector Machines</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Relevance Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmLinear&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmPoly&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>scale</code> (Scale)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmRadial&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Ridge_Regression">

</div>
</div>
<div id="ridge-regression" class="section level3">
<h3><span class="header-section-number">7.0.39</span> Ridge Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Ridge Regression with Variable Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;foba&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Variables Retained)</li>
<li><code>lambda</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>foba</code></p>
<div id="Robust_Methods">

</div>
</div>
<div id="robust-methods" class="section level3">
<h3><span class="header-section-number">7.0.40</span> Robust Methods</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Relevance Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmLinear&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmPoly&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>scale</code> (Scale)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmRadial&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Robust Mixture Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rmda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Subclasses Per Class)</li>
<li><code>model</code> (Model)</li>
</ul>
<p>Required packages: <code>robustDA</code></p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmPoly&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadial&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialSigma&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p>Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Robust_Model">

</div>
</div>
<div id="robust-model" class="section level3">
<h3><span class="header-section-number">7.0.41</span> Robust Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Quantile Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Robust Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Linda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlm&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
<li><code>psi</code> (psi)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Robust Regularized Linear Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rrlda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>hp</code> (Robustness Parameter)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rrlda</code></p>
<p><strong>Robust SIMCA</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RSimca&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcovHD</code></p>
<p><strong>SIMCA</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;CSimca&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcovHD</code></p>
<div id="ROC_Curves">

</div>
</div>
<div id="roc-curves" class="section level3">
<h3><span class="header-section-number">7.0.42</span> ROC Curves</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>ROC-Based Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rocc&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>xgenes</code> (#Variables Retained)</li>
</ul>
<p>Required packages: <code>rocc</code></p>
<div id="Rule_Based_Model">

</div>
</div>
<div id="rule-based-model" class="section level3">
<h3><span class="header-section-number">7.0.43</span> Rule-Based Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adaptive-Network-Based Fuzzy Inference System</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ANFIS&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cubist</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p><strong>Dynamic Evolving Neural-Fuzzy Inference System </strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;DENFIS&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>Dthr</code> (Threshold)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Inference Rules by Descent Method</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FIR.DM&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules Using Chi’s Method</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FRBCS.CHI&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>type.mf</code> (Membership Function)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules Using Genetic Cooperative-Competitive Learning</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;GFS.GCCL&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>popu.size</code> (Population Size)</li>
<li><code>max.gen</code> (Max. Generations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules Using Genetic Cooperative-Competitive Learning and Pittsburgh</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FH.GBML&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>max.num.rule</code> (Max. #Rules)</li>
<li><code>popu.size</code> (Population Size)</li>
<li><code>max.gen</code> (Max. Generations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules Using the Structural Learning Algorithm on Vague Environment</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;SLAVE&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
<li><code>max.gen</code> (Max. Generations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules via MOGUL</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;GFS.FR.MOGUL&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>max.gen</code> (Max. Generations)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
<li><code>max.tune</code> (Max. Tuning Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules via Thrift</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;GFS.THRIFT&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>popu.size</code> (Population Size)</li>
<li><code>num.labels</code> (# Fuzzy Labels)</li>
<li><code>max.gen</code> (Max. Generations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules with Weight Factor</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FRBCS.W&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>type.mf</code> (Membership Function)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Genetic Lateral Tuning and Rule Selection of Linguistic Fuzzy Systems</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;GFS.LT.RS&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>popu.size</code> (Population Size)</li>
<li><code>num.labels</code> (# Fuzzy Labels)</li>
<li><code>max.gen</code> (Max. Generations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Hybrid Neural Fuzzy Inference System</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;HYFIS&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Model Rules</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5Rules&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p><strong>Rule-Based Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;JRip&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumOpt</code> (# Optimizations)</li>
<li><code>NumFolds</code> (# Folds)</li>
<li><code>MinWeights</code> (Min Weights)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Rule-Based Classifier</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PART&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Confidence Threshold)</li>
<li><code>pruned</code> (Pruning)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Simplified TSK Fuzzy Rules</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FS.HGD&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Single C5.0 Ruleset</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Rules&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p><strong>Single Rule Classification</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;OneR&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Subtractive Clustering and Fuzzy c-Means Rules</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;SBC&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>r.a</code> (Radius)</li>
<li><code>eps.high</code> (Upper Threshold)</li>
<li><code>eps.low</code> (Lower Threshold)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Wang and Mendel Fuzzy Rules</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;WM&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>type.mf</code> (Membership Function)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<div id="Self_Organising_Maps">

</div>
</div>
<div id="self-organising-maps" class="section level3">
<h3><span class="header-section-number">7.0.44</span> Self-Organising Maps</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Self-Organizing Map</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bdk&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>xdim</code> (Row)</li>
<li><code>ydim</code> (Columns)</li>
<li><code>xweight</code> (X Weight)</li>
<li><code>topo</code> (Topology)</li>
</ul>
<p>Required packages: <code>kohonen</code></p>
<p><strong>Self-Organizing Maps</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xyf&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>xdim</code> (Row)</li>
<li><code>ydim</code> (Columns)</li>
<li><code>xweight</code> (X Weight)</li>
<li><code>topo</code> (Topology)</li>
</ul>
<p>Required packages: <code>kohonen</code></p>
<div id="String_Kernel">

</div>
</div>
<div id="string-kernel" class="section level3">
<h3><span class="header-section-number">7.0.45</span> String Kernel</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Support_Vector_Machines">

</div>
</div>
<div id="support-vector-machines" class="section level3">
<h3><span class="header-section-number">7.0.46</span> Support Vector Machines</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Least Squares Support Vector Machine</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmLinear&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmPoly&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmPoly&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadial&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialCost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialSigma&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p>Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Text_Mining">

</div>
</div>
<div id="text-mining" class="section level3">
<h3><span class="header-section-number">7.0.47</span> Text Mining</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Tree_Based_Model">

</div>
</div>
<div id="tree-based-model" class="section level3">
<h3><span class="header-section-number">7.0.48</span> Tree-Based Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>AdaBoost.M1</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Bagged AdaBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p><strong>Bagged CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;treebag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code>, <code>plyr</code>, <code>e1071</code></p>
<p><strong>Bayesian Additive Regression Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bartMachine&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_trees</code> (#Trees)</li>
<li><code>k</code> (Prior Boundary)</li>
<li><code>alpha</code> (Base Terminal Node Hyperparameter)</li>
<li><code>beta</code> (Power Terminal Node Hyperparameter)</li>
<li><code>nu</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>bartMachine</code></p>
<p><strong>Boosted Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Boosted Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blackboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>party</code>, <code>mboost</code>, <code>plyr</code></p>
<p><strong>Boosted Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstTree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>C4.5-like Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;J48&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Confidence Threshold)</li>
<li><code>M</code> (Minimum Instances Per Leaf)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart1SE&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rpart</code></p>
<p>Notes: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>CART or Ordinal Responses</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p><strong>CHi-squared Automated Interaction Detection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;chaid&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha2</code> (Merging Threshold)</li>
<li><code>alpha3</code> (Splitting former Merged Threshold)</li>
<li><code>alpha4</code> ( Splitting former Merged Threshold)</li>
</ul>
<p>Required packages: <code>CHAID</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree2&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cost-Sensitive CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>DeepBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ntrees</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>min_rows</code> (Min. Terminal Node Size)</li>
<li><code>learn_rate</code> (Shrinkage)</li>
<li><code>col_sample_rate</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>Model Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre></div>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Oblique Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;oblique.tree&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>oblique.splits</code> (Oblique Splits)</li>
<li><code>variable.selection</code> (Variable Selection Method)</li>
</ul>
<p>Required packages: <code>oblique.tree</code></p>
<p><strong>Random Forest with Additional Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Boruta&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>Boruta</code>, <code>randomForest</code></p>
<p><strong>Rotation Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForest&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
</ul>
<p>Required packages: <code>rotationForest</code></p>
<p><strong>Rotation Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForestCp&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code>, <code>rotationForest</code></p>
<p><strong>Single C5.0 Tree</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Tree&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p><strong>Tree Models from Genetic Algorithms</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;evtree&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>evtree</code></p>
<p><strong>Tree-Based Ensembles</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nodeHarvest&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxinter</code> (Maximum Interaction Depth)</li>
<li><code>mode</code> (Prediction Mode)</li>
</ul>
<p>Required packages: <code>nodeHarvest</code></p>
<div id="Two_Class_Only">

</div>
</div>
<div id="two-class-only" class="section level3">
<h3><span class="header-section-number">7.0.49</span> Two Class Only</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>Bagged Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p><strong>Bayesian Additive Regression Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bartMachine&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_trees</code> (#Trees)</li>
<li><code>k</code> (Prior Boundary)</li>
<li><code>alpha</code> (Base Terminal Node Hyperparameter)</li>
<li><code>beta</code> (Power Terminal Node Hyperparameter)</li>
<li><code>nu</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>bartMachine</code></p>
<p><strong>Binary Discriminant Analysis</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;binda&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda.freqs</code> (Shrinkage Intensity)</li>
</ul>
<p>Required packages: <code>binda</code></p>
<p><strong>Boosted Classification Trees</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>CHi-squared Automated Interaction Detection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;chaid&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha2</code> (Merging Threshold)</li>
<li><code>alpha3</code> (Splitting former Merged Threshold)</li>
<li><code>alpha4</code> ( Splitting former Merged Threshold)</li>
</ul>
<p>Required packages: <code>CHAID</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p><strong>Cost-Sensitive CART</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p><strong>DeepBoost</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Generalized Linear Model</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>glmnet</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Logic Regression</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Oblique Random Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p><strong>Partial Least Squares Generalized Linear Models </strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plsRglm&#39;</span></code></pre></div>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nt</code> (#PLS Components)</li>
<li><code>alpha.pvals.expli</code> (p-Value threshold)</li>
</ul>
<p>Required packages: <code>plsRglm</code></p>
<p><strong>Rotation Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForest&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
</ul>
<p>Required packages: <code>rotationForest</code></p>
<p><strong>Rotation Forest</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForestCp&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code>, <code>rotationForest</code></p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre></div>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Tree-Based Ensembles</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nodeHarvest&#39;</span></code></pre></div>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxinter</code> (Maximum Interaction Depth)</li>
<li><code>mode</code> (Prediction Mode)</li>
</ul>
<p>Required packages: <code>nodeHarvest</code></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="available-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="models-clustered-by-tag-similarity.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
